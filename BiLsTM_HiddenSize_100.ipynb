{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLsTM_Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "kjt9iuwEgm5T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tGMIDRTHgm5b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "26_C6S4Ugm5e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    header = ['tweet','label']\n",
        "    data_set = pandas.read_csv('cleaned_data.txt',delimiter='\\t',names = header)\n",
        "    return data_set\n",
        "\n",
        "def split_tweet(tweet):\n",
        "    return tweet.split()    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bmOzSIDpgm5h",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_set = get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ey347C1Zgm5l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "glove = torchtext.vocab.GloVe(name=\"twitter.27B\",dim=50)\n",
        "\n",
        "# inser padding character into glove embedding, we overwrite the first element and that's okay\n",
        "# because we don't use the first element in our vocab\n",
        "glove.vectors[0] = torch.tensor(np.zeros(50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BOG8rSwsB5Q7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "cbcb2b4c-ab52-4740-f325-4020cabf620d"
      },
      "cell_type": "code",
      "source": [
        "emb = nn.Embedding.from_pretrained(glove.vectors)\n",
        "[glove.stoi['!']]\n",
        "glove[\"dont\"]\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.6383,  0.6054, -0.8134,  0.3608, -0.1735, -0.2565,  0.9910,  0.3405,\n",
              "         0.8239,  1.5643, -0.5653, -0.0611, -5.0323, -0.5217, -0.7541,  0.4807,\n",
              "        -0.8197, -0.4939, -0.6875, -0.2293,  0.5157, -0.2352,  0.4733,  0.7174,\n",
              "        -0.1841,  0.0429,  0.4783,  0.4756, -0.5787, -0.3646,  0.0121,  0.0453,\n",
              "        -0.0818,  0.4173, -0.3661,  0.5307, -0.5937, -0.7334,  0.5946,  0.5258,\n",
              "        -0.4736,  0.1934,  0.3714, -0.2320, -0.4313, -0.9560,  0.9012,  0.2571,\n",
              "         0.4316, -0.0859])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sQ8JIzR4gm5q",
        "outputId": "7d69ffb6-d2fe-4504-8c61-5ae5711f8dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "len(data_set)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "320928"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tA4Ahaifgm53",
        "outputId": "82ca0f1b-631d-46e5-ab64-584a6bca83d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1959
        }
      },
      "cell_type": "code",
      "source": [
        "data_set"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hoping i dont screw up this interview</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i feel like a baby kangaroo stuck in its mothe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>girl ppl should be happy i even remembered her...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>oh ,  the irony if misha wins the choice tv sc...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i miss you to  ,  you so fake now</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>i miss you to  ,  you so fake now</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>i know</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>bacolod please ? !</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>things can change so quickly</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>all me &amp;amp ;  vic do is laugh .  .  anybody o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>do u have to remind me i was bored right after...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>its ok love .  you are sweet ,  but i know you...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>nainai has said bitch twice in 4 minutes and i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>louis thank you for everything you do you dese...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>impractical jokers its almost always on in our...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>my eyes are in pain .  cant get over the movie...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>daydrinking</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>genitin</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>like like like</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>eye need to get these brows done asap</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>i almost choked on my retainer just now</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>ms .  mendiola works miracles! shes so great</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>oh and parking dont even get me started on the...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>oh and parking dont even get me started on the...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>trying to stay my ass in the burbs but im gett...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>oh my god riley is so cute</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>why doe</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>i cant wait to be on the beach tomorrow</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>- warrior basti also will have his claws cut .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>not bein selfish .  .  im only workin wit one arm</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320898</th>\n",
              "      <td>no ,  im not</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320899</th>\n",
              "      <td>no ,  im not</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320900</th>\n",
              "      <td>turning 20 in 9 days</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320901</th>\n",
              "      <td>turning 20 in 9 days</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320902</th>\n",
              "      <td>im tweeting lyrics</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320903</th>\n",
              "      <td>im tweeting lyrics</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320904</th>\n",
              "      <td>you dont realize how powerful snapchat filters...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320905</th>\n",
              "      <td>you dont realize how powerful snapchat filters...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320906</th>\n",
              "      <td>free ?</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320907</th>\n",
              "      <td>free ?</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320908</th>\n",
              "      <td>dean fucking ambrose is the wwe champion! holy...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320909</th>\n",
              "      <td>the line taco bell rn is ridiculous</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320910</th>\n",
              "      <td>the line taco bell rn is ridiculous</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320911</th>\n",
              "      <td>a frog just followed me into my house .  .  .  .</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320912</th>\n",
              "      <td>a frog just followed me into my house .  .  .  .</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320913</th>\n",
              "      <td>posted about this earlier .  . my mind was blown</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320914</th>\n",
              "      <td>posted about this earlier .  . my mind was blown</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320915</th>\n",
              "      <td>okay klay</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320916</th>\n",
              "      <td>okay klay</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320917</th>\n",
              "      <td>why touch the mans toastie that would piss me ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320918</th>\n",
              "      <td>why touch the mans toastie that would piss me ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320919</th>\n",
              "      <td>imagine if tupac had a twitter</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320920</th>\n",
              "      <td>imagine if tupac had a twitter</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320921</th>\n",
              "      <td>me last summer .  .  .  i watched each movie a...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320922</th>\n",
              "      <td>me last summer .  .  .  i watched each movie a...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320923</th>\n",
              "      <td>vegas and phoenix weather is crazy this weekend</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320924</th>\n",
              "      <td>leavers have not been represented today at #pm...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320925</th>\n",
              "      <td>gotta get up in 1hr</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320926</th>\n",
              "      <td>i wanted to sniff cocaine</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320927</th>\n",
              "      <td>i wanted to sniff cocaine</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>320928 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    tweet  label\n",
              "0                   hoping i dont screw up this interview      0\n",
              "1       i feel like a baby kangaroo stuck in its mothe...      1\n",
              "2       girl ppl should be happy i even remembered her...      0\n",
              "3       oh ,  the irony if misha wins the choice tv sc...      2\n",
              "4                       i miss you to  ,  you so fake now      2\n",
              "5                       i miss you to  ,  you so fake now      4\n",
              "6                                                  i know      1\n",
              "7                                      bacolod please ? !      0\n",
              "8                            things can change so quickly      1\n",
              "9       all me &amp ;  vic do is laugh .  .  anybody o...      1\n",
              "10      do u have to remind me i was bored right after...      0\n",
              "11      its ok love .  you are sweet ,  but i know you...      0\n",
              "12      nainai has said bitch twice in 4 minutes and i...      0\n",
              "13      louis thank you for everything you do you dese...      0\n",
              "14      impractical jokers its almost always on in our...      0\n",
              "15      my eyes are in pain .  cant get over the movie...      1\n",
              "16                                            daydrinking      0\n",
              "17                                                genitin      0\n",
              "18                                         like like like      0\n",
              "19                  eye need to get these brows done asap      1\n",
              "20                i almost choked on my retainer just now      1\n",
              "21           ms .  mendiola works miracles! shes so great      1\n",
              "22      oh and parking dont even get me started on the...      2\n",
              "23      oh and parking dont even get me started on the...      4\n",
              "24      trying to stay my ass in the burbs but im gett...      1\n",
              "25                             oh my god riley is so cute      0\n",
              "26                                                why doe      0\n",
              "27                i cant wait to be on the beach tomorrow      0\n",
              "28        - warrior basti also will have his claws cut .       0\n",
              "29      not bein selfish .  .  im only workin wit one arm      0\n",
              "...                                                   ...    ...\n",
              "320898                                       no ,  im not      3\n",
              "320899                                       no ,  im not      5\n",
              "320900                               turning 20 in 9 days      2\n",
              "320901                               turning 20 in 9 days      5\n",
              "320902                                 im tweeting lyrics      3\n",
              "320903                                 im tweeting lyrics      5\n",
              "320904  you dont realize how powerful snapchat filters...      3\n",
              "320905  you dont realize how powerful snapchat filters...      5\n",
              "320906                                            free ?       3\n",
              "320907                                            free ?       5\n",
              "320908  dean fucking ambrose is the wwe champion! holy...      5\n",
              "320909                the line taco bell rn is ridiculous      3\n",
              "320910                the line taco bell rn is ridiculous      5\n",
              "320911  a frog just followed me into my house .  .  .  .       3\n",
              "320912  a frog just followed me into my house .  .  .  .       5\n",
              "320913   posted about this earlier .  . my mind was blown      3\n",
              "320914   posted about this earlier .  . my mind was blown      5\n",
              "320915                                          okay klay      3\n",
              "320916                                          okay klay      5\n",
              "320917  why touch the mans toastie that would piss me ...      3\n",
              "320918  why touch the mans toastie that would piss me ...      5\n",
              "320919                     imagine if tupac had a twitter      3\n",
              "320920                     imagine if tupac had a twitter      5\n",
              "320921  me last summer .  .  .  i watched each movie a...      3\n",
              "320922  me last summer .  .  .  i watched each movie a...      5\n",
              "320923    vegas and phoenix weather is crazy this weekend      5\n",
              "320924  leavers have not been represented today at #pm...      5\n",
              "320925                                gotta get up in 1hr      5\n",
              "320926                          i wanted to sniff cocaine      2\n",
              "320927                          i wanted to sniff cocaine      5\n",
              "\n",
              "[320928 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "qEmdNBXmEf0E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def missed_tweet_cleanup(tweet):\n",
        "    tweet = tweet.replace(\"!\", \" ! \")\\\n",
        "                 .replace(\"#\", \" # \")\n",
        "    return tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rj8r0_H9AFqt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def inference_tweet_cleanup(tweet):\n",
        "    tweet = tweet.replace(\".\", \" . \") \\\n",
        "                 .replace(\",\", \" , \") \\\n",
        "                 .replace(\";\", \" ; \") \\\n",
        "                 .replace(\"?\", \" ? \") \\\n",
        "                 .replace(\"\\'\", \"\") \\\n",
        "                 .replace(\"\\\"\", \"\")\\\n",
        "                 .replace(\"!\", \" ! \")\\\n",
        "                 .replace(\"#\", \" # \")\n",
        "\n",
        "    return tweet.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hvPLzeHsDs7l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "6704f038-4280-4d0b-e45b-b9021b8ed88a"
      },
      "cell_type": "code",
      "source": [
        "inp = \"Don't do; this!!! #howareyou\"\n",
        "print(inference_tweet_cleanup(inp))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dont do ;  this !  !  !   # howareyou\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cWYeV80Sgm59",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_set_to_glove_index(glove_dict):\n",
        "    tweets_ints, encoded_labels = [],[]\n",
        "    for i in range(len(data_set)):\n",
        "        tweet = data_set['tweet'][i]\n",
        "        label = data_set['label'][i]\n",
        "        if(type(tweet) != str):\n",
        "            continue\n",
        "        tweet = missed_tweet_cleanup(tweet)\n",
        "        idxs = [glove_dict.stoi[w]        # lookup the index of word\n",
        "            for w in tweet.split()\n",
        "            if w in glove_dict.stoi] # keep words that has an embedding\n",
        "        if not idxs: # ignore tweets without any word with an embedding\n",
        "            continue\n",
        "        tweets_ints.append(idxs)\n",
        "        encoded_labels.append(label)\n",
        "    return tweets_ints, encoded_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uci6Anmugm5-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets_ints, encoded_labels = data_set_to_glove_index(glove)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sB-kS3issuZ8",
        "outputId": "4fdb8117-6de8-4cce-d68e-b213fadabc0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# outlier review stats\n",
        "tweets_lens = Counter([len(x) for x in tweets_ints])\n",
        "print(\"Zero-length tweets: {}\".format(tweets_lens[0]))\n",
        "print(\"Maximum tweet length: {}\".format(max(tweets_lens)))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length tweets: 0\n",
            "Maximum tweet length: 62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CxaLejWos_Tw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_features(tweets_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''\n",
        "    \n",
        "    # getting the correct rows x cols shape\n",
        "    features = np.zeros((len(tweets_ints), seq_length), dtype=int)\n",
        "\n",
        "    # for each review, I grab that review and \n",
        "    for i, row in enumerate(tweets_ints):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wy8MFokXs_9K",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_length = max(tweets_lens)\n",
        "\n",
        "features = pad_features(tweets_ints, seq_length=seq_length)\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "R9siA7B4tAc7",
        "outputId": "4c9eaa58-44ed-4abb-8414-840a7522b0b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "print(features[:30,])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0 ...    85    53  2706]\n",
            " [    0     0     0 ...   221  9193 50306]\n",
            " [    0     0     0 ...   316   226   325]\n",
            " ...\n",
            " [    0     0     0 ...    13  1863   328]\n",
            " [    0     0     0 ... 58381  1465     1]\n",
            " [    0     0     0 ...  1104    96  4799]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gm-jUzQ8teCo"
      },
      "cell_type": "markdown",
      "source": [
        "### Train, Validation, Test Split"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8yZGBcHztBGC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "features, encoded_labels = shuffle(features, encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0nYglhndtgYi",
        "outputId": "bd356d75-d601-4bc8-8cc3-0df6cac2c410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "split_idx = int(len(features)*split_frac)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(256116, 62) \n",
            "Validation set: \t(32014, 62) \n",
            "Test set: \t\t(32015, 62)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BV-wQQmb5d7j"
      },
      "cell_type": "markdown",
      "source": [
        "### DataLoader and Batching"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FHgkWFzb5eRF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "# make sure the SHUFFLE your training data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DxNTXSGU5mpX",
        "outputId": "449d5a64-f43f-4ab4-c702-54728b203d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AkI6wmX8gm6U"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TdnI3kw0gm6V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TweetLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, use_gpu):\n",
        "        super(TweetLSTM, self).__init__()\n",
        "        self.use_gpu = use_gpu\n",
        "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size,bidirectional=True,batch_first=True)\n",
        "        self.fc = nn.Linear(2 * hidden_size, num_classes) # 2 * hidden_size because LSTM is bidrectional \n",
        "    def forward(self, x):\n",
        "        # Look up the embedding\n",
        "        x = self.emb(x)\n",
        "        # Set an initial hidden state and cell state\n",
        "        \n",
        "        if self.use_gpu:\n",
        "          h0 = torch.zeros(2, x.size(0), self.hidden_size).cuda()\n",
        "          c0 = torch.zeros(2, x.size(0), self.hidden_size).cuda()\n",
        "        else:\n",
        "          h0 = torch.zeros(2, x.size(0), self.hidden_size)\n",
        "          c0 = torch.zeros(2, x.size(0), self.hidden_size)\n",
        "        # Forward propagate the LSTM\n",
        "        out, _ = self.rnn(x, (h0, c0))\n",
        "        # Pass the output of the last time step to the classifier\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "num_classes = 6\n",
        "model = TweetLSTM(input_size=50, hidden_size=50, num_classes=num_classes,use_gpu=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "90sHCtA9gm6c",
        "outputId": "6ca18a7e-af8b-43ce-b4fe-490828cf11d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, data_loader, use_gpu):\n",
        "    if use_gpu:\n",
        "      model.cuda()\n",
        "    correct, total = 0, 0\n",
        "    for tweets, labels in data_loader:\n",
        "        if use_gpu:\n",
        "          tweets,labels  = tweets.cuda(), labels.cuda()\n",
        "        output = model(tweets)\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += labels.shape[0]\n",
        "    return float(correct) / float(total)\n",
        "  \n",
        "\n",
        "get_accuracy(model, test_loader,use_gpu = True)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18484375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XUd3LjcAgm6g",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_rnn_network(model, train, valid, num_epochs=5, learning_rate=1e-5,use_gpu=True):\n",
        "    if use_gpu:\n",
        "      model.cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    losses, train_acc, valid_acc = [], [], []\n",
        "    epochs = []\n",
        "    counter = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        batch_loss = []\n",
        "        for tweets, labels in train:\n",
        "            if use_gpu:\n",
        "              tweets,labels  = tweets.cuda(), labels.cuda()\n",
        "            counter += 1\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(tweets)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            batch_loss.append(float(loss))\n",
        "            if counter % 100 == 0:\n",
        "              print(\"Step %d of Epoch: %d; Loss %f \" % ( counter/100 ,epoch+1,np.mean(batch_loss)))\n",
        "        counter = 0\n",
        "        epoch_loss = np.mean(batch_loss)\n",
        "        losses.append(epoch_loss)\n",
        "        \n",
        "        epochs.append(epoch)\n",
        "        train_acc.append(get_accuracy(model, train_loader,use_gpu))\n",
        "        valid_acc.append(get_accuracy(model, valid_loader,use_gpu))\n",
        "        print(\"Final Result for Epoch %d: Loss %f; Val Acc %f; Train Acc %f\" % (\n",
        "              epoch+1, epoch_loss, valid_acc[-1], train_acc[-1]))\n",
        "        print(\"-------------------------------------------------------------\")\n",
        "        \n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(losses, label=\"Train\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(epochs, train_acc, label=\"Train\")\n",
        "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aPz9Hioigm6i",
        "outputId": "92bf26c6-ccfe-47b9-8270-b851331a6ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70162
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "model = TweetLSTM(input_size=50, hidden_size=100, num_classes=num_classes,use_gpu=True)\n",
        "train_rnn_network(model, train_loader, valid_loader, num_epochs=100, learning_rate=5e-4,use_gpu=True)\n",
        "end = time.time()\n",
        "print(\"total training time:\", int(end -start), \"s\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 of Epoch: 1; Loss 1.778985 \n",
            "Step 2 of Epoch: 1; Loss 1.765233 \n",
            "Step 3 of Epoch: 1; Loss 1.752440 \n",
            "Step 4 of Epoch: 1; Loss 1.742330 \n",
            "Step 5 of Epoch: 1; Loss 1.736665 \n",
            "Step 6 of Epoch: 1; Loss 1.730931 \n",
            "Step 7 of Epoch: 1; Loss 1.725926 \n",
            "Step 8 of Epoch: 1; Loss 1.722787 \n",
            "Step 9 of Epoch: 1; Loss 1.721058 \n",
            "Step 10 of Epoch: 1; Loss 1.718691 \n",
            "Step 11 of Epoch: 1; Loss 1.717538 \n",
            "Step 12 of Epoch: 1; Loss 1.716015 \n",
            "Step 13 of Epoch: 1; Loss 1.714813 \n",
            "Step 14 of Epoch: 1; Loss 1.713274 \n",
            "Step 15 of Epoch: 1; Loss 1.711535 \n",
            "Step 16 of Epoch: 1; Loss 1.710682 \n",
            "Step 17 of Epoch: 1; Loss 1.709163 \n",
            "Step 18 of Epoch: 1; Loss 1.708204 \n",
            "Step 19 of Epoch: 1; Loss 1.707015 \n",
            "Step 20 of Epoch: 1; Loss 1.706291 \n",
            "Step 21 of Epoch: 1; Loss 1.705886 \n",
            "Step 22 of Epoch: 1; Loss 1.705088 \n",
            "Step 23 of Epoch: 1; Loss 1.704261 \n",
            "Step 24 of Epoch: 1; Loss 1.703658 \n",
            "Step 25 of Epoch: 1; Loss 1.702655 \n",
            "Step 26 of Epoch: 1; Loss 1.702043 \n",
            "Step 27 of Epoch: 1; Loss 1.701083 \n",
            "Step 28 of Epoch: 1; Loss 1.700383 \n",
            "Step 29 of Epoch: 1; Loss 1.699564 \n",
            "Step 30 of Epoch: 1; Loss 1.698911 \n",
            "Step 31 of Epoch: 1; Loss 1.697797 \n",
            "Step 32 of Epoch: 1; Loss 1.697216 \n",
            "Step 33 of Epoch: 1; Loss 1.696699 \n",
            "Step 34 of Epoch: 1; Loss 1.695911 \n",
            "Step 35 of Epoch: 1; Loss 1.695493 \n",
            "Step 36 of Epoch: 1; Loss 1.695022 \n",
            "Step 37 of Epoch: 1; Loss 1.694484 \n",
            "Step 38 of Epoch: 1; Loss 1.693886 \n",
            "Step 39 of Epoch: 1; Loss 1.693292 \n",
            "Step 40 of Epoch: 1; Loss 1.692771 \n",
            "Final Result for Epoch 1: Loss 1.692786; Val Acc 0.289938; Train Acc 0.297336\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 2; Loss 1.663518 \n",
            "Step 2 of Epoch: 2; Loss 1.661579 \n",
            "Step 3 of Epoch: 2; Loss 1.667895 \n",
            "Step 4 of Epoch: 2; Loss 1.669366 \n",
            "Step 5 of Epoch: 2; Loss 1.670748 \n",
            "Step 6 of Epoch: 2; Loss 1.669825 \n",
            "Step 7 of Epoch: 2; Loss 1.668733 \n",
            "Step 8 of Epoch: 2; Loss 1.669649 \n",
            "Step 9 of Epoch: 2; Loss 1.668958 \n",
            "Step 10 of Epoch: 2; Loss 1.668948 \n",
            "Step 11 of Epoch: 2; Loss 1.668361 \n",
            "Step 12 of Epoch: 2; Loss 1.668444 \n",
            "Step 13 of Epoch: 2; Loss 1.668254 \n",
            "Step 14 of Epoch: 2; Loss 1.667759 \n",
            "Step 15 of Epoch: 2; Loss 1.667554 \n",
            "Step 16 of Epoch: 2; Loss 1.666944 \n",
            "Step 17 of Epoch: 2; Loss 1.667104 \n",
            "Step 18 of Epoch: 2; Loss 1.666675 \n",
            "Step 19 of Epoch: 2; Loss 1.666378 \n",
            "Step 20 of Epoch: 2; Loss 1.666249 \n",
            "Step 21 of Epoch: 2; Loss 1.665778 \n",
            "Step 22 of Epoch: 2; Loss 1.665613 \n",
            "Step 23 of Epoch: 2; Loss 1.665501 \n",
            "Step 24 of Epoch: 2; Loss 1.664533 \n",
            "Step 25 of Epoch: 2; Loss 1.664308 \n",
            "Step 26 of Epoch: 2; Loss 1.663887 \n",
            "Step 27 of Epoch: 2; Loss 1.663760 \n",
            "Step 28 of Epoch: 2; Loss 1.663157 \n",
            "Step 29 of Epoch: 2; Loss 1.663185 \n",
            "Step 30 of Epoch: 2; Loss 1.662915 \n",
            "Step 31 of Epoch: 2; Loss 1.662844 \n",
            "Step 32 of Epoch: 2; Loss 1.662898 \n",
            "Step 33 of Epoch: 2; Loss 1.662659 \n",
            "Step 34 of Epoch: 2; Loss 1.662613 \n",
            "Step 35 of Epoch: 2; Loss 1.662340 \n",
            "Step 36 of Epoch: 2; Loss 1.662157 \n",
            "Step 37 of Epoch: 2; Loss 1.662105 \n",
            "Step 38 of Epoch: 2; Loss 1.661992 \n",
            "Step 39 of Epoch: 2; Loss 1.661717 \n",
            "Step 40 of Epoch: 2; Loss 1.661556 \n",
            "Final Result for Epoch 2: Loss 1.661578; Val Acc 0.299969; Train Acc 0.306482\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 3; Loss 1.651395 \n",
            "Step 2 of Epoch: 3; Loss 1.649573 \n",
            "Step 3 of Epoch: 3; Loss 1.651055 \n",
            "Step 4 of Epoch: 3; Loss 1.651758 \n",
            "Step 5 of Epoch: 3; Loss 1.650654 \n",
            "Step 6 of Epoch: 3; Loss 1.649465 \n",
            "Step 7 of Epoch: 3; Loss 1.648383 \n",
            "Step 8 of Epoch: 3; Loss 1.648929 \n",
            "Step 9 of Epoch: 3; Loss 1.648328 \n",
            "Step 10 of Epoch: 3; Loss 1.648555 \n",
            "Step 11 of Epoch: 3; Loss 1.648845 \n",
            "Step 12 of Epoch: 3; Loss 1.648761 \n",
            "Step 13 of Epoch: 3; Loss 1.647995 \n",
            "Step 14 of Epoch: 3; Loss 1.647893 \n",
            "Step 15 of Epoch: 3; Loss 1.648534 \n",
            "Step 16 of Epoch: 3; Loss 1.648802 \n",
            "Step 17 of Epoch: 3; Loss 1.647757 \n",
            "Step 18 of Epoch: 3; Loss 1.647696 \n",
            "Step 19 of Epoch: 3; Loss 1.647787 \n",
            "Step 20 of Epoch: 3; Loss 1.647824 \n",
            "Step 21 of Epoch: 3; Loss 1.647001 \n",
            "Step 22 of Epoch: 3; Loss 1.646799 \n",
            "Step 23 of Epoch: 3; Loss 1.647069 \n",
            "Step 24 of Epoch: 3; Loss 1.647222 \n",
            "Step 25 of Epoch: 3; Loss 1.646930 \n",
            "Step 26 of Epoch: 3; Loss 1.646898 \n",
            "Step 27 of Epoch: 3; Loss 1.646979 \n",
            "Step 28 of Epoch: 3; Loss 1.646992 \n",
            "Step 29 of Epoch: 3; Loss 1.647095 \n",
            "Step 30 of Epoch: 3; Loss 1.647161 \n",
            "Step 31 of Epoch: 3; Loss 1.647003 \n",
            "Step 32 of Epoch: 3; Loss 1.646982 \n",
            "Step 33 of Epoch: 3; Loss 1.646952 \n",
            "Step 34 of Epoch: 3; Loss 1.646722 \n",
            "Step 35 of Epoch: 3; Loss 1.646851 \n",
            "Step 36 of Epoch: 3; Loss 1.646755 \n",
            "Step 37 of Epoch: 3; Loss 1.646626 \n",
            "Step 38 of Epoch: 3; Loss 1.646529 \n",
            "Step 39 of Epoch: 3; Loss 1.646436 \n",
            "Step 40 of Epoch: 3; Loss 1.646320 \n",
            "Final Result for Epoch 3: Loss 1.646350; Val Acc 0.304906; Train Acc 0.317346\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 4; Loss 1.629116 \n",
            "Step 2 of Epoch: 4; Loss 1.633252 \n",
            "Step 3 of Epoch: 4; Loss 1.634123 \n",
            "Step 4 of Epoch: 4; Loss 1.636371 \n",
            "Step 5 of Epoch: 4; Loss 1.635144 \n",
            "Step 6 of Epoch: 4; Loss 1.633470 \n",
            "Step 7 of Epoch: 4; Loss 1.633639 \n",
            "Step 8 of Epoch: 4; Loss 1.633651 \n",
            "Step 9 of Epoch: 4; Loss 1.634748 \n",
            "Step 10 of Epoch: 4; Loss 1.634753 \n",
            "Step 11 of Epoch: 4; Loss 1.634376 \n",
            "Step 12 of Epoch: 4; Loss 1.633771 \n",
            "Step 13 of Epoch: 4; Loss 1.632907 \n",
            "Step 14 of Epoch: 4; Loss 1.633590 \n",
            "Step 15 of Epoch: 4; Loss 1.634619 \n",
            "Step 16 of Epoch: 4; Loss 1.634853 \n",
            "Step 17 of Epoch: 4; Loss 1.635118 \n",
            "Step 18 of Epoch: 4; Loss 1.634856 \n",
            "Step 19 of Epoch: 4; Loss 1.635260 \n",
            "Step 20 of Epoch: 4; Loss 1.634711 \n",
            "Step 21 of Epoch: 4; Loss 1.634967 \n",
            "Step 22 of Epoch: 4; Loss 1.634780 \n",
            "Step 23 of Epoch: 4; Loss 1.634633 \n",
            "Step 24 of Epoch: 4; Loss 1.635104 \n",
            "Step 25 of Epoch: 4; Loss 1.634982 \n",
            "Step 26 of Epoch: 4; Loss 1.634893 \n",
            "Step 27 of Epoch: 4; Loss 1.634466 \n",
            "Step 28 of Epoch: 4; Loss 1.634846 \n",
            "Step 29 of Epoch: 4; Loss 1.634532 \n",
            "Step 30 of Epoch: 4; Loss 1.634223 \n",
            "Step 31 of Epoch: 4; Loss 1.634832 \n",
            "Step 32 of Epoch: 4; Loss 1.634879 \n",
            "Step 33 of Epoch: 4; Loss 1.635062 \n",
            "Step 34 of Epoch: 4; Loss 1.635124 \n",
            "Step 35 of Epoch: 4; Loss 1.634848 \n",
            "Step 36 of Epoch: 4; Loss 1.634805 \n",
            "Step 37 of Epoch: 4; Loss 1.634769 \n",
            "Step 38 of Epoch: 4; Loss 1.634926 \n",
            "Step 39 of Epoch: 4; Loss 1.634849 \n",
            "Step 40 of Epoch: 4; Loss 1.634863 \n",
            "Final Result for Epoch 4: Loss 1.634856; Val Acc 0.311719; Train Acc 0.324727\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 5; Loss 1.630601 \n",
            "Step 2 of Epoch: 5; Loss 1.627200 \n",
            "Step 3 of Epoch: 5; Loss 1.629452 \n",
            "Step 4 of Epoch: 5; Loss 1.629525 \n",
            "Step 5 of Epoch: 5; Loss 1.625195 \n",
            "Step 6 of Epoch: 5; Loss 1.624938 \n",
            "Step 7 of Epoch: 5; Loss 1.626673 \n",
            "Step 8 of Epoch: 5; Loss 1.625891 \n",
            "Step 9 of Epoch: 5; Loss 1.626466 \n",
            "Step 10 of Epoch: 5; Loss 1.626535 \n",
            "Step 11 of Epoch: 5; Loss 1.626461 \n",
            "Step 12 of Epoch: 5; Loss 1.625449 \n",
            "Step 13 of Epoch: 5; Loss 1.624177 \n",
            "Step 14 of Epoch: 5; Loss 1.623904 \n",
            "Step 15 of Epoch: 5; Loss 1.624010 \n",
            "Step 16 of Epoch: 5; Loss 1.623318 \n",
            "Step 17 of Epoch: 5; Loss 1.623535 \n",
            "Step 18 of Epoch: 5; Loss 1.623545 \n",
            "Step 19 of Epoch: 5; Loss 1.623004 \n",
            "Step 20 of Epoch: 5; Loss 1.623398 \n",
            "Step 21 of Epoch: 5; Loss 1.623324 \n",
            "Step 22 of Epoch: 5; Loss 1.623171 \n",
            "Step 23 of Epoch: 5; Loss 1.622785 \n",
            "Step 24 of Epoch: 5; Loss 1.623093 \n",
            "Step 25 of Epoch: 5; Loss 1.623231 \n",
            "Step 26 of Epoch: 5; Loss 1.623536 \n",
            "Step 27 of Epoch: 5; Loss 1.623835 \n",
            "Step 28 of Epoch: 5; Loss 1.624207 \n",
            "Step 29 of Epoch: 5; Loss 1.624230 \n",
            "Step 30 of Epoch: 5; Loss 1.624371 \n",
            "Step 31 of Epoch: 5; Loss 1.624523 \n",
            "Step 32 of Epoch: 5; Loss 1.624801 \n",
            "Step 33 of Epoch: 5; Loss 1.624885 \n",
            "Step 34 of Epoch: 5; Loss 1.624957 \n",
            "Step 35 of Epoch: 5; Loss 1.624846 \n",
            "Step 36 of Epoch: 5; Loss 1.624947 \n",
            "Step 37 of Epoch: 5; Loss 1.624817 \n",
            "Step 38 of Epoch: 5; Loss 1.624859 \n",
            "Step 39 of Epoch: 5; Loss 1.625016 \n",
            "Step 40 of Epoch: 5; Loss 1.625140 \n",
            "Final Result for Epoch 5: Loss 1.625150; Val Acc 0.310750; Train Acc 0.328008\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 6; Loss 1.616573 \n",
            "Step 2 of Epoch: 6; Loss 1.621074 \n",
            "Step 3 of Epoch: 6; Loss 1.621366 \n",
            "Step 4 of Epoch: 6; Loss 1.616811 \n",
            "Step 5 of Epoch: 6; Loss 1.618345 \n",
            "Step 6 of Epoch: 6; Loss 1.618530 \n",
            "Step 7 of Epoch: 6; Loss 1.616794 \n",
            "Step 8 of Epoch: 6; Loss 1.617185 \n",
            "Step 9 of Epoch: 6; Loss 1.616521 \n",
            "Step 10 of Epoch: 6; Loss 1.616065 \n",
            "Step 11 of Epoch: 6; Loss 1.617132 \n",
            "Step 12 of Epoch: 6; Loss 1.616102 \n",
            "Step 13 of Epoch: 6; Loss 1.616206 \n",
            "Step 14 of Epoch: 6; Loss 1.616693 \n",
            "Step 15 of Epoch: 6; Loss 1.617924 \n",
            "Step 16 of Epoch: 6; Loss 1.618089 \n",
            "Step 17 of Epoch: 6; Loss 1.618542 \n",
            "Step 18 of Epoch: 6; Loss 1.618873 \n",
            "Step 19 of Epoch: 6; Loss 1.618409 \n",
            "Step 20 of Epoch: 6; Loss 1.617923 \n",
            "Step 21 of Epoch: 6; Loss 1.617757 \n",
            "Step 22 of Epoch: 6; Loss 1.617263 \n",
            "Step 23 of Epoch: 6; Loss 1.617267 \n",
            "Step 24 of Epoch: 6; Loss 1.617360 \n",
            "Step 25 of Epoch: 6; Loss 1.617333 \n",
            "Step 26 of Epoch: 6; Loss 1.617356 \n",
            "Step 27 of Epoch: 6; Loss 1.617255 \n",
            "Step 28 of Epoch: 6; Loss 1.617274 \n",
            "Step 29 of Epoch: 6; Loss 1.617412 \n",
            "Step 30 of Epoch: 6; Loss 1.617225 \n",
            "Step 31 of Epoch: 6; Loss 1.616987 \n",
            "Step 32 of Epoch: 6; Loss 1.617276 \n",
            "Step 33 of Epoch: 6; Loss 1.617206 \n",
            "Step 34 of Epoch: 6; Loss 1.617290 \n",
            "Step 35 of Epoch: 6; Loss 1.617555 \n",
            "Step 36 of Epoch: 6; Loss 1.617192 \n",
            "Step 37 of Epoch: 6; Loss 1.617061 \n",
            "Step 38 of Epoch: 6; Loss 1.616823 \n",
            "Step 39 of Epoch: 6; Loss 1.616733 \n",
            "Step 40 of Epoch: 6; Loss 1.616449 \n",
            "Final Result for Epoch 6: Loss 1.616453; Val Acc 0.312469; Train Acc 0.334748\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 7; Loss 1.598313 \n",
            "Step 2 of Epoch: 7; Loss 1.599976 \n",
            "Step 3 of Epoch: 7; Loss 1.602275 \n",
            "Step 4 of Epoch: 7; Loss 1.605948 \n",
            "Step 5 of Epoch: 7; Loss 1.606048 \n",
            "Step 6 of Epoch: 7; Loss 1.604367 \n",
            "Step 7 of Epoch: 7; Loss 1.603442 \n",
            "Step 8 of Epoch: 7; Loss 1.604149 \n",
            "Step 9 of Epoch: 7; Loss 1.604070 \n",
            "Step 10 of Epoch: 7; Loss 1.601501 \n",
            "Step 11 of Epoch: 7; Loss 1.601711 \n",
            "Step 12 of Epoch: 7; Loss 1.601671 \n",
            "Step 13 of Epoch: 7; Loss 1.601870 \n",
            "Step 14 of Epoch: 7; Loss 1.602159 \n",
            "Step 15 of Epoch: 7; Loss 1.602519 \n",
            "Step 16 of Epoch: 7; Loss 1.603729 \n",
            "Step 17 of Epoch: 7; Loss 1.603409 \n",
            "Step 18 of Epoch: 7; Loss 1.604117 \n",
            "Step 19 of Epoch: 7; Loss 1.604588 \n",
            "Step 20 of Epoch: 7; Loss 1.605462 \n",
            "Step 21 of Epoch: 7; Loss 1.605726 \n",
            "Step 22 of Epoch: 7; Loss 1.605911 \n",
            "Step 23 of Epoch: 7; Loss 1.605826 \n",
            "Step 24 of Epoch: 7; Loss 1.605964 \n",
            "Step 25 of Epoch: 7; Loss 1.606097 \n",
            "Step 26 of Epoch: 7; Loss 1.606213 \n",
            "Step 27 of Epoch: 7; Loss 1.606474 \n",
            "Step 28 of Epoch: 7; Loss 1.606479 \n",
            "Step 29 of Epoch: 7; Loss 1.606892 \n",
            "Step 30 of Epoch: 7; Loss 1.607102 \n",
            "Step 31 of Epoch: 7; Loss 1.606960 \n",
            "Step 32 of Epoch: 7; Loss 1.606768 \n",
            "Step 33 of Epoch: 7; Loss 1.606562 \n",
            "Step 34 of Epoch: 7; Loss 1.607079 \n",
            "Step 35 of Epoch: 7; Loss 1.607053 \n",
            "Step 36 of Epoch: 7; Loss 1.607204 \n",
            "Step 37 of Epoch: 7; Loss 1.607430 \n",
            "Step 38 of Epoch: 7; Loss 1.607407 \n",
            "Step 39 of Epoch: 7; Loss 1.607455 \n",
            "Step 40 of Epoch: 7; Loss 1.607608 \n",
            "Final Result for Epoch 7: Loss 1.607590; Val Acc 0.315281; Train Acc 0.336631\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 8; Loss 1.591246 \n",
            "Step 2 of Epoch: 8; Loss 1.599986 \n",
            "Step 3 of Epoch: 8; Loss 1.599987 \n",
            "Step 4 of Epoch: 8; Loss 1.599179 \n",
            "Step 5 of Epoch: 8; Loss 1.599112 \n",
            "Step 6 of Epoch: 8; Loss 1.597600 \n",
            "Step 7 of Epoch: 8; Loss 1.597733 \n",
            "Step 8 of Epoch: 8; Loss 1.598470 \n",
            "Step 9 of Epoch: 8; Loss 1.598656 \n",
            "Step 10 of Epoch: 8; Loss 1.597847 \n",
            "Step 11 of Epoch: 8; Loss 1.598189 \n",
            "Step 12 of Epoch: 8; Loss 1.597575 \n",
            "Step 13 of Epoch: 8; Loss 1.598264 \n",
            "Step 14 of Epoch: 8; Loss 1.598457 \n",
            "Step 15 of Epoch: 8; Loss 1.597783 \n",
            "Step 16 of Epoch: 8; Loss 1.598368 \n",
            "Step 17 of Epoch: 8; Loss 1.598983 \n",
            "Step 18 of Epoch: 8; Loss 1.598925 \n",
            "Step 19 of Epoch: 8; Loss 1.598611 \n",
            "Step 20 of Epoch: 8; Loss 1.598431 \n",
            "Step 21 of Epoch: 8; Loss 1.598818 \n",
            "Step 22 of Epoch: 8; Loss 1.598859 \n",
            "Step 23 of Epoch: 8; Loss 1.598960 \n",
            "Step 24 of Epoch: 8; Loss 1.598913 \n",
            "Step 25 of Epoch: 8; Loss 1.599711 \n",
            "Step 26 of Epoch: 8; Loss 1.599586 \n",
            "Step 27 of Epoch: 8; Loss 1.599125 \n",
            "Step 28 of Epoch: 8; Loss 1.598768 \n",
            "Step 29 of Epoch: 8; Loss 1.598965 \n",
            "Step 30 of Epoch: 8; Loss 1.599076 \n",
            "Step 31 of Epoch: 8; Loss 1.599307 \n",
            "Step 32 of Epoch: 8; Loss 1.599042 \n",
            "Step 33 of Epoch: 8; Loss 1.598817 \n",
            "Step 34 of Epoch: 8; Loss 1.598750 \n",
            "Step 35 of Epoch: 8; Loss 1.599252 \n",
            "Step 36 of Epoch: 8; Loss 1.599372 \n",
            "Step 37 of Epoch: 8; Loss 1.599344 \n",
            "Step 38 of Epoch: 8; Loss 1.599544 \n",
            "Step 39 of Epoch: 8; Loss 1.599623 \n",
            "Step 40 of Epoch: 8; Loss 1.599509 \n",
            "Final Result for Epoch 8: Loss 1.599520; Val Acc 0.315531; Train Acc 0.344359\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 9; Loss 1.584190 \n",
            "Step 2 of Epoch: 9; Loss 1.585999 \n",
            "Step 3 of Epoch: 9; Loss 1.593297 \n",
            "Step 4 of Epoch: 9; Loss 1.591298 \n",
            "Step 5 of Epoch: 9; Loss 1.589571 \n",
            "Step 6 of Epoch: 9; Loss 1.590091 \n",
            "Step 7 of Epoch: 9; Loss 1.590597 \n",
            "Step 8 of Epoch: 9; Loss 1.589472 \n",
            "Step 9 of Epoch: 9; Loss 1.589833 \n",
            "Step 10 of Epoch: 9; Loss 1.590153 \n",
            "Step 11 of Epoch: 9; Loss 1.590485 \n",
            "Step 12 of Epoch: 9; Loss 1.590642 \n",
            "Step 13 of Epoch: 9; Loss 1.589647 \n",
            "Step 14 of Epoch: 9; Loss 1.590559 \n",
            "Step 15 of Epoch: 9; Loss 1.590627 \n",
            "Step 16 of Epoch: 9; Loss 1.591019 \n",
            "Step 17 of Epoch: 9; Loss 1.590103 \n",
            "Step 18 of Epoch: 9; Loss 1.590001 \n",
            "Step 19 of Epoch: 9; Loss 1.589777 \n",
            "Step 20 of Epoch: 9; Loss 1.589028 \n",
            "Step 21 of Epoch: 9; Loss 1.589390 \n",
            "Step 22 of Epoch: 9; Loss 1.589133 \n",
            "Step 23 of Epoch: 9; Loss 1.589155 \n",
            "Step 24 of Epoch: 9; Loss 1.589357 \n",
            "Step 25 of Epoch: 9; Loss 1.589349 \n",
            "Step 26 of Epoch: 9; Loss 1.589527 \n",
            "Step 27 of Epoch: 9; Loss 1.589674 \n",
            "Step 28 of Epoch: 9; Loss 1.589764 \n",
            "Step 29 of Epoch: 9; Loss 1.590458 \n",
            "Step 30 of Epoch: 9; Loss 1.590448 \n",
            "Step 31 of Epoch: 9; Loss 1.590673 \n",
            "Step 32 of Epoch: 9; Loss 1.590529 \n",
            "Step 33 of Epoch: 9; Loss 1.590229 \n",
            "Step 34 of Epoch: 9; Loss 1.590546 \n",
            "Step 35 of Epoch: 9; Loss 1.590763 \n",
            "Step 36 of Epoch: 9; Loss 1.590814 \n",
            "Step 37 of Epoch: 9; Loss 1.591113 \n",
            "Step 38 of Epoch: 9; Loss 1.590937 \n",
            "Step 39 of Epoch: 9; Loss 1.590880 \n",
            "Step 40 of Epoch: 9; Loss 1.590855 \n",
            "Final Result for Epoch 9: Loss 1.590881; Val Acc 0.314469; Train Acc 0.346101\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 10; Loss 1.585684 \n",
            "Step 2 of Epoch: 10; Loss 1.584455 \n",
            "Step 3 of Epoch: 10; Loss 1.580542 \n",
            "Step 4 of Epoch: 10; Loss 1.581873 \n",
            "Step 5 of Epoch: 10; Loss 1.578875 \n",
            "Step 6 of Epoch: 10; Loss 1.579229 \n",
            "Step 7 of Epoch: 10; Loss 1.578605 \n",
            "Step 8 of Epoch: 10; Loss 1.577567 \n",
            "Step 9 of Epoch: 10; Loss 1.578167 \n",
            "Step 10 of Epoch: 10; Loss 1.578575 \n",
            "Step 11 of Epoch: 10; Loss 1.579288 \n",
            "Step 12 of Epoch: 10; Loss 1.579978 \n",
            "Step 13 of Epoch: 10; Loss 1.580492 \n",
            "Step 14 of Epoch: 10; Loss 1.579731 \n",
            "Step 15 of Epoch: 10; Loss 1.580492 \n",
            "Step 16 of Epoch: 10; Loss 1.580338 \n",
            "Step 17 of Epoch: 10; Loss 1.579750 \n",
            "Step 18 of Epoch: 10; Loss 1.579614 \n",
            "Step 19 of Epoch: 10; Loss 1.580059 \n",
            "Step 20 of Epoch: 10; Loss 1.580282 \n",
            "Step 21 of Epoch: 10; Loss 1.580774 \n",
            "Step 22 of Epoch: 10; Loss 1.581083 \n",
            "Step 23 of Epoch: 10; Loss 1.581602 \n",
            "Step 24 of Epoch: 10; Loss 1.580846 \n",
            "Step 25 of Epoch: 10; Loss 1.581375 \n",
            "Step 26 of Epoch: 10; Loss 1.581452 \n",
            "Step 27 of Epoch: 10; Loss 1.581898 \n",
            "Step 28 of Epoch: 10; Loss 1.581837 \n",
            "Step 29 of Epoch: 10; Loss 1.582310 \n",
            "Step 30 of Epoch: 10; Loss 1.582507 \n",
            "Step 31 of Epoch: 10; Loss 1.582339 \n",
            "Step 32 of Epoch: 10; Loss 1.581941 \n",
            "Step 33 of Epoch: 10; Loss 1.582215 \n",
            "Step 34 of Epoch: 10; Loss 1.582386 \n",
            "Step 35 of Epoch: 10; Loss 1.582055 \n",
            "Step 36 of Epoch: 10; Loss 1.582328 \n",
            "Step 37 of Epoch: 10; Loss 1.582600 \n",
            "Step 38 of Epoch: 10; Loss 1.582867 \n",
            "Step 39 of Epoch: 10; Loss 1.582908 \n",
            "Step 40 of Epoch: 10; Loss 1.582825 \n",
            "Final Result for Epoch 10: Loss 1.582841; Val Acc 0.316344; Train Acc 0.352705\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 11; Loss 1.578279 \n",
            "Step 2 of Epoch: 11; Loss 1.577340 \n",
            "Step 3 of Epoch: 11; Loss 1.575739 \n",
            "Step 4 of Epoch: 11; Loss 1.571631 \n",
            "Step 5 of Epoch: 11; Loss 1.573119 \n",
            "Step 6 of Epoch: 11; Loss 1.571562 \n",
            "Step 7 of Epoch: 11; Loss 1.570437 \n",
            "Step 8 of Epoch: 11; Loss 1.570785 \n",
            "Step 9 of Epoch: 11; Loss 1.570710 \n",
            "Step 10 of Epoch: 11; Loss 1.572245 \n",
            "Step 11 of Epoch: 11; Loss 1.572439 \n",
            "Step 12 of Epoch: 11; Loss 1.573314 \n",
            "Step 13 of Epoch: 11; Loss 1.573384 \n",
            "Step 14 of Epoch: 11; Loss 1.573568 \n",
            "Step 15 of Epoch: 11; Loss 1.573671 \n",
            "Step 16 of Epoch: 11; Loss 1.574159 \n",
            "Step 17 of Epoch: 11; Loss 1.574240 \n",
            "Step 18 of Epoch: 11; Loss 1.574248 \n",
            "Step 19 of Epoch: 11; Loss 1.574626 \n",
            "Step 20 of Epoch: 11; Loss 1.574610 \n",
            "Step 21 of Epoch: 11; Loss 1.574129 \n",
            "Step 22 of Epoch: 11; Loss 1.574371 \n",
            "Step 23 of Epoch: 11; Loss 1.574392 \n",
            "Step 24 of Epoch: 11; Loss 1.574649 \n",
            "Step 25 of Epoch: 11; Loss 1.575263 \n",
            "Step 26 of Epoch: 11; Loss 1.575300 \n",
            "Step 27 of Epoch: 11; Loss 1.575108 \n",
            "Step 28 of Epoch: 11; Loss 1.575297 \n",
            "Step 29 of Epoch: 11; Loss 1.574984 \n",
            "Step 30 of Epoch: 11; Loss 1.574712 \n",
            "Step 31 of Epoch: 11; Loss 1.574722 \n",
            "Step 32 of Epoch: 11; Loss 1.574710 \n",
            "Step 33 of Epoch: 11; Loss 1.574921 \n",
            "Step 34 of Epoch: 11; Loss 1.574771 \n",
            "Step 35 of Epoch: 11; Loss 1.574702 \n",
            "Step 36 of Epoch: 11; Loss 1.574723 \n",
            "Step 37 of Epoch: 11; Loss 1.574806 \n",
            "Step 38 of Epoch: 11; Loss 1.574830 \n",
            "Step 39 of Epoch: 11; Loss 1.574888 \n",
            "Step 40 of Epoch: 11; Loss 1.574701 \n",
            "Final Result for Epoch 11: Loss 1.574683; Val Acc 0.314469; Train Acc 0.358008\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 12; Loss 1.547457 \n",
            "Step 2 of Epoch: 12; Loss 1.554551 \n",
            "Step 3 of Epoch: 12; Loss 1.558210 \n",
            "Step 4 of Epoch: 12; Loss 1.560488 \n",
            "Step 5 of Epoch: 12; Loss 1.559399 \n",
            "Step 6 of Epoch: 12; Loss 1.560609 \n",
            "Step 7 of Epoch: 12; Loss 1.562931 \n",
            "Step 8 of Epoch: 12; Loss 1.564642 \n",
            "Step 9 of Epoch: 12; Loss 1.564635 \n",
            "Step 10 of Epoch: 12; Loss 1.564966 \n",
            "Step 11 of Epoch: 12; Loss 1.564061 \n",
            "Step 12 of Epoch: 12; Loss 1.564220 \n",
            "Step 13 of Epoch: 12; Loss 1.563848 \n",
            "Step 14 of Epoch: 12; Loss 1.563644 \n",
            "Step 15 of Epoch: 12; Loss 1.563818 \n",
            "Step 16 of Epoch: 12; Loss 1.565001 \n",
            "Step 17 of Epoch: 12; Loss 1.564344 \n",
            "Step 18 of Epoch: 12; Loss 1.564037 \n",
            "Step 19 of Epoch: 12; Loss 1.564704 \n",
            "Step 20 of Epoch: 12; Loss 1.565018 \n",
            "Step 21 of Epoch: 12; Loss 1.565696 \n",
            "Step 22 of Epoch: 12; Loss 1.565253 \n",
            "Step 23 of Epoch: 12; Loss 1.565430 \n",
            "Step 24 of Epoch: 12; Loss 1.565270 \n",
            "Step 25 of Epoch: 12; Loss 1.565434 \n",
            "Step 26 of Epoch: 12; Loss 1.565537 \n",
            "Step 27 of Epoch: 12; Loss 1.565559 \n",
            "Step 28 of Epoch: 12; Loss 1.565353 \n",
            "Step 29 of Epoch: 12; Loss 1.565225 \n",
            "Step 30 of Epoch: 12; Loss 1.565018 \n",
            "Step 31 of Epoch: 12; Loss 1.565527 \n",
            "Step 32 of Epoch: 12; Loss 1.565682 \n",
            "Step 33 of Epoch: 12; Loss 1.565552 \n",
            "Step 34 of Epoch: 12; Loss 1.565545 \n",
            "Step 35 of Epoch: 12; Loss 1.565579 \n",
            "Step 36 of Epoch: 12; Loss 1.565847 \n",
            "Step 37 of Epoch: 12; Loss 1.566064 \n",
            "Step 38 of Epoch: 12; Loss 1.566435 \n",
            "Step 39 of Epoch: 12; Loss 1.566821 \n",
            "Step 40 of Epoch: 12; Loss 1.566786 \n",
            "Final Result for Epoch 12: Loss 1.566785; Val Acc 0.315031; Train Acc 0.359332\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 13; Loss 1.548467 \n",
            "Step 2 of Epoch: 13; Loss 1.544551 \n",
            "Step 3 of Epoch: 13; Loss 1.549746 \n",
            "Step 4 of Epoch: 13; Loss 1.550373 \n",
            "Step 5 of Epoch: 13; Loss 1.550970 \n",
            "Step 6 of Epoch: 13; Loss 1.551627 \n",
            "Step 7 of Epoch: 13; Loss 1.551042 \n",
            "Step 8 of Epoch: 13; Loss 1.551097 \n",
            "Step 9 of Epoch: 13; Loss 1.550584 \n",
            "Step 10 of Epoch: 13; Loss 1.549861 \n",
            "Step 11 of Epoch: 13; Loss 1.550355 \n",
            "Step 12 of Epoch: 13; Loss 1.551206 \n",
            "Step 13 of Epoch: 13; Loss 1.551114 \n",
            "Step 14 of Epoch: 13; Loss 1.551765 \n",
            "Step 15 of Epoch: 13; Loss 1.552245 \n",
            "Step 16 of Epoch: 13; Loss 1.552274 \n",
            "Step 17 of Epoch: 13; Loss 1.552594 \n",
            "Step 18 of Epoch: 13; Loss 1.552887 \n",
            "Step 19 of Epoch: 13; Loss 1.553006 \n",
            "Step 20 of Epoch: 13; Loss 1.553335 \n",
            "Step 21 of Epoch: 13; Loss 1.553572 \n",
            "Step 22 of Epoch: 13; Loss 1.554065 \n",
            "Step 23 of Epoch: 13; Loss 1.554306 \n",
            "Step 24 of Epoch: 13; Loss 1.554704 \n",
            "Step 25 of Epoch: 13; Loss 1.554909 \n",
            "Step 26 of Epoch: 13; Loss 1.554804 \n",
            "Step 27 of Epoch: 13; Loss 1.555157 \n",
            "Step 28 of Epoch: 13; Loss 1.555186 \n",
            "Step 29 of Epoch: 13; Loss 1.555828 \n",
            "Step 30 of Epoch: 13; Loss 1.555867 \n",
            "Step 31 of Epoch: 13; Loss 1.555996 \n",
            "Step 32 of Epoch: 13; Loss 1.555853 \n",
            "Step 33 of Epoch: 13; Loss 1.555705 \n",
            "Step 34 of Epoch: 13; Loss 1.556421 \n",
            "Step 35 of Epoch: 13; Loss 1.556737 \n",
            "Step 36 of Epoch: 13; Loss 1.556878 \n",
            "Step 37 of Epoch: 13; Loss 1.557074 \n",
            "Step 38 of Epoch: 13; Loss 1.557163 \n",
            "Step 39 of Epoch: 13; Loss 1.558002 \n",
            "Step 40 of Epoch: 13; Loss 1.558262 \n",
            "Final Result for Epoch 13: Loss 1.558255; Val Acc 0.309812; Train Acc 0.364292\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 14; Loss 1.543468 \n",
            "Step 2 of Epoch: 14; Loss 1.539789 \n",
            "Step 3 of Epoch: 14; Loss 1.538415 \n",
            "Step 4 of Epoch: 14; Loss 1.540963 \n",
            "Step 5 of Epoch: 14; Loss 1.540093 \n",
            "Step 6 of Epoch: 14; Loss 1.542219 \n",
            "Step 7 of Epoch: 14; Loss 1.541229 \n",
            "Step 8 of Epoch: 14; Loss 1.542359 \n",
            "Step 9 of Epoch: 14; Loss 1.541903 \n",
            "Step 10 of Epoch: 14; Loss 1.542554 \n",
            "Step 11 of Epoch: 14; Loss 1.542243 \n",
            "Step 12 of Epoch: 14; Loss 1.543015 \n",
            "Step 13 of Epoch: 14; Loss 1.544028 \n",
            "Step 14 of Epoch: 14; Loss 1.545583 \n",
            "Step 15 of Epoch: 14; Loss 1.545872 \n",
            "Step 16 of Epoch: 14; Loss 1.545963 \n",
            "Step 17 of Epoch: 14; Loss 1.546720 \n",
            "Step 18 of Epoch: 14; Loss 1.546259 \n",
            "Step 19 of Epoch: 14; Loss 1.546338 \n",
            "Step 20 of Epoch: 14; Loss 1.546765 \n",
            "Step 21 of Epoch: 14; Loss 1.546928 \n",
            "Step 22 of Epoch: 14; Loss 1.547726 \n",
            "Step 23 of Epoch: 14; Loss 1.548015 \n",
            "Step 24 of Epoch: 14; Loss 1.548119 \n",
            "Step 25 of Epoch: 14; Loss 1.548013 \n",
            "Step 26 of Epoch: 14; Loss 1.548472 \n",
            "Step 27 of Epoch: 14; Loss 1.548409 \n",
            "Step 28 of Epoch: 14; Loss 1.548671 \n",
            "Step 29 of Epoch: 14; Loss 1.548127 \n",
            "Step 30 of Epoch: 14; Loss 1.547826 \n",
            "Step 31 of Epoch: 14; Loss 1.548082 \n",
            "Step 32 of Epoch: 14; Loss 1.548311 \n",
            "Step 33 of Epoch: 14; Loss 1.548718 \n",
            "Step 34 of Epoch: 14; Loss 1.549304 \n",
            "Step 35 of Epoch: 14; Loss 1.549484 \n",
            "Step 36 of Epoch: 14; Loss 1.549867 \n",
            "Step 37 of Epoch: 14; Loss 1.549852 \n",
            "Step 38 of Epoch: 14; Loss 1.550127 \n",
            "Step 39 of Epoch: 14; Loss 1.550368 \n",
            "Step 40 of Epoch: 14; Loss 1.550290 \n",
            "Final Result for Epoch 14: Loss 1.550281; Val Acc 0.311375; Train Acc 0.367994\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 15; Loss 1.532474 \n",
            "Step 2 of Epoch: 15; Loss 1.531840 \n",
            "Step 3 of Epoch: 15; Loss 1.534164 \n",
            "Step 4 of Epoch: 15; Loss 1.534680 \n",
            "Step 5 of Epoch: 15; Loss 1.531667 \n",
            "Step 6 of Epoch: 15; Loss 1.532379 \n",
            "Step 7 of Epoch: 15; Loss 1.534041 \n",
            "Step 8 of Epoch: 15; Loss 1.535220 \n",
            "Step 9 of Epoch: 15; Loss 1.535834 \n",
            "Step 10 of Epoch: 15; Loss 1.536718 \n",
            "Step 11 of Epoch: 15; Loss 1.536691 \n",
            "Step 12 of Epoch: 15; Loss 1.536600 \n",
            "Step 13 of Epoch: 15; Loss 1.537814 \n",
            "Step 14 of Epoch: 15; Loss 1.539032 \n",
            "Step 15 of Epoch: 15; Loss 1.538816 \n",
            "Step 16 of Epoch: 15; Loss 1.538058 \n",
            "Step 17 of Epoch: 15; Loss 1.537928 \n",
            "Step 18 of Epoch: 15; Loss 1.537090 \n",
            "Step 19 of Epoch: 15; Loss 1.537295 \n",
            "Step 20 of Epoch: 15; Loss 1.537964 \n",
            "Step 21 of Epoch: 15; Loss 1.538046 \n",
            "Step 22 of Epoch: 15; Loss 1.538471 \n",
            "Step 23 of Epoch: 15; Loss 1.538688 \n",
            "Step 24 of Epoch: 15; Loss 1.539239 \n",
            "Step 25 of Epoch: 15; Loss 1.539337 \n",
            "Step 26 of Epoch: 15; Loss 1.539429 \n",
            "Step 27 of Epoch: 15; Loss 1.539610 \n",
            "Step 28 of Epoch: 15; Loss 1.539799 \n",
            "Step 29 of Epoch: 15; Loss 1.539758 \n",
            "Step 30 of Epoch: 15; Loss 1.540697 \n",
            "Step 31 of Epoch: 15; Loss 1.540727 \n",
            "Step 32 of Epoch: 15; Loss 1.540933 \n",
            "Step 33 of Epoch: 15; Loss 1.541091 \n",
            "Step 34 of Epoch: 15; Loss 1.541382 \n",
            "Step 35 of Epoch: 15; Loss 1.541790 \n",
            "Step 36 of Epoch: 15; Loss 1.541821 \n",
            "Step 37 of Epoch: 15; Loss 1.541849 \n",
            "Step 38 of Epoch: 15; Loss 1.542274 \n",
            "Step 39 of Epoch: 15; Loss 1.542205 \n",
            "Step 40 of Epoch: 15; Loss 1.542095 \n",
            "Final Result for Epoch 15: Loss 1.542154; Val Acc 0.311031; Train Acc 0.371200\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 16; Loss 1.533446 \n",
            "Step 2 of Epoch: 16; Loss 1.518187 \n",
            "Step 3 of Epoch: 16; Loss 1.521485 \n",
            "Step 4 of Epoch: 16; Loss 1.524593 \n",
            "Step 5 of Epoch: 16; Loss 1.522509 \n",
            "Step 6 of Epoch: 16; Loss 1.523828 \n",
            "Step 7 of Epoch: 16; Loss 1.524230 \n",
            "Step 8 of Epoch: 16; Loss 1.524562 \n",
            "Step 9 of Epoch: 16; Loss 1.524199 \n",
            "Step 10 of Epoch: 16; Loss 1.524973 \n",
            "Step 11 of Epoch: 16; Loss 1.525193 \n",
            "Step 12 of Epoch: 16; Loss 1.524913 \n",
            "Step 13 of Epoch: 16; Loss 1.525405 \n",
            "Step 14 of Epoch: 16; Loss 1.526213 \n",
            "Step 15 of Epoch: 16; Loss 1.526519 \n",
            "Step 16 of Epoch: 16; Loss 1.527226 \n",
            "Step 17 of Epoch: 16; Loss 1.527753 \n",
            "Step 18 of Epoch: 16; Loss 1.528215 \n",
            "Step 19 of Epoch: 16; Loss 1.528927 \n",
            "Step 20 of Epoch: 16; Loss 1.528655 \n",
            "Step 21 of Epoch: 16; Loss 1.528867 \n",
            "Step 22 of Epoch: 16; Loss 1.528810 \n",
            "Step 23 of Epoch: 16; Loss 1.528627 \n",
            "Step 24 of Epoch: 16; Loss 1.529771 \n",
            "Step 25 of Epoch: 16; Loss 1.530333 \n",
            "Step 26 of Epoch: 16; Loss 1.530625 \n",
            "Step 27 of Epoch: 16; Loss 1.530855 \n",
            "Step 28 of Epoch: 16; Loss 1.531379 \n",
            "Step 29 of Epoch: 16; Loss 1.531103 \n",
            "Step 30 of Epoch: 16; Loss 1.531063 \n",
            "Step 31 of Epoch: 16; Loss 1.531633 \n",
            "Step 32 of Epoch: 16; Loss 1.531775 \n",
            "Step 33 of Epoch: 16; Loss 1.532400 \n",
            "Step 34 of Epoch: 16; Loss 1.532851 \n",
            "Step 35 of Epoch: 16; Loss 1.532808 \n",
            "Step 36 of Epoch: 16; Loss 1.533073 \n",
            "Step 37 of Epoch: 16; Loss 1.533613 \n",
            "Step 38 of Epoch: 16; Loss 1.533796 \n",
            "Step 39 of Epoch: 16; Loss 1.533897 \n",
            "Step 40 of Epoch: 16; Loss 1.534224 \n",
            "Final Result for Epoch 16: Loss 1.534227; Val Acc 0.311906; Train Acc 0.377222\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 17; Loss 1.511300 \n",
            "Step 2 of Epoch: 17; Loss 1.507412 \n",
            "Step 3 of Epoch: 17; Loss 1.510930 \n",
            "Step 4 of Epoch: 17; Loss 1.513977 \n",
            "Step 5 of Epoch: 17; Loss 1.516033 \n",
            "Step 6 of Epoch: 17; Loss 1.515537 \n",
            "Step 7 of Epoch: 17; Loss 1.516687 \n",
            "Step 8 of Epoch: 17; Loss 1.516813 \n",
            "Step 9 of Epoch: 17; Loss 1.518296 \n",
            "Step 10 of Epoch: 17; Loss 1.517923 \n",
            "Step 11 of Epoch: 17; Loss 1.516861 \n",
            "Step 12 of Epoch: 17; Loss 1.517489 \n",
            "Step 13 of Epoch: 17; Loss 1.518112 \n",
            "Step 14 of Epoch: 17; Loss 1.518129 \n",
            "Step 15 of Epoch: 17; Loss 1.518993 \n",
            "Step 16 of Epoch: 17; Loss 1.519756 \n",
            "Step 17 of Epoch: 17; Loss 1.520122 \n",
            "Step 18 of Epoch: 17; Loss 1.521456 \n",
            "Step 19 of Epoch: 17; Loss 1.522228 \n",
            "Step 20 of Epoch: 17; Loss 1.522608 \n",
            "Step 21 of Epoch: 17; Loss 1.522708 \n",
            "Step 22 of Epoch: 17; Loss 1.522316 \n",
            "Step 23 of Epoch: 17; Loss 1.522989 \n",
            "Step 24 of Epoch: 17; Loss 1.522965 \n",
            "Step 25 of Epoch: 17; Loss 1.523446 \n",
            "Step 26 of Epoch: 17; Loss 1.523351 \n",
            "Step 27 of Epoch: 17; Loss 1.523538 \n",
            "Step 28 of Epoch: 17; Loss 1.523937 \n",
            "Step 29 of Epoch: 17; Loss 1.524192 \n",
            "Step 30 of Epoch: 17; Loss 1.524449 \n",
            "Step 31 of Epoch: 17; Loss 1.524367 \n",
            "Step 32 of Epoch: 17; Loss 1.524272 \n",
            "Step 33 of Epoch: 17; Loss 1.524627 \n",
            "Step 34 of Epoch: 17; Loss 1.524811 \n",
            "Step 35 of Epoch: 17; Loss 1.524843 \n",
            "Step 36 of Epoch: 17; Loss 1.525084 \n",
            "Step 37 of Epoch: 17; Loss 1.524951 \n",
            "Step 38 of Epoch: 17; Loss 1.525544 \n",
            "Step 39 of Epoch: 17; Loss 1.525608 \n",
            "Step 40 of Epoch: 17; Loss 1.526194 \n",
            "Final Result for Epoch 17: Loss 1.526210; Val Acc 0.310219; Train Acc 0.379679\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 18; Loss 1.503627 \n",
            "Step 2 of Epoch: 18; Loss 1.513762 \n",
            "Step 3 of Epoch: 18; Loss 1.510362 \n",
            "Step 4 of Epoch: 18; Loss 1.509269 \n",
            "Step 5 of Epoch: 18; Loss 1.507167 \n",
            "Step 6 of Epoch: 18; Loss 1.507455 \n",
            "Step 7 of Epoch: 18; Loss 1.509566 \n",
            "Step 8 of Epoch: 18; Loss 1.511365 \n",
            "Step 9 of Epoch: 18; Loss 1.511728 \n",
            "Step 10 of Epoch: 18; Loss 1.512626 \n",
            "Step 11 of Epoch: 18; Loss 1.513767 \n",
            "Step 12 of Epoch: 18; Loss 1.513429 \n",
            "Step 13 of Epoch: 18; Loss 1.513933 \n",
            "Step 14 of Epoch: 18; Loss 1.514016 \n",
            "Step 15 of Epoch: 18; Loss 1.514944 \n",
            "Step 16 of Epoch: 18; Loss 1.515510 \n",
            "Step 17 of Epoch: 18; Loss 1.516367 \n",
            "Step 18 of Epoch: 18; Loss 1.515689 \n",
            "Step 19 of Epoch: 18; Loss 1.515536 \n",
            "Step 20 of Epoch: 18; Loss 1.515594 \n",
            "Step 21 of Epoch: 18; Loss 1.516431 \n",
            "Step 22 of Epoch: 18; Loss 1.516062 \n",
            "Step 23 of Epoch: 18; Loss 1.516342 \n",
            "Step 24 of Epoch: 18; Loss 1.516242 \n",
            "Step 25 of Epoch: 18; Loss 1.516252 \n",
            "Step 26 of Epoch: 18; Loss 1.517078 \n",
            "Step 27 of Epoch: 18; Loss 1.517499 \n",
            "Step 28 of Epoch: 18; Loss 1.517748 \n",
            "Step 29 of Epoch: 18; Loss 1.517828 \n",
            "Step 30 of Epoch: 18; Loss 1.517341 \n",
            "Step 31 of Epoch: 18; Loss 1.517283 \n",
            "Step 32 of Epoch: 18; Loss 1.517265 \n",
            "Step 33 of Epoch: 18; Loss 1.517543 \n",
            "Step 34 of Epoch: 18; Loss 1.517814 \n",
            "Step 35 of Epoch: 18; Loss 1.517581 \n",
            "Step 36 of Epoch: 18; Loss 1.517400 \n",
            "Step 37 of Epoch: 18; Loss 1.517967 \n",
            "Step 38 of Epoch: 18; Loss 1.518396 \n",
            "Step 39 of Epoch: 18; Loss 1.518515 \n",
            "Step 40 of Epoch: 18; Loss 1.518433 \n",
            "Final Result for Epoch 18: Loss 1.518432; Val Acc 0.310219; Train Acc 0.385896\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 19; Loss 1.503772 \n",
            "Step 2 of Epoch: 19; Loss 1.508175 \n",
            "Step 3 of Epoch: 19; Loss 1.505219 \n",
            "Step 4 of Epoch: 19; Loss 1.503930 \n",
            "Step 5 of Epoch: 19; Loss 1.504637 \n",
            "Step 6 of Epoch: 19; Loss 1.505474 \n",
            "Step 7 of Epoch: 19; Loss 1.505643 \n",
            "Step 8 of Epoch: 19; Loss 1.505158 \n",
            "Step 9 of Epoch: 19; Loss 1.506197 \n",
            "Step 10 of Epoch: 19; Loss 1.506332 \n",
            "Step 11 of Epoch: 19; Loss 1.505679 \n",
            "Step 12 of Epoch: 19; Loss 1.505354 \n",
            "Step 13 of Epoch: 19; Loss 1.505306 \n",
            "Step 14 of Epoch: 19; Loss 1.505777 \n",
            "Step 15 of Epoch: 19; Loss 1.505122 \n",
            "Step 16 of Epoch: 19; Loss 1.504271 \n",
            "Step 17 of Epoch: 19; Loss 1.504346 \n",
            "Step 18 of Epoch: 19; Loss 1.504338 \n",
            "Step 19 of Epoch: 19; Loss 1.504400 \n",
            "Step 20 of Epoch: 19; Loss 1.504894 \n",
            "Step 21 of Epoch: 19; Loss 1.505699 \n",
            "Step 22 of Epoch: 19; Loss 1.506459 \n",
            "Step 23 of Epoch: 19; Loss 1.507389 \n",
            "Step 24 of Epoch: 19; Loss 1.507867 \n",
            "Step 25 of Epoch: 19; Loss 1.507980 \n",
            "Step 26 of Epoch: 19; Loss 1.508297 \n",
            "Step 27 of Epoch: 19; Loss 1.507813 \n",
            "Step 28 of Epoch: 19; Loss 1.507868 \n",
            "Step 29 of Epoch: 19; Loss 1.508195 \n",
            "Step 30 of Epoch: 19; Loss 1.508431 \n",
            "Step 31 of Epoch: 19; Loss 1.508519 \n",
            "Step 32 of Epoch: 19; Loss 1.508597 \n",
            "Step 33 of Epoch: 19; Loss 1.509049 \n",
            "Step 34 of Epoch: 19; Loss 1.509238 \n",
            "Step 35 of Epoch: 19; Loss 1.509618 \n",
            "Step 36 of Epoch: 19; Loss 1.509627 \n",
            "Step 37 of Epoch: 19; Loss 1.509705 \n",
            "Step 38 of Epoch: 19; Loss 1.510161 \n",
            "Step 39 of Epoch: 19; Loss 1.510389 \n",
            "Step 40 of Epoch: 19; Loss 1.510665 \n",
            "Final Result for Epoch 19: Loss 1.510682; Val Acc 0.310625; Train Acc 0.388126\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 20; Loss 1.488097 \n",
            "Step 2 of Epoch: 20; Loss 1.487330 \n",
            "Step 3 of Epoch: 20; Loss 1.487305 \n",
            "Step 4 of Epoch: 20; Loss 1.488834 \n",
            "Step 5 of Epoch: 20; Loss 1.489211 \n",
            "Step 6 of Epoch: 20; Loss 1.490523 \n",
            "Step 7 of Epoch: 20; Loss 1.490558 \n",
            "Step 8 of Epoch: 20; Loss 1.492122 \n",
            "Step 9 of Epoch: 20; Loss 1.493485 \n",
            "Step 10 of Epoch: 20; Loss 1.494103 \n",
            "Step 11 of Epoch: 20; Loss 1.496825 \n",
            "Step 12 of Epoch: 20; Loss 1.497052 \n",
            "Step 13 of Epoch: 20; Loss 1.497905 \n",
            "Step 14 of Epoch: 20; Loss 1.498454 \n",
            "Step 15 of Epoch: 20; Loss 1.498167 \n",
            "Step 16 of Epoch: 20; Loss 1.498773 \n",
            "Step 17 of Epoch: 20; Loss 1.498564 \n",
            "Step 18 of Epoch: 20; Loss 1.498258 \n",
            "Step 19 of Epoch: 20; Loss 1.499209 \n",
            "Step 20 of Epoch: 20; Loss 1.499448 \n",
            "Step 21 of Epoch: 20; Loss 1.499101 \n",
            "Step 22 of Epoch: 20; Loss 1.499709 \n",
            "Step 23 of Epoch: 20; Loss 1.499772 \n",
            "Step 24 of Epoch: 20; Loss 1.500081 \n",
            "Step 25 of Epoch: 20; Loss 1.500459 \n",
            "Step 26 of Epoch: 20; Loss 1.500893 \n",
            "Step 27 of Epoch: 20; Loss 1.500905 \n",
            "Step 28 of Epoch: 20; Loss 1.501129 \n",
            "Step 29 of Epoch: 20; Loss 1.500913 \n",
            "Step 30 of Epoch: 20; Loss 1.501206 \n",
            "Step 31 of Epoch: 20; Loss 1.501195 \n",
            "Step 32 of Epoch: 20; Loss 1.501555 \n",
            "Step 33 of Epoch: 20; Loss 1.501734 \n",
            "Step 34 of Epoch: 20; Loss 1.501724 \n",
            "Step 35 of Epoch: 20; Loss 1.502412 \n",
            "Step 36 of Epoch: 20; Loss 1.502285 \n",
            "Step 37 of Epoch: 20; Loss 1.502562 \n",
            "Step 38 of Epoch: 20; Loss 1.502903 \n",
            "Step 39 of Epoch: 20; Loss 1.503144 \n",
            "Step 40 of Epoch: 20; Loss 1.503274 \n",
            "Final Result for Epoch 20: Loss 1.503271; Val Acc 0.308906; Train Acc 0.392726\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 21; Loss 1.471219 \n",
            "Step 2 of Epoch: 21; Loss 1.471768 \n",
            "Step 3 of Epoch: 21; Loss 1.478073 \n",
            "Step 4 of Epoch: 21; Loss 1.479808 \n",
            "Step 5 of Epoch: 21; Loss 1.483658 \n",
            "Step 6 of Epoch: 21; Loss 1.483399 \n",
            "Step 7 of Epoch: 21; Loss 1.486003 \n",
            "Step 8 of Epoch: 21; Loss 1.487713 \n",
            "Step 9 of Epoch: 21; Loss 1.486866 \n",
            "Step 10 of Epoch: 21; Loss 1.487629 \n",
            "Step 11 of Epoch: 21; Loss 1.487086 \n",
            "Step 12 of Epoch: 21; Loss 1.487114 \n",
            "Step 13 of Epoch: 21; Loss 1.487268 \n",
            "Step 14 of Epoch: 21; Loss 1.487533 \n",
            "Step 15 of Epoch: 21; Loss 1.488221 \n",
            "Step 16 of Epoch: 21; Loss 1.489029 \n",
            "Step 17 of Epoch: 21; Loss 1.489027 \n",
            "Step 18 of Epoch: 21; Loss 1.490381 \n",
            "Step 19 of Epoch: 21; Loss 1.491292 \n",
            "Step 20 of Epoch: 21; Loss 1.491576 \n",
            "Step 21 of Epoch: 21; Loss 1.491340 \n",
            "Step 22 of Epoch: 21; Loss 1.491355 \n",
            "Step 23 of Epoch: 21; Loss 1.491361 \n",
            "Step 24 of Epoch: 21; Loss 1.491672 \n",
            "Step 25 of Epoch: 21; Loss 1.492479 \n",
            "Step 26 of Epoch: 21; Loss 1.492885 \n",
            "Step 27 of Epoch: 21; Loss 1.492880 \n",
            "Step 28 of Epoch: 21; Loss 1.492660 \n",
            "Step 29 of Epoch: 21; Loss 1.492390 \n",
            "Step 30 of Epoch: 21; Loss 1.492894 \n",
            "Step 31 of Epoch: 21; Loss 1.493314 \n",
            "Step 32 of Epoch: 21; Loss 1.493760 \n",
            "Step 33 of Epoch: 21; Loss 1.494009 \n",
            "Step 34 of Epoch: 21; Loss 1.494188 \n",
            "Step 35 of Epoch: 21; Loss 1.494197 \n",
            "Step 36 of Epoch: 21; Loss 1.494759 \n",
            "Step 37 of Epoch: 21; Loss 1.494777 \n",
            "Step 38 of Epoch: 21; Loss 1.495517 \n",
            "Step 39 of Epoch: 21; Loss 1.495704 \n",
            "Step 40 of Epoch: 21; Loss 1.495935 \n",
            "Final Result for Epoch 21: Loss 1.495993; Val Acc 0.308156; Train Acc 0.395432\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 22; Loss 1.468761 \n",
            "Step 2 of Epoch: 22; Loss 1.469494 \n",
            "Step 3 of Epoch: 22; Loss 1.475633 \n",
            "Step 4 of Epoch: 22; Loss 1.476633 \n",
            "Step 5 of Epoch: 22; Loss 1.479744 \n",
            "Step 6 of Epoch: 22; Loss 1.477662 \n",
            "Step 7 of Epoch: 22; Loss 1.478171 \n",
            "Step 8 of Epoch: 22; Loss 1.478283 \n",
            "Step 9 of Epoch: 22; Loss 1.477976 \n",
            "Step 10 of Epoch: 22; Loss 1.479027 \n",
            "Step 11 of Epoch: 22; Loss 1.479253 \n",
            "Step 12 of Epoch: 22; Loss 1.479144 \n",
            "Step 13 of Epoch: 22; Loss 1.480235 \n",
            "Step 14 of Epoch: 22; Loss 1.480247 \n",
            "Step 15 of Epoch: 22; Loss 1.481159 \n",
            "Step 16 of Epoch: 22; Loss 1.482680 \n",
            "Step 17 of Epoch: 22; Loss 1.482703 \n",
            "Step 18 of Epoch: 22; Loss 1.484088 \n",
            "Step 19 of Epoch: 22; Loss 1.484593 \n",
            "Step 20 of Epoch: 22; Loss 1.484978 \n",
            "Step 21 of Epoch: 22; Loss 1.484810 \n",
            "Step 22 of Epoch: 22; Loss 1.484875 \n",
            "Step 23 of Epoch: 22; Loss 1.484025 \n",
            "Step 24 of Epoch: 22; Loss 1.485063 \n",
            "Step 25 of Epoch: 22; Loss 1.485539 \n",
            "Step 26 of Epoch: 22; Loss 1.485827 \n",
            "Step 27 of Epoch: 22; Loss 1.485951 \n",
            "Step 28 of Epoch: 22; Loss 1.486184 \n",
            "Step 29 of Epoch: 22; Loss 1.486415 \n",
            "Step 30 of Epoch: 22; Loss 1.486862 \n",
            "Step 31 of Epoch: 22; Loss 1.487009 \n",
            "Step 32 of Epoch: 22; Loss 1.486917 \n",
            "Step 33 of Epoch: 22; Loss 1.487420 \n",
            "Step 34 of Epoch: 22; Loss 1.487797 \n",
            "Step 35 of Epoch: 22; Loss 1.487858 \n",
            "Step 36 of Epoch: 22; Loss 1.488418 \n",
            "Step 37 of Epoch: 22; Loss 1.488448 \n",
            "Step 38 of Epoch: 22; Loss 1.488386 \n",
            "Step 39 of Epoch: 22; Loss 1.488512 \n",
            "Step 40 of Epoch: 22; Loss 1.488729 \n",
            "Final Result for Epoch 22: Loss 1.488713; Val Acc 0.307344; Train Acc 0.397791\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 23; Loss 1.465377 \n",
            "Step 2 of Epoch: 23; Loss 1.458836 \n",
            "Step 3 of Epoch: 23; Loss 1.461901 \n",
            "Step 4 of Epoch: 23; Loss 1.466983 \n",
            "Step 5 of Epoch: 23; Loss 1.464710 \n",
            "Step 6 of Epoch: 23; Loss 1.466089 \n",
            "Step 7 of Epoch: 23; Loss 1.467493 \n",
            "Step 8 of Epoch: 23; Loss 1.467239 \n",
            "Step 9 of Epoch: 23; Loss 1.470239 \n",
            "Step 10 of Epoch: 23; Loss 1.472470 \n",
            "Step 11 of Epoch: 23; Loss 1.473146 \n",
            "Step 12 of Epoch: 23; Loss 1.473019 \n",
            "Step 13 of Epoch: 23; Loss 1.473062 \n",
            "Step 14 of Epoch: 23; Loss 1.472402 \n",
            "Step 15 of Epoch: 23; Loss 1.472470 \n",
            "Step 16 of Epoch: 23; Loss 1.473002 \n",
            "Step 17 of Epoch: 23; Loss 1.474060 \n",
            "Step 18 of Epoch: 23; Loss 1.474251 \n",
            "Step 19 of Epoch: 23; Loss 1.473883 \n",
            "Step 20 of Epoch: 23; Loss 1.474280 \n",
            "Step 21 of Epoch: 23; Loss 1.474829 \n",
            "Step 22 of Epoch: 23; Loss 1.475180 \n",
            "Step 23 of Epoch: 23; Loss 1.475742 \n",
            "Step 24 of Epoch: 23; Loss 1.476537 \n",
            "Step 25 of Epoch: 23; Loss 1.477126 \n",
            "Step 26 of Epoch: 23; Loss 1.478026 \n",
            "Step 27 of Epoch: 23; Loss 1.478137 \n",
            "Step 28 of Epoch: 23; Loss 1.478113 \n",
            "Step 29 of Epoch: 23; Loss 1.478384 \n",
            "Step 30 of Epoch: 23; Loss 1.478769 \n",
            "Step 31 of Epoch: 23; Loss 1.479006 \n",
            "Step 32 of Epoch: 23; Loss 1.479049 \n",
            "Step 33 of Epoch: 23; Loss 1.479804 \n",
            "Step 34 of Epoch: 23; Loss 1.480057 \n",
            "Step 35 of Epoch: 23; Loss 1.480523 \n",
            "Step 36 of Epoch: 23; Loss 1.480417 \n",
            "Step 37 of Epoch: 23; Loss 1.480889 \n",
            "Step 38 of Epoch: 23; Loss 1.481433 \n",
            "Step 39 of Epoch: 23; Loss 1.481973 \n",
            "Step 40 of Epoch: 23; Loss 1.482076 \n",
            "Final Result for Epoch 23: Loss 1.482120; Val Acc 0.303594; Train Acc 0.400560\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 24; Loss 1.449241 \n",
            "Step 2 of Epoch: 24; Loss 1.452311 \n",
            "Step 3 of Epoch: 24; Loss 1.452509 \n",
            "Step 4 of Epoch: 24; Loss 1.456679 \n",
            "Step 5 of Epoch: 24; Loss 1.459959 \n",
            "Step 6 of Epoch: 24; Loss 1.458955 \n",
            "Step 7 of Epoch: 24; Loss 1.461796 \n",
            "Step 8 of Epoch: 24; Loss 1.461083 \n",
            "Step 9 of Epoch: 24; Loss 1.462749 \n",
            "Step 10 of Epoch: 24; Loss 1.463228 \n",
            "Step 11 of Epoch: 24; Loss 1.463995 \n",
            "Step 12 of Epoch: 24; Loss 1.464057 \n",
            "Step 13 of Epoch: 24; Loss 1.464714 \n",
            "Step 14 of Epoch: 24; Loss 1.465370 \n",
            "Step 15 of Epoch: 24; Loss 1.466361 \n",
            "Step 16 of Epoch: 24; Loss 1.466030 \n",
            "Step 17 of Epoch: 24; Loss 1.467112 \n",
            "Step 18 of Epoch: 24; Loss 1.467168 \n",
            "Step 19 of Epoch: 24; Loss 1.467873 \n",
            "Step 20 of Epoch: 24; Loss 1.468094 \n",
            "Step 21 of Epoch: 24; Loss 1.468480 \n",
            "Step 22 of Epoch: 24; Loss 1.468825 \n",
            "Step 23 of Epoch: 24; Loss 1.468880 \n",
            "Step 24 of Epoch: 24; Loss 1.469392 \n",
            "Step 25 of Epoch: 24; Loss 1.469727 \n",
            "Step 26 of Epoch: 24; Loss 1.469872 \n",
            "Step 27 of Epoch: 24; Loss 1.470324 \n",
            "Step 28 of Epoch: 24; Loss 1.471160 \n",
            "Step 29 of Epoch: 24; Loss 1.472182 \n",
            "Step 30 of Epoch: 24; Loss 1.472867 \n",
            "Step 31 of Epoch: 24; Loss 1.472829 \n",
            "Step 32 of Epoch: 24; Loss 1.473314 \n",
            "Step 33 of Epoch: 24; Loss 1.474227 \n",
            "Step 34 of Epoch: 24; Loss 1.474229 \n",
            "Step 35 of Epoch: 24; Loss 1.473997 \n",
            "Step 36 of Epoch: 24; Loss 1.474387 \n",
            "Step 37 of Epoch: 24; Loss 1.474687 \n",
            "Step 38 of Epoch: 24; Loss 1.474657 \n",
            "Step 39 of Epoch: 24; Loss 1.475000 \n",
            "Step 40 of Epoch: 24; Loss 1.475242 \n",
            "Final Result for Epoch 24: Loss 1.475226; Val Acc 0.305375; Train Acc 0.403805\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 25; Loss 1.445232 \n",
            "Step 2 of Epoch: 25; Loss 1.441948 \n",
            "Step 3 of Epoch: 25; Loss 1.445907 \n",
            "Step 4 of Epoch: 25; Loss 1.444161 \n",
            "Step 5 of Epoch: 25; Loss 1.448185 \n",
            "Step 6 of Epoch: 25; Loss 1.449832 \n",
            "Step 7 of Epoch: 25; Loss 1.451019 \n",
            "Step 8 of Epoch: 25; Loss 1.453500 \n",
            "Step 9 of Epoch: 25; Loss 1.456195 \n",
            "Step 10 of Epoch: 25; Loss 1.455915 \n",
            "Step 11 of Epoch: 25; Loss 1.456264 \n",
            "Step 12 of Epoch: 25; Loss 1.456334 \n",
            "Step 13 of Epoch: 25; Loss 1.457298 \n",
            "Step 14 of Epoch: 25; Loss 1.458059 \n",
            "Step 15 of Epoch: 25; Loss 1.459305 \n",
            "Step 16 of Epoch: 25; Loss 1.459575 \n",
            "Step 17 of Epoch: 25; Loss 1.459692 \n",
            "Step 18 of Epoch: 25; Loss 1.460326 \n",
            "Step 19 of Epoch: 25; Loss 1.461013 \n",
            "Step 20 of Epoch: 25; Loss 1.461345 \n",
            "Step 21 of Epoch: 25; Loss 1.461379 \n",
            "Step 22 of Epoch: 25; Loss 1.461107 \n",
            "Step 23 of Epoch: 25; Loss 1.461790 \n",
            "Step 24 of Epoch: 25; Loss 1.462672 \n",
            "Step 25 of Epoch: 25; Loss 1.462988 \n",
            "Step 26 of Epoch: 25; Loss 1.463385 \n",
            "Step 27 of Epoch: 25; Loss 1.463606 \n",
            "Step 28 of Epoch: 25; Loss 1.464077 \n",
            "Step 29 of Epoch: 25; Loss 1.464736 \n",
            "Step 30 of Epoch: 25; Loss 1.465193 \n",
            "Step 31 of Epoch: 25; Loss 1.465496 \n",
            "Step 32 of Epoch: 25; Loss 1.465545 \n",
            "Step 33 of Epoch: 25; Loss 1.465808 \n",
            "Step 34 of Epoch: 25; Loss 1.466075 \n",
            "Step 35 of Epoch: 25; Loss 1.466278 \n",
            "Step 36 of Epoch: 25; Loss 1.466767 \n",
            "Step 37 of Epoch: 25; Loss 1.466948 \n",
            "Step 38 of Epoch: 25; Loss 1.467367 \n",
            "Step 39 of Epoch: 25; Loss 1.467720 \n",
            "Step 40 of Epoch: 25; Loss 1.468235 \n",
            "Final Result for Epoch 25: Loss 1.468228; Val Acc 0.305906; Train Acc 0.407308\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 26; Loss 1.441315 \n",
            "Step 2 of Epoch: 26; Loss 1.441365 \n",
            "Step 3 of Epoch: 26; Loss 1.440069 \n",
            "Step 4 of Epoch: 26; Loss 1.443348 \n",
            "Step 5 of Epoch: 26; Loss 1.443411 \n",
            "Step 6 of Epoch: 26; Loss 1.443618 \n",
            "Step 7 of Epoch: 26; Loss 1.444954 \n",
            "Step 8 of Epoch: 26; Loss 1.446585 \n",
            "Step 9 of Epoch: 26; Loss 1.447185 \n",
            "Step 10 of Epoch: 26; Loss 1.448589 \n",
            "Step 11 of Epoch: 26; Loss 1.448725 \n",
            "Step 12 of Epoch: 26; Loss 1.450315 \n",
            "Step 13 of Epoch: 26; Loss 1.451333 \n",
            "Step 14 of Epoch: 26; Loss 1.452460 \n",
            "Step 15 of Epoch: 26; Loss 1.452692 \n",
            "Step 16 of Epoch: 26; Loss 1.453127 \n",
            "Step 17 of Epoch: 26; Loss 1.453789 \n",
            "Step 18 of Epoch: 26; Loss 1.453807 \n",
            "Step 19 of Epoch: 26; Loss 1.454516 \n",
            "Step 20 of Epoch: 26; Loss 1.454866 \n",
            "Step 21 of Epoch: 26; Loss 1.455424 \n",
            "Step 22 of Epoch: 26; Loss 1.456534 \n",
            "Step 23 of Epoch: 26; Loss 1.457094 \n",
            "Step 24 of Epoch: 26; Loss 1.457044 \n",
            "Step 25 of Epoch: 26; Loss 1.457539 \n",
            "Step 26 of Epoch: 26; Loss 1.457958 \n",
            "Step 27 of Epoch: 26; Loss 1.458530 \n",
            "Step 28 of Epoch: 26; Loss 1.458875 \n",
            "Step 29 of Epoch: 26; Loss 1.459268 \n",
            "Step 30 of Epoch: 26; Loss 1.459855 \n",
            "Step 31 of Epoch: 26; Loss 1.460465 \n",
            "Step 32 of Epoch: 26; Loss 1.460957 \n",
            "Step 33 of Epoch: 26; Loss 1.460962 \n",
            "Step 34 of Epoch: 26; Loss 1.460948 \n",
            "Step 35 of Epoch: 26; Loss 1.461257 \n",
            "Step 36 of Epoch: 26; Loss 1.461567 \n",
            "Step 37 of Epoch: 26; Loss 1.462002 \n",
            "Step 38 of Epoch: 26; Loss 1.462037 \n",
            "Step 39 of Epoch: 26; Loss 1.462293 \n",
            "Step 40 of Epoch: 26; Loss 1.462217 \n",
            "Final Result for Epoch 26: Loss 1.462216; Val Acc 0.305812; Train Acc 0.408870\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 27; Loss 1.442049 \n",
            "Step 2 of Epoch: 27; Loss 1.444203 \n",
            "Step 3 of Epoch: 27; Loss 1.441736 \n",
            "Step 4 of Epoch: 27; Loss 1.442187 \n",
            "Step 5 of Epoch: 27; Loss 1.442928 \n",
            "Step 6 of Epoch: 27; Loss 1.443358 \n",
            "Step 7 of Epoch: 27; Loss 1.441715 \n",
            "Step 8 of Epoch: 27; Loss 1.441952 \n",
            "Step 9 of Epoch: 27; Loss 1.441932 \n",
            "Step 10 of Epoch: 27; Loss 1.442518 \n",
            "Step 11 of Epoch: 27; Loss 1.443370 \n",
            "Step 12 of Epoch: 27; Loss 1.444375 \n",
            "Step 13 of Epoch: 27; Loss 1.444081 \n",
            "Step 14 of Epoch: 27; Loss 1.444659 \n",
            "Step 15 of Epoch: 27; Loss 1.445008 \n",
            "Step 16 of Epoch: 27; Loss 1.445761 \n",
            "Step 17 of Epoch: 27; Loss 1.446567 \n",
            "Step 18 of Epoch: 27; Loss 1.447424 \n",
            "Step 19 of Epoch: 27; Loss 1.448631 \n",
            "Step 20 of Epoch: 27; Loss 1.449681 \n",
            "Step 21 of Epoch: 27; Loss 1.449697 \n",
            "Step 22 of Epoch: 27; Loss 1.449951 \n",
            "Step 23 of Epoch: 27; Loss 1.450536 \n",
            "Step 24 of Epoch: 27; Loss 1.450514 \n",
            "Step 25 of Epoch: 27; Loss 1.450901 \n",
            "Step 26 of Epoch: 27; Loss 1.451519 \n",
            "Step 27 of Epoch: 27; Loss 1.451960 \n",
            "Step 28 of Epoch: 27; Loss 1.452448 \n",
            "Step 29 of Epoch: 27; Loss 1.452528 \n",
            "Step 30 of Epoch: 27; Loss 1.452797 \n",
            "Step 31 of Epoch: 27; Loss 1.453176 \n",
            "Step 32 of Epoch: 27; Loss 1.453769 \n",
            "Step 33 of Epoch: 27; Loss 1.454201 \n",
            "Step 34 of Epoch: 27; Loss 1.454185 \n",
            "Step 35 of Epoch: 27; Loss 1.454326 \n",
            "Step 36 of Epoch: 27; Loss 1.454482 \n",
            "Step 37 of Epoch: 27; Loss 1.454841 \n",
            "Step 38 of Epoch: 27; Loss 1.455213 \n",
            "Step 39 of Epoch: 27; Loss 1.455394 \n",
            "Step 40 of Epoch: 27; Loss 1.455839 \n",
            "Final Result for Epoch 27: Loss 1.455864; Val Acc 0.301563; Train Acc 0.412006\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 28; Loss 1.425796 \n",
            "Step 2 of Epoch: 28; Loss 1.425805 \n",
            "Step 3 of Epoch: 28; Loss 1.426994 \n",
            "Step 4 of Epoch: 28; Loss 1.433613 \n",
            "Step 5 of Epoch: 28; Loss 1.436620 \n",
            "Step 6 of Epoch: 28; Loss 1.435514 \n",
            "Step 7 of Epoch: 28; Loss 1.438240 \n",
            "Step 8 of Epoch: 28; Loss 1.437188 \n",
            "Step 9 of Epoch: 28; Loss 1.437383 \n",
            "Step 10 of Epoch: 28; Loss 1.439685 \n",
            "Step 11 of Epoch: 28; Loss 1.437827 \n",
            "Step 12 of Epoch: 28; Loss 1.439222 \n",
            "Step 13 of Epoch: 28; Loss 1.439995 \n",
            "Step 14 of Epoch: 28; Loss 1.440404 \n",
            "Step 15 of Epoch: 28; Loss 1.440977 \n",
            "Step 16 of Epoch: 28; Loss 1.440715 \n",
            "Step 17 of Epoch: 28; Loss 1.440701 \n",
            "Step 18 of Epoch: 28; Loss 1.440364 \n",
            "Step 19 of Epoch: 28; Loss 1.440990 \n",
            "Step 20 of Epoch: 28; Loss 1.442286 \n",
            "Step 21 of Epoch: 28; Loss 1.442656 \n",
            "Step 22 of Epoch: 28; Loss 1.443051 \n",
            "Step 23 of Epoch: 28; Loss 1.443381 \n",
            "Step 24 of Epoch: 28; Loss 1.443750 \n",
            "Step 25 of Epoch: 28; Loss 1.445058 \n",
            "Step 26 of Epoch: 28; Loss 1.445001 \n",
            "Step 27 of Epoch: 28; Loss 1.444553 \n",
            "Step 28 of Epoch: 28; Loss 1.445272 \n",
            "Step 29 of Epoch: 28; Loss 1.445563 \n",
            "Step 30 of Epoch: 28; Loss 1.445818 \n",
            "Step 31 of Epoch: 28; Loss 1.446699 \n",
            "Step 32 of Epoch: 28; Loss 1.446968 \n",
            "Step 33 of Epoch: 28; Loss 1.446840 \n",
            "Step 34 of Epoch: 28; Loss 1.447161 \n",
            "Step 35 of Epoch: 28; Loss 1.447594 \n",
            "Step 36 of Epoch: 28; Loss 1.448117 \n",
            "Step 37 of Epoch: 28; Loss 1.448502 \n",
            "Step 38 of Epoch: 28; Loss 1.448754 \n",
            "Step 39 of Epoch: 28; Loss 1.449335 \n",
            "Step 40 of Epoch: 28; Loss 1.449865 \n",
            "Final Result for Epoch 28: Loss 1.449869; Val Acc 0.301063; Train Acc 0.414346\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 29; Loss 1.400705 \n",
            "Step 2 of Epoch: 29; Loss 1.419764 \n",
            "Step 3 of Epoch: 29; Loss 1.423672 \n",
            "Step 4 of Epoch: 29; Loss 1.425160 \n",
            "Step 5 of Epoch: 29; Loss 1.425562 \n",
            "Step 6 of Epoch: 29; Loss 1.427657 \n",
            "Step 7 of Epoch: 29; Loss 1.430318 \n",
            "Step 8 of Epoch: 29; Loss 1.432834 \n",
            "Step 9 of Epoch: 29; Loss 1.433392 \n",
            "Step 10 of Epoch: 29; Loss 1.432851 \n",
            "Step 11 of Epoch: 29; Loss 1.433232 \n",
            "Step 12 of Epoch: 29; Loss 1.433408 \n",
            "Step 13 of Epoch: 29; Loss 1.433404 \n",
            "Step 14 of Epoch: 29; Loss 1.434773 \n",
            "Step 15 of Epoch: 29; Loss 1.434229 \n",
            "Step 16 of Epoch: 29; Loss 1.434366 \n",
            "Step 17 of Epoch: 29; Loss 1.434507 \n",
            "Step 18 of Epoch: 29; Loss 1.435188 \n",
            "Step 19 of Epoch: 29; Loss 1.436120 \n",
            "Step 20 of Epoch: 29; Loss 1.436657 \n",
            "Step 21 of Epoch: 29; Loss 1.436451 \n",
            "Step 22 of Epoch: 29; Loss 1.436621 \n",
            "Step 23 of Epoch: 29; Loss 1.437581 \n",
            "Step 24 of Epoch: 29; Loss 1.438539 \n",
            "Step 25 of Epoch: 29; Loss 1.439117 \n",
            "Step 26 of Epoch: 29; Loss 1.439566 \n",
            "Step 27 of Epoch: 29; Loss 1.440109 \n",
            "Step 28 of Epoch: 29; Loss 1.440353 \n",
            "Step 29 of Epoch: 29; Loss 1.440429 \n",
            "Step 30 of Epoch: 29; Loss 1.440870 \n",
            "Step 31 of Epoch: 29; Loss 1.441484 \n",
            "Step 32 of Epoch: 29; Loss 1.442620 \n",
            "Step 33 of Epoch: 29; Loss 1.442928 \n",
            "Step 34 of Epoch: 29; Loss 1.443273 \n",
            "Step 35 of Epoch: 29; Loss 1.443577 \n",
            "Step 36 of Epoch: 29; Loss 1.443998 \n",
            "Step 37 of Epoch: 29; Loss 1.444037 \n",
            "Step 38 of Epoch: 29; Loss 1.444072 \n",
            "Step 39 of Epoch: 29; Loss 1.444132 \n",
            "Step 40 of Epoch: 29; Loss 1.444033 \n",
            "Final Result for Epoch 29: Loss 1.444018; Val Acc 0.302938; Train Acc 0.418431\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 30; Loss 1.417463 \n",
            "Step 2 of Epoch: 30; Loss 1.410494 \n",
            "Step 3 of Epoch: 30; Loss 1.407035 \n",
            "Step 4 of Epoch: 30; Loss 1.411535 \n",
            "Step 5 of Epoch: 30; Loss 1.415938 \n",
            "Step 6 of Epoch: 30; Loss 1.417702 \n",
            "Step 7 of Epoch: 30; Loss 1.420470 \n",
            "Step 8 of Epoch: 30; Loss 1.422073 \n",
            "Step 9 of Epoch: 30; Loss 1.424929 \n",
            "Step 10 of Epoch: 30; Loss 1.424907 \n",
            "Step 11 of Epoch: 30; Loss 1.425058 \n",
            "Step 12 of Epoch: 30; Loss 1.426246 \n",
            "Step 13 of Epoch: 30; Loss 1.426654 \n",
            "Step 14 of Epoch: 30; Loss 1.427371 \n",
            "Step 15 of Epoch: 30; Loss 1.427830 \n",
            "Step 16 of Epoch: 30; Loss 1.427775 \n",
            "Step 17 of Epoch: 30; Loss 1.428845 \n",
            "Step 18 of Epoch: 30; Loss 1.429392 \n",
            "Step 19 of Epoch: 30; Loss 1.430302 \n",
            "Step 20 of Epoch: 30; Loss 1.430813 \n",
            "Step 21 of Epoch: 30; Loss 1.431342 \n",
            "Step 22 of Epoch: 30; Loss 1.432069 \n",
            "Step 23 of Epoch: 30; Loss 1.432741 \n",
            "Step 24 of Epoch: 30; Loss 1.432989 \n",
            "Step 25 of Epoch: 30; Loss 1.433184 \n",
            "Step 26 of Epoch: 30; Loss 1.434462 \n",
            "Step 27 of Epoch: 30; Loss 1.434570 \n",
            "Step 28 of Epoch: 30; Loss 1.434911 \n",
            "Step 29 of Epoch: 30; Loss 1.434884 \n",
            "Step 30 of Epoch: 30; Loss 1.435174 \n",
            "Step 31 of Epoch: 30; Loss 1.435270 \n",
            "Step 32 of Epoch: 30; Loss 1.435661 \n",
            "Step 33 of Epoch: 30; Loss 1.436199 \n",
            "Step 34 of Epoch: 30; Loss 1.436459 \n",
            "Step 35 of Epoch: 30; Loss 1.437012 \n",
            "Step 36 of Epoch: 30; Loss 1.437149 \n",
            "Step 37 of Epoch: 30; Loss 1.438018 \n",
            "Step 38 of Epoch: 30; Loss 1.438352 \n",
            "Step 39 of Epoch: 30; Loss 1.438843 \n",
            "Step 40 of Epoch: 30; Loss 1.438896 \n",
            "Final Result for Epoch 30: Loss 1.438903; Val Acc 0.301156; Train Acc 0.419735\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 31; Loss 1.421986 \n",
            "Step 2 of Epoch: 31; Loss 1.415448 \n",
            "Step 3 of Epoch: 31; Loss 1.413920 \n",
            "Step 4 of Epoch: 31; Loss 1.415619 \n",
            "Step 5 of Epoch: 31; Loss 1.413252 \n",
            "Step 6 of Epoch: 31; Loss 1.415826 \n",
            "Step 7 of Epoch: 31; Loss 1.416029 \n",
            "Step 8 of Epoch: 31; Loss 1.416629 \n",
            "Step 9 of Epoch: 31; Loss 1.417879 \n",
            "Step 10 of Epoch: 31; Loss 1.418587 \n",
            "Step 11 of Epoch: 31; Loss 1.419639 \n",
            "Step 12 of Epoch: 31; Loss 1.420905 \n",
            "Step 13 of Epoch: 31; Loss 1.420837 \n",
            "Step 14 of Epoch: 31; Loss 1.421497 \n",
            "Step 15 of Epoch: 31; Loss 1.422284 \n",
            "Step 16 of Epoch: 31; Loss 1.422309 \n",
            "Step 17 of Epoch: 31; Loss 1.423152 \n",
            "Step 18 of Epoch: 31; Loss 1.423487 \n",
            "Step 19 of Epoch: 31; Loss 1.424372 \n",
            "Step 20 of Epoch: 31; Loss 1.424160 \n",
            "Step 21 of Epoch: 31; Loss 1.424980 \n",
            "Step 22 of Epoch: 31; Loss 1.425782 \n",
            "Step 23 of Epoch: 31; Loss 1.426600 \n",
            "Step 24 of Epoch: 31; Loss 1.427315 \n",
            "Step 25 of Epoch: 31; Loss 1.427957 \n",
            "Step 26 of Epoch: 31; Loss 1.427862 \n",
            "Step 27 of Epoch: 31; Loss 1.428472 \n",
            "Step 28 of Epoch: 31; Loss 1.428905 \n",
            "Step 29 of Epoch: 31; Loss 1.429703 \n",
            "Step 30 of Epoch: 31; Loss 1.429770 \n",
            "Step 31 of Epoch: 31; Loss 1.430577 \n",
            "Step 32 of Epoch: 31; Loss 1.430805 \n",
            "Step 33 of Epoch: 31; Loss 1.430988 \n",
            "Step 34 of Epoch: 31; Loss 1.431275 \n",
            "Step 35 of Epoch: 31; Loss 1.431429 \n",
            "Step 36 of Epoch: 31; Loss 1.431602 \n",
            "Step 37 of Epoch: 31; Loss 1.431951 \n",
            "Step 38 of Epoch: 31; Loss 1.432612 \n",
            "Step 39 of Epoch: 31; Loss 1.432889 \n",
            "Step 40 of Epoch: 31; Loss 1.433367 \n",
            "Final Result for Epoch 31: Loss 1.433364; Val Acc 0.300875; Train Acc 0.421363\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 32; Loss 1.414954 \n",
            "Step 2 of Epoch: 32; Loss 1.412724 \n",
            "Step 3 of Epoch: 32; Loss 1.409513 \n",
            "Step 4 of Epoch: 32; Loss 1.410955 \n",
            "Step 5 of Epoch: 32; Loss 1.411678 \n",
            "Step 6 of Epoch: 32; Loss 1.410448 \n",
            "Step 7 of Epoch: 32; Loss 1.411560 \n",
            "Step 8 of Epoch: 32; Loss 1.412691 \n",
            "Step 9 of Epoch: 32; Loss 1.412809 \n",
            "Step 10 of Epoch: 32; Loss 1.414046 \n",
            "Step 11 of Epoch: 32; Loss 1.414099 \n",
            "Step 12 of Epoch: 32; Loss 1.414744 \n",
            "Step 13 of Epoch: 32; Loss 1.414360 \n",
            "Step 14 of Epoch: 32; Loss 1.414574 \n",
            "Step 15 of Epoch: 32; Loss 1.414112 \n",
            "Step 16 of Epoch: 32; Loss 1.415274 \n",
            "Step 17 of Epoch: 32; Loss 1.416201 \n",
            "Step 18 of Epoch: 32; Loss 1.417116 \n",
            "Step 19 of Epoch: 32; Loss 1.418366 \n",
            "Step 20 of Epoch: 32; Loss 1.419191 \n",
            "Step 21 of Epoch: 32; Loss 1.419318 \n",
            "Step 22 of Epoch: 32; Loss 1.420165 \n",
            "Step 23 of Epoch: 32; Loss 1.420367 \n",
            "Step 24 of Epoch: 32; Loss 1.421340 \n",
            "Step 25 of Epoch: 32; Loss 1.422142 \n",
            "Step 26 of Epoch: 32; Loss 1.422597 \n",
            "Step 27 of Epoch: 32; Loss 1.423031 \n",
            "Step 28 of Epoch: 32; Loss 1.423804 \n",
            "Step 29 of Epoch: 32; Loss 1.423784 \n",
            "Step 30 of Epoch: 32; Loss 1.424401 \n",
            "Step 31 of Epoch: 32; Loss 1.424699 \n",
            "Step 32 of Epoch: 32; Loss 1.425295 \n",
            "Step 33 of Epoch: 32; Loss 1.425236 \n",
            "Step 34 of Epoch: 32; Loss 1.425256 \n",
            "Step 35 of Epoch: 32; Loss 1.425727 \n",
            "Step 36 of Epoch: 32; Loss 1.425796 \n",
            "Step 37 of Epoch: 32; Loss 1.426069 \n",
            "Step 38 of Epoch: 32; Loss 1.426449 \n",
            "Step 39 of Epoch: 32; Loss 1.426963 \n",
            "Step 40 of Epoch: 32; Loss 1.427445 \n",
            "Final Result for Epoch 32: Loss 1.427424; Val Acc 0.298906; Train Acc 0.423890\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 33; Loss 1.411697 \n",
            "Step 2 of Epoch: 33; Loss 1.406601 \n",
            "Step 3 of Epoch: 33; Loss 1.406098 \n",
            "Step 4 of Epoch: 33; Loss 1.406647 \n",
            "Step 5 of Epoch: 33; Loss 1.406459 \n",
            "Step 6 of Epoch: 33; Loss 1.406608 \n",
            "Step 7 of Epoch: 33; Loss 1.408696 \n",
            "Step 8 of Epoch: 33; Loss 1.409747 \n",
            "Step 9 of Epoch: 33; Loss 1.410174 \n",
            "Step 10 of Epoch: 33; Loss 1.409241 \n",
            "Step 11 of Epoch: 33; Loss 1.411247 \n",
            "Step 12 of Epoch: 33; Loss 1.410892 \n",
            "Step 13 of Epoch: 33; Loss 1.412222 \n",
            "Step 14 of Epoch: 33; Loss 1.412644 \n",
            "Step 15 of Epoch: 33; Loss 1.413287 \n",
            "Step 16 of Epoch: 33; Loss 1.414929 \n",
            "Step 17 of Epoch: 33; Loss 1.415464 \n",
            "Step 18 of Epoch: 33; Loss 1.415382 \n",
            "Step 19 of Epoch: 33; Loss 1.415458 \n",
            "Step 20 of Epoch: 33; Loss 1.416147 \n",
            "Step 21 of Epoch: 33; Loss 1.416761 \n",
            "Step 22 of Epoch: 33; Loss 1.416410 \n",
            "Step 23 of Epoch: 33; Loss 1.417022 \n",
            "Step 24 of Epoch: 33; Loss 1.417286 \n",
            "Step 25 of Epoch: 33; Loss 1.418453 \n",
            "Step 26 of Epoch: 33; Loss 1.418869 \n",
            "Step 27 of Epoch: 33; Loss 1.419140 \n",
            "Step 28 of Epoch: 33; Loss 1.419243 \n",
            "Step 29 of Epoch: 33; Loss 1.419672 \n",
            "Step 30 of Epoch: 33; Loss 1.419174 \n",
            "Step 31 of Epoch: 33; Loss 1.419890 \n",
            "Step 32 of Epoch: 33; Loss 1.420037 \n",
            "Step 33 of Epoch: 33; Loss 1.420576 \n",
            "Step 34 of Epoch: 33; Loss 1.421197 \n",
            "Step 35 of Epoch: 33; Loss 1.421186 \n",
            "Step 36 of Epoch: 33; Loss 1.421520 \n",
            "Step 37 of Epoch: 33; Loss 1.422133 \n",
            "Step 38 of Epoch: 33; Loss 1.422287 \n",
            "Step 39 of Epoch: 33; Loss 1.422242 \n",
            "Step 40 of Epoch: 33; Loss 1.422416 \n",
            "Final Result for Epoch 33: Loss 1.422446; Val Acc 0.300000; Train Acc 0.425155\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 34; Loss 1.411330 \n",
            "Step 2 of Epoch: 34; Loss 1.401843 \n",
            "Step 3 of Epoch: 34; Loss 1.406980 \n",
            "Step 4 of Epoch: 34; Loss 1.407616 \n",
            "Step 5 of Epoch: 34; Loss 1.408074 \n",
            "Step 6 of Epoch: 34; Loss 1.409044 \n",
            "Step 7 of Epoch: 34; Loss 1.407446 \n",
            "Step 8 of Epoch: 34; Loss 1.408688 \n",
            "Step 9 of Epoch: 34; Loss 1.407713 \n",
            "Step 10 of Epoch: 34; Loss 1.406908 \n",
            "Step 11 of Epoch: 34; Loss 1.408005 \n",
            "Step 12 of Epoch: 34; Loss 1.408506 \n",
            "Step 13 of Epoch: 34; Loss 1.408136 \n",
            "Step 14 of Epoch: 34; Loss 1.407896 \n",
            "Step 15 of Epoch: 34; Loss 1.407488 \n",
            "Step 16 of Epoch: 34; Loss 1.407320 \n",
            "Step 17 of Epoch: 34; Loss 1.408225 \n",
            "Step 18 of Epoch: 34; Loss 1.409441 \n",
            "Step 19 of Epoch: 34; Loss 1.410004 \n",
            "Step 20 of Epoch: 34; Loss 1.410004 \n",
            "Step 21 of Epoch: 34; Loss 1.411591 \n",
            "Step 22 of Epoch: 34; Loss 1.411578 \n",
            "Step 23 of Epoch: 34; Loss 1.412041 \n",
            "Step 24 of Epoch: 34; Loss 1.412413 \n",
            "Step 25 of Epoch: 34; Loss 1.412575 \n",
            "Step 26 of Epoch: 34; Loss 1.412843 \n",
            "Step 27 of Epoch: 34; Loss 1.412828 \n",
            "Step 28 of Epoch: 34; Loss 1.413386 \n",
            "Step 29 of Epoch: 34; Loss 1.413554 \n",
            "Step 30 of Epoch: 34; Loss 1.414442 \n",
            "Step 31 of Epoch: 34; Loss 1.414641 \n",
            "Step 32 of Epoch: 34; Loss 1.414939 \n",
            "Step 33 of Epoch: 34; Loss 1.415609 \n",
            "Step 34 of Epoch: 34; Loss 1.415684 \n",
            "Step 35 of Epoch: 34; Loss 1.415727 \n",
            "Step 36 of Epoch: 34; Loss 1.416717 \n",
            "Step 37 of Epoch: 34; Loss 1.417033 \n",
            "Step 38 of Epoch: 34; Loss 1.417160 \n",
            "Step 39 of Epoch: 34; Loss 1.417465 \n",
            "Step 40 of Epoch: 34; Loss 1.417671 \n",
            "Final Result for Epoch 34: Loss 1.417680; Val Acc 0.300969; Train Acc 0.428858\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 35; Loss 1.399102 \n",
            "Step 2 of Epoch: 35; Loss 1.398465 \n",
            "Step 3 of Epoch: 35; Loss 1.398808 \n",
            "Step 4 of Epoch: 35; Loss 1.399789 \n",
            "Step 5 of Epoch: 35; Loss 1.403620 \n",
            "Step 6 of Epoch: 35; Loss 1.403293 \n",
            "Step 7 of Epoch: 35; Loss 1.405059 \n",
            "Step 8 of Epoch: 35; Loss 1.405054 \n",
            "Step 9 of Epoch: 35; Loss 1.404672 \n",
            "Step 10 of Epoch: 35; Loss 1.405717 \n",
            "Step 11 of Epoch: 35; Loss 1.405165 \n",
            "Step 12 of Epoch: 35; Loss 1.406295 \n",
            "Step 13 of Epoch: 35; Loss 1.406080 \n",
            "Step 14 of Epoch: 35; Loss 1.405112 \n",
            "Step 15 of Epoch: 35; Loss 1.404903 \n",
            "Step 16 of Epoch: 35; Loss 1.405194 \n",
            "Step 17 of Epoch: 35; Loss 1.404849 \n",
            "Step 18 of Epoch: 35; Loss 1.406139 \n",
            "Step 19 of Epoch: 35; Loss 1.405571 \n",
            "Step 20 of Epoch: 35; Loss 1.405771 \n",
            "Step 21 of Epoch: 35; Loss 1.406098 \n",
            "Step 22 of Epoch: 35; Loss 1.407123 \n",
            "Step 23 of Epoch: 35; Loss 1.406834 \n",
            "Step 24 of Epoch: 35; Loss 1.407480 \n",
            "Step 25 of Epoch: 35; Loss 1.408113 \n",
            "Step 26 of Epoch: 35; Loss 1.408291 \n",
            "Step 27 of Epoch: 35; Loss 1.408216 \n",
            "Step 28 of Epoch: 35; Loss 1.408526 \n",
            "Step 29 of Epoch: 35; Loss 1.408798 \n",
            "Step 30 of Epoch: 35; Loss 1.409042 \n",
            "Step 31 of Epoch: 35; Loss 1.409621 \n",
            "Step 32 of Epoch: 35; Loss 1.409842 \n",
            "Step 33 of Epoch: 35; Loss 1.409857 \n",
            "Step 34 of Epoch: 35; Loss 1.410155 \n",
            "Step 35 of Epoch: 35; Loss 1.410975 \n",
            "Step 36 of Epoch: 35; Loss 1.411328 \n",
            "Step 37 of Epoch: 35; Loss 1.411585 \n",
            "Step 38 of Epoch: 35; Loss 1.412431 \n",
            "Step 39 of Epoch: 35; Loss 1.412516 \n",
            "Step 40 of Epoch: 35; Loss 1.412725 \n",
            "Final Result for Epoch 35: Loss 1.412747; Val Acc 0.301219; Train Acc 0.427616\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 36; Loss 1.385851 \n",
            "Step 2 of Epoch: 36; Loss 1.390491 \n",
            "Step 3 of Epoch: 36; Loss 1.391327 \n",
            "Step 4 of Epoch: 36; Loss 1.394850 \n",
            "Step 5 of Epoch: 36; Loss 1.392966 \n",
            "Step 6 of Epoch: 36; Loss 1.393837 \n",
            "Step 7 of Epoch: 36; Loss 1.393758 \n",
            "Step 8 of Epoch: 36; Loss 1.392993 \n",
            "Step 9 of Epoch: 36; Loss 1.392775 \n",
            "Step 10 of Epoch: 36; Loss 1.394860 \n",
            "Step 11 of Epoch: 36; Loss 1.394441 \n",
            "Step 12 of Epoch: 36; Loss 1.395906 \n",
            "Step 13 of Epoch: 36; Loss 1.396102 \n",
            "Step 14 of Epoch: 36; Loss 1.395139 \n",
            "Step 15 of Epoch: 36; Loss 1.395949 \n",
            "Step 16 of Epoch: 36; Loss 1.396722 \n",
            "Step 17 of Epoch: 36; Loss 1.396382 \n",
            "Step 18 of Epoch: 36; Loss 1.396802 \n",
            "Step 19 of Epoch: 36; Loss 1.397901 \n",
            "Step 20 of Epoch: 36; Loss 1.398558 \n",
            "Step 21 of Epoch: 36; Loss 1.398934 \n",
            "Step 22 of Epoch: 36; Loss 1.399602 \n",
            "Step 23 of Epoch: 36; Loss 1.400391 \n",
            "Step 24 of Epoch: 36; Loss 1.400880 \n",
            "Step 25 of Epoch: 36; Loss 1.401313 \n",
            "Step 26 of Epoch: 36; Loss 1.401368 \n",
            "Step 27 of Epoch: 36; Loss 1.401713 \n",
            "Step 28 of Epoch: 36; Loss 1.402601 \n",
            "Step 29 of Epoch: 36; Loss 1.402972 \n",
            "Step 30 of Epoch: 36; Loss 1.403960 \n",
            "Step 31 of Epoch: 36; Loss 1.404084 \n",
            "Step 32 of Epoch: 36; Loss 1.404751 \n",
            "Step 33 of Epoch: 36; Loss 1.405354 \n",
            "Step 34 of Epoch: 36; Loss 1.405611 \n",
            "Step 35 of Epoch: 36; Loss 1.406445 \n",
            "Step 36 of Epoch: 36; Loss 1.406741 \n",
            "Step 37 of Epoch: 36; Loss 1.406576 \n",
            "Step 38 of Epoch: 36; Loss 1.407379 \n",
            "Step 39 of Epoch: 36; Loss 1.407674 \n",
            "Step 40 of Epoch: 36; Loss 1.407900 \n",
            "Final Result for Epoch 36: Loss 1.407893; Val Acc 0.300063; Train Acc 0.432408\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 37; Loss 1.376061 \n",
            "Step 2 of Epoch: 37; Loss 1.387744 \n",
            "Step 3 of Epoch: 37; Loss 1.388451 \n",
            "Step 4 of Epoch: 37; Loss 1.386919 \n",
            "Step 5 of Epoch: 37; Loss 1.388518 \n",
            "Step 6 of Epoch: 37; Loss 1.388904 \n",
            "Step 7 of Epoch: 37; Loss 1.392868 \n",
            "Step 8 of Epoch: 37; Loss 1.393450 \n",
            "Step 9 of Epoch: 37; Loss 1.394623 \n",
            "Step 10 of Epoch: 37; Loss 1.393738 \n",
            "Step 11 of Epoch: 37; Loss 1.393302 \n",
            "Step 12 of Epoch: 37; Loss 1.393605 \n",
            "Step 13 of Epoch: 37; Loss 1.393927 \n",
            "Step 14 of Epoch: 37; Loss 1.393771 \n",
            "Step 15 of Epoch: 37; Loss 1.395227 \n",
            "Step 16 of Epoch: 37; Loss 1.394717 \n",
            "Step 17 of Epoch: 37; Loss 1.395989 \n",
            "Step 18 of Epoch: 37; Loss 1.396217 \n",
            "Step 19 of Epoch: 37; Loss 1.397494 \n",
            "Step 20 of Epoch: 37; Loss 1.397141 \n",
            "Step 21 of Epoch: 37; Loss 1.397384 \n",
            "Step 22 of Epoch: 37; Loss 1.397686 \n",
            "Step 23 of Epoch: 37; Loss 1.397543 \n",
            "Step 24 of Epoch: 37; Loss 1.397794 \n",
            "Step 25 of Epoch: 37; Loss 1.398375 \n",
            "Step 26 of Epoch: 37; Loss 1.399054 \n",
            "Step 27 of Epoch: 37; Loss 1.399226 \n",
            "Step 28 of Epoch: 37; Loss 1.399498 \n",
            "Step 29 of Epoch: 37; Loss 1.400339 \n",
            "Step 30 of Epoch: 37; Loss 1.401127 \n",
            "Step 31 of Epoch: 37; Loss 1.401209 \n",
            "Step 32 of Epoch: 37; Loss 1.401514 \n",
            "Step 33 of Epoch: 37; Loss 1.401676 \n",
            "Step 34 of Epoch: 37; Loss 1.401856 \n",
            "Step 35 of Epoch: 37; Loss 1.402561 \n",
            "Step 36 of Epoch: 37; Loss 1.402764 \n",
            "Step 37 of Epoch: 37; Loss 1.402867 \n",
            "Step 38 of Epoch: 37; Loss 1.403253 \n",
            "Step 39 of Epoch: 37; Loss 1.403569 \n",
            "Step 40 of Epoch: 37; Loss 1.403868 \n",
            "Final Result for Epoch 37: Loss 1.403867; Val Acc 0.300500; Train Acc 0.434911\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 38; Loss 1.365691 \n",
            "Step 2 of Epoch: 38; Loss 1.374426 \n",
            "Step 3 of Epoch: 38; Loss 1.374838 \n",
            "Step 4 of Epoch: 38; Loss 1.374559 \n",
            "Step 5 of Epoch: 38; Loss 1.379069 \n",
            "Step 6 of Epoch: 38; Loss 1.381431 \n",
            "Step 7 of Epoch: 38; Loss 1.384548 \n",
            "Step 8 of Epoch: 38; Loss 1.387509 \n",
            "Step 9 of Epoch: 38; Loss 1.387896 \n",
            "Step 10 of Epoch: 38; Loss 1.388338 \n",
            "Step 11 of Epoch: 38; Loss 1.389099 \n",
            "Step 12 of Epoch: 38; Loss 1.390108 \n",
            "Step 13 of Epoch: 38; Loss 1.389162 \n",
            "Step 14 of Epoch: 38; Loss 1.389591 \n",
            "Step 15 of Epoch: 38; Loss 1.389422 \n",
            "Step 16 of Epoch: 38; Loss 1.390134 \n",
            "Step 17 of Epoch: 38; Loss 1.390478 \n",
            "Step 18 of Epoch: 38; Loss 1.391002 \n",
            "Step 19 of Epoch: 38; Loss 1.391741 \n",
            "Step 20 of Epoch: 38; Loss 1.391135 \n",
            "Step 21 of Epoch: 38; Loss 1.391912 \n",
            "Step 22 of Epoch: 38; Loss 1.391775 \n",
            "Step 23 of Epoch: 38; Loss 1.392072 \n",
            "Step 24 of Epoch: 38; Loss 1.392350 \n",
            "Step 25 of Epoch: 38; Loss 1.392589 \n",
            "Step 26 of Epoch: 38; Loss 1.394121 \n",
            "Step 27 of Epoch: 38; Loss 1.394797 \n",
            "Step 28 of Epoch: 38; Loss 1.395057 \n",
            "Step 29 of Epoch: 38; Loss 1.395964 \n",
            "Step 30 of Epoch: 38; Loss 1.396420 \n",
            "Step 31 of Epoch: 38; Loss 1.396833 \n",
            "Step 32 of Epoch: 38; Loss 1.397063 \n",
            "Step 33 of Epoch: 38; Loss 1.397403 \n",
            "Step 34 of Epoch: 38; Loss 1.397315 \n",
            "Step 35 of Epoch: 38; Loss 1.397544 \n",
            "Step 36 of Epoch: 38; Loss 1.398448 \n",
            "Step 37 of Epoch: 38; Loss 1.398917 \n",
            "Step 38 of Epoch: 38; Loss 1.399203 \n",
            "Step 39 of Epoch: 38; Loss 1.399138 \n",
            "Step 40 of Epoch: 38; Loss 1.399380 \n",
            "Final Result for Epoch 38: Loss 1.399384; Val Acc 0.298500; Train Acc 0.434731\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 39; Loss 1.379615 \n",
            "Step 2 of Epoch: 39; Loss 1.377334 \n",
            "Step 3 of Epoch: 39; Loss 1.381155 \n",
            "Step 4 of Epoch: 39; Loss 1.377923 \n",
            "Step 5 of Epoch: 39; Loss 1.379175 \n",
            "Step 6 of Epoch: 39; Loss 1.379741 \n",
            "Step 7 of Epoch: 39; Loss 1.380346 \n",
            "Step 8 of Epoch: 39; Loss 1.378956 \n",
            "Step 9 of Epoch: 39; Loss 1.380588 \n",
            "Step 10 of Epoch: 39; Loss 1.381623 \n",
            "Step 11 of Epoch: 39; Loss 1.381490 \n",
            "Step 12 of Epoch: 39; Loss 1.382322 \n",
            "Step 13 of Epoch: 39; Loss 1.383789 \n",
            "Step 14 of Epoch: 39; Loss 1.384527 \n",
            "Step 15 of Epoch: 39; Loss 1.384750 \n",
            "Step 16 of Epoch: 39; Loss 1.386012 \n",
            "Step 17 of Epoch: 39; Loss 1.385898 \n",
            "Step 18 of Epoch: 39; Loss 1.386114 \n",
            "Step 19 of Epoch: 39; Loss 1.386725 \n",
            "Step 20 of Epoch: 39; Loss 1.386977 \n",
            "Step 21 of Epoch: 39; Loss 1.387261 \n",
            "Step 22 of Epoch: 39; Loss 1.387158 \n",
            "Step 23 of Epoch: 39; Loss 1.387452 \n",
            "Step 24 of Epoch: 39; Loss 1.387661 \n",
            "Step 25 of Epoch: 39; Loss 1.387718 \n",
            "Step 26 of Epoch: 39; Loss 1.388886 \n",
            "Step 27 of Epoch: 39; Loss 1.389334 \n",
            "Step 28 of Epoch: 39; Loss 1.390240 \n",
            "Step 29 of Epoch: 39; Loss 1.390801 \n",
            "Step 30 of Epoch: 39; Loss 1.391511 \n",
            "Step 31 of Epoch: 39; Loss 1.392198 \n",
            "Step 32 of Epoch: 39; Loss 1.392471 \n",
            "Step 33 of Epoch: 39; Loss 1.392586 \n",
            "Step 34 of Epoch: 39; Loss 1.393288 \n",
            "Step 35 of Epoch: 39; Loss 1.393693 \n",
            "Step 36 of Epoch: 39; Loss 1.394478 \n",
            "Step 37 of Epoch: 39; Loss 1.394724 \n",
            "Step 38 of Epoch: 39; Loss 1.394738 \n",
            "Step 39 of Epoch: 39; Loss 1.394675 \n",
            "Step 40 of Epoch: 39; Loss 1.395162 \n",
            "Final Result for Epoch 39: Loss 1.395132; Val Acc 0.298000; Train Acc 0.435711\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 40; Loss 1.370677 \n",
            "Step 2 of Epoch: 40; Loss 1.368199 \n",
            "Step 3 of Epoch: 40; Loss 1.369091 \n",
            "Step 4 of Epoch: 40; Loss 1.372646 \n",
            "Step 5 of Epoch: 40; Loss 1.372565 \n",
            "Step 6 of Epoch: 40; Loss 1.375816 \n",
            "Step 7 of Epoch: 40; Loss 1.375637 \n",
            "Step 8 of Epoch: 40; Loss 1.376201 \n",
            "Step 9 of Epoch: 40; Loss 1.375744 \n",
            "Step 10 of Epoch: 40; Loss 1.377797 \n",
            "Step 11 of Epoch: 40; Loss 1.377528 \n",
            "Step 12 of Epoch: 40; Loss 1.378002 \n",
            "Step 13 of Epoch: 40; Loss 1.377297 \n",
            "Step 14 of Epoch: 40; Loss 1.376895 \n",
            "Step 15 of Epoch: 40; Loss 1.377831 \n",
            "Step 16 of Epoch: 40; Loss 1.378714 \n",
            "Step 17 of Epoch: 40; Loss 1.379513 \n",
            "Step 18 of Epoch: 40; Loss 1.380411 \n",
            "Step 19 of Epoch: 40; Loss 1.381009 \n",
            "Step 20 of Epoch: 40; Loss 1.381775 \n",
            "Step 21 of Epoch: 40; Loss 1.382664 \n",
            "Step 22 of Epoch: 40; Loss 1.383212 \n",
            "Step 23 of Epoch: 40; Loss 1.383901 \n",
            "Step 24 of Epoch: 40; Loss 1.384227 \n",
            "Step 25 of Epoch: 40; Loss 1.384516 \n",
            "Step 26 of Epoch: 40; Loss 1.384794 \n",
            "Step 27 of Epoch: 40; Loss 1.385424 \n",
            "Step 28 of Epoch: 40; Loss 1.385362 \n",
            "Step 29 of Epoch: 40; Loss 1.386225 \n",
            "Step 30 of Epoch: 40; Loss 1.386902 \n",
            "Step 31 of Epoch: 40; Loss 1.387373 \n",
            "Step 32 of Epoch: 40; Loss 1.388176 \n",
            "Step 33 of Epoch: 40; Loss 1.388254 \n",
            "Step 34 of Epoch: 40; Loss 1.388477 \n",
            "Step 35 of Epoch: 40; Loss 1.388975 \n",
            "Step 36 of Epoch: 40; Loss 1.389515 \n",
            "Step 37 of Epoch: 40; Loss 1.390099 \n",
            "Step 38 of Epoch: 40; Loss 1.390713 \n",
            "Step 39 of Epoch: 40; Loss 1.390841 \n",
            "Step 40 of Epoch: 40; Loss 1.391759 \n",
            "Final Result for Epoch 40: Loss 1.391781; Val Acc 0.298500; Train Acc 0.439390\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 41; Loss 1.373319 \n",
            "Step 2 of Epoch: 41; Loss 1.369700 \n",
            "Step 3 of Epoch: 41; Loss 1.374305 \n",
            "Step 4 of Epoch: 41; Loss 1.372840 \n",
            "Step 5 of Epoch: 41; Loss 1.370525 \n",
            "Step 6 of Epoch: 41; Loss 1.369862 \n",
            "Step 7 of Epoch: 41; Loss 1.368059 \n",
            "Step 8 of Epoch: 41; Loss 1.367222 \n",
            "Step 9 of Epoch: 41; Loss 1.369374 \n",
            "Step 10 of Epoch: 41; Loss 1.368980 \n",
            "Step 11 of Epoch: 41; Loss 1.369723 \n",
            "Step 12 of Epoch: 41; Loss 1.371697 \n",
            "Step 13 of Epoch: 41; Loss 1.373680 \n",
            "Step 14 of Epoch: 41; Loss 1.375666 \n",
            "Step 15 of Epoch: 41; Loss 1.377125 \n",
            "Step 16 of Epoch: 41; Loss 1.377644 \n",
            "Step 17 of Epoch: 41; Loss 1.378237 \n",
            "Step 18 of Epoch: 41; Loss 1.378901 \n",
            "Step 19 of Epoch: 41; Loss 1.379129 \n",
            "Step 20 of Epoch: 41; Loss 1.378937 \n",
            "Step 21 of Epoch: 41; Loss 1.379381 \n",
            "Step 22 of Epoch: 41; Loss 1.379777 \n",
            "Step 23 of Epoch: 41; Loss 1.380792 \n",
            "Step 24 of Epoch: 41; Loss 1.381333 \n",
            "Step 25 of Epoch: 41; Loss 1.381702 \n",
            "Step 26 of Epoch: 41; Loss 1.381927 \n",
            "Step 27 of Epoch: 41; Loss 1.381918 \n",
            "Step 28 of Epoch: 41; Loss 1.382526 \n",
            "Step 29 of Epoch: 41; Loss 1.382994 \n",
            "Step 30 of Epoch: 41; Loss 1.383446 \n",
            "Step 31 of Epoch: 41; Loss 1.383867 \n",
            "Step 32 of Epoch: 41; Loss 1.384394 \n",
            "Step 33 of Epoch: 41; Loss 1.384722 \n",
            "Step 34 of Epoch: 41; Loss 1.385136 \n",
            "Step 35 of Epoch: 41; Loss 1.385798 \n",
            "Step 36 of Epoch: 41; Loss 1.386170 \n",
            "Step 37 of Epoch: 41; Loss 1.386265 \n",
            "Step 38 of Epoch: 41; Loss 1.386993 \n",
            "Step 39 of Epoch: 41; Loss 1.387423 \n",
            "Step 40 of Epoch: 41; Loss 1.387702 \n",
            "Final Result for Epoch 41: Loss 1.387710; Val Acc 0.296625; Train Acc 0.441776\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 42; Loss 1.358586 \n",
            "Step 2 of Epoch: 42; Loss 1.366725 \n",
            "Step 3 of Epoch: 42; Loss 1.357679 \n",
            "Step 4 of Epoch: 42; Loss 1.362649 \n",
            "Step 5 of Epoch: 42; Loss 1.363214 \n",
            "Step 6 of Epoch: 42; Loss 1.365020 \n",
            "Step 7 of Epoch: 42; Loss 1.368232 \n",
            "Step 8 of Epoch: 42; Loss 1.367778 \n",
            "Step 9 of Epoch: 42; Loss 1.368388 \n",
            "Step 10 of Epoch: 42; Loss 1.368871 \n",
            "Step 11 of Epoch: 42; Loss 1.369348 \n",
            "Step 12 of Epoch: 42; Loss 1.369409 \n",
            "Step 13 of Epoch: 42; Loss 1.369859 \n",
            "Step 14 of Epoch: 42; Loss 1.371036 \n",
            "Step 15 of Epoch: 42; Loss 1.372530 \n",
            "Step 16 of Epoch: 42; Loss 1.374320 \n",
            "Step 17 of Epoch: 42; Loss 1.373962 \n",
            "Step 18 of Epoch: 42; Loss 1.374336 \n",
            "Step 19 of Epoch: 42; Loss 1.374760 \n",
            "Step 20 of Epoch: 42; Loss 1.376470 \n",
            "Step 21 of Epoch: 42; Loss 1.375967 \n",
            "Step 22 of Epoch: 42; Loss 1.377019 \n",
            "Step 23 of Epoch: 42; Loss 1.376688 \n",
            "Step 24 of Epoch: 42; Loss 1.377113 \n",
            "Step 25 of Epoch: 42; Loss 1.377548 \n",
            "Step 26 of Epoch: 42; Loss 1.377271 \n",
            "Step 27 of Epoch: 42; Loss 1.377717 \n",
            "Step 28 of Epoch: 42; Loss 1.377920 \n",
            "Step 29 of Epoch: 42; Loss 1.378380 \n",
            "Step 30 of Epoch: 42; Loss 1.378860 \n",
            "Step 31 of Epoch: 42; Loss 1.379483 \n",
            "Step 32 of Epoch: 42; Loss 1.379705 \n",
            "Step 33 of Epoch: 42; Loss 1.380819 \n",
            "Step 34 of Epoch: 42; Loss 1.381468 \n",
            "Step 35 of Epoch: 42; Loss 1.382004 \n",
            "Step 36 of Epoch: 42; Loss 1.382720 \n",
            "Step 37 of Epoch: 42; Loss 1.383052 \n",
            "Step 38 of Epoch: 42; Loss 1.383452 \n",
            "Step 39 of Epoch: 42; Loss 1.383676 \n",
            "Step 40 of Epoch: 42; Loss 1.383938 \n",
            "Final Result for Epoch 42: Loss 1.383965; Val Acc 0.298500; Train Acc 0.442026\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 43; Loss 1.365869 \n",
            "Step 2 of Epoch: 43; Loss 1.363215 \n",
            "Step 3 of Epoch: 43; Loss 1.364550 \n",
            "Step 4 of Epoch: 43; Loss 1.367020 \n",
            "Step 5 of Epoch: 43; Loss 1.366210 \n",
            "Step 6 of Epoch: 43; Loss 1.366946 \n",
            "Step 7 of Epoch: 43; Loss 1.366353 \n",
            "Step 8 of Epoch: 43; Loss 1.365821 \n",
            "Step 9 of Epoch: 43; Loss 1.366294 \n",
            "Step 10 of Epoch: 43; Loss 1.366358 \n",
            "Step 11 of Epoch: 43; Loss 1.365828 \n",
            "Step 12 of Epoch: 43; Loss 1.364729 \n",
            "Step 13 of Epoch: 43; Loss 1.365704 \n",
            "Step 14 of Epoch: 43; Loss 1.366203 \n",
            "Step 15 of Epoch: 43; Loss 1.366775 \n",
            "Step 16 of Epoch: 43; Loss 1.367767 \n",
            "Step 17 of Epoch: 43; Loss 1.369084 \n",
            "Step 18 of Epoch: 43; Loss 1.370233 \n",
            "Step 19 of Epoch: 43; Loss 1.370835 \n",
            "Step 20 of Epoch: 43; Loss 1.371360 \n",
            "Step 21 of Epoch: 43; Loss 1.371425 \n",
            "Step 22 of Epoch: 43; Loss 1.371787 \n",
            "Step 23 of Epoch: 43; Loss 1.372641 \n",
            "Step 24 of Epoch: 43; Loss 1.373361 \n",
            "Step 25 of Epoch: 43; Loss 1.373509 \n",
            "Step 26 of Epoch: 43; Loss 1.374042 \n",
            "Step 27 of Epoch: 43; Loss 1.374320 \n",
            "Step 28 of Epoch: 43; Loss 1.374715 \n",
            "Step 29 of Epoch: 43; Loss 1.375295 \n",
            "Step 30 of Epoch: 43; Loss 1.375516 \n",
            "Step 31 of Epoch: 43; Loss 1.376567 \n",
            "Step 32 of Epoch: 43; Loss 1.376494 \n",
            "Step 33 of Epoch: 43; Loss 1.376453 \n",
            "Step 34 of Epoch: 43; Loss 1.376987 \n",
            "Step 35 of Epoch: 43; Loss 1.377720 \n",
            "Step 36 of Epoch: 43; Loss 1.378043 \n",
            "Step 37 of Epoch: 43; Loss 1.378735 \n",
            "Step 38 of Epoch: 43; Loss 1.379084 \n",
            "Step 39 of Epoch: 43; Loss 1.379411 \n",
            "Step 40 of Epoch: 43; Loss 1.379968 \n",
            "Final Result for Epoch 43: Loss 1.380002; Val Acc 0.294594; Train Acc 0.442503\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 44; Loss 1.347189 \n",
            "Step 2 of Epoch: 44; Loss 1.356169 \n",
            "Step 3 of Epoch: 44; Loss 1.356919 \n",
            "Step 4 of Epoch: 44; Loss 1.352202 \n",
            "Step 5 of Epoch: 44; Loss 1.355046 \n",
            "Step 6 of Epoch: 44; Loss 1.355788 \n",
            "Step 7 of Epoch: 44; Loss 1.355725 \n",
            "Step 8 of Epoch: 44; Loss 1.357706 \n",
            "Step 9 of Epoch: 44; Loss 1.359259 \n",
            "Step 10 of Epoch: 44; Loss 1.359628 \n",
            "Step 11 of Epoch: 44; Loss 1.361413 \n",
            "Step 12 of Epoch: 44; Loss 1.362258 \n",
            "Step 13 of Epoch: 44; Loss 1.363414 \n",
            "Step 14 of Epoch: 44; Loss 1.364073 \n",
            "Step 15 of Epoch: 44; Loss 1.365526 \n",
            "Step 16 of Epoch: 44; Loss 1.366348 \n",
            "Step 17 of Epoch: 44; Loss 1.367331 \n",
            "Step 18 of Epoch: 44; Loss 1.367943 \n",
            "Step 19 of Epoch: 44; Loss 1.369214 \n",
            "Step 20 of Epoch: 44; Loss 1.370136 \n",
            "Step 21 of Epoch: 44; Loss 1.370118 \n",
            "Step 22 of Epoch: 44; Loss 1.370774 \n",
            "Step 23 of Epoch: 44; Loss 1.371004 \n",
            "Step 24 of Epoch: 44; Loss 1.371918 \n",
            "Step 25 of Epoch: 44; Loss 1.372047 \n",
            "Step 26 of Epoch: 44; Loss 1.372008 \n",
            "Step 27 of Epoch: 44; Loss 1.372071 \n",
            "Step 28 of Epoch: 44; Loss 1.371508 \n",
            "Step 29 of Epoch: 44; Loss 1.372162 \n",
            "Step 30 of Epoch: 44; Loss 1.372152 \n",
            "Step 31 of Epoch: 44; Loss 1.372677 \n",
            "Step 32 of Epoch: 44; Loss 1.372959 \n",
            "Step 33 of Epoch: 44; Loss 1.373421 \n",
            "Step 34 of Epoch: 44; Loss 1.374283 \n",
            "Step 35 of Epoch: 44; Loss 1.374462 \n",
            "Step 36 of Epoch: 44; Loss 1.374610 \n",
            "Step 37 of Epoch: 44; Loss 1.375410 \n",
            "Step 38 of Epoch: 44; Loss 1.375509 \n",
            "Step 39 of Epoch: 44; Loss 1.376152 \n",
            "Step 40 of Epoch: 44; Loss 1.377031 \n",
            "Final Result for Epoch 44: Loss 1.377029; Val Acc 0.295625; Train Acc 0.446260\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 45; Loss 1.334077 \n",
            "Step 2 of Epoch: 45; Loss 1.340768 \n",
            "Step 3 of Epoch: 45; Loss 1.352305 \n",
            "Step 4 of Epoch: 45; Loss 1.352441 \n",
            "Step 5 of Epoch: 45; Loss 1.355396 \n",
            "Step 6 of Epoch: 45; Loss 1.356372 \n",
            "Step 7 of Epoch: 45; Loss 1.357462 \n",
            "Step 8 of Epoch: 45; Loss 1.358042 \n",
            "Step 9 of Epoch: 45; Loss 1.359247 \n",
            "Step 10 of Epoch: 45; Loss 1.359564 \n",
            "Step 11 of Epoch: 45; Loss 1.360069 \n",
            "Step 12 of Epoch: 45; Loss 1.360588 \n",
            "Step 13 of Epoch: 45; Loss 1.362064 \n",
            "Step 14 of Epoch: 45; Loss 1.363772 \n",
            "Step 15 of Epoch: 45; Loss 1.363756 \n",
            "Step 16 of Epoch: 45; Loss 1.363377 \n",
            "Step 17 of Epoch: 45; Loss 1.363023 \n",
            "Step 18 of Epoch: 45; Loss 1.363018 \n",
            "Step 19 of Epoch: 45; Loss 1.363404 \n",
            "Step 20 of Epoch: 45; Loss 1.363564 \n",
            "Step 21 of Epoch: 45; Loss 1.364690 \n",
            "Step 22 of Epoch: 45; Loss 1.365442 \n",
            "Step 23 of Epoch: 45; Loss 1.366167 \n",
            "Step 24 of Epoch: 45; Loss 1.366772 \n",
            "Step 25 of Epoch: 45; Loss 1.366712 \n",
            "Step 26 of Epoch: 45; Loss 1.367721 \n",
            "Step 27 of Epoch: 45; Loss 1.368043 \n",
            "Step 28 of Epoch: 45; Loss 1.368579 \n",
            "Step 29 of Epoch: 45; Loss 1.368695 \n",
            "Step 30 of Epoch: 45; Loss 1.369958 \n",
            "Step 31 of Epoch: 45; Loss 1.370877 \n",
            "Step 32 of Epoch: 45; Loss 1.370904 \n",
            "Step 33 of Epoch: 45; Loss 1.371156 \n",
            "Step 34 of Epoch: 45; Loss 1.371733 \n",
            "Step 35 of Epoch: 45; Loss 1.371499 \n",
            "Step 36 of Epoch: 45; Loss 1.371580 \n",
            "Step 37 of Epoch: 45; Loss 1.372133 \n",
            "Step 38 of Epoch: 45; Loss 1.372667 \n",
            "Step 39 of Epoch: 45; Loss 1.372793 \n",
            "Step 40 of Epoch: 45; Loss 1.373190 \n",
            "Final Result for Epoch 45: Loss 1.373179; Val Acc 0.297187; Train Acc 0.446748\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 46; Loss 1.352685 \n",
            "Step 2 of Epoch: 46; Loss 1.347530 \n",
            "Step 3 of Epoch: 46; Loss 1.350261 \n",
            "Step 4 of Epoch: 46; Loss 1.352322 \n",
            "Step 5 of Epoch: 46; Loss 1.349528 \n",
            "Step 6 of Epoch: 46; Loss 1.347785 \n",
            "Step 7 of Epoch: 46; Loss 1.348189 \n",
            "Step 8 of Epoch: 46; Loss 1.350336 \n",
            "Step 9 of Epoch: 46; Loss 1.353169 \n",
            "Step 10 of Epoch: 46; Loss 1.354144 \n",
            "Step 11 of Epoch: 46; Loss 1.354696 \n",
            "Step 12 of Epoch: 46; Loss 1.354074 \n",
            "Step 13 of Epoch: 46; Loss 1.355673 \n",
            "Step 14 of Epoch: 46; Loss 1.357482 \n",
            "Step 15 of Epoch: 46; Loss 1.358571 \n",
            "Step 16 of Epoch: 46; Loss 1.358299 \n",
            "Step 17 of Epoch: 46; Loss 1.359112 \n",
            "Step 18 of Epoch: 46; Loss 1.360354 \n",
            "Step 19 of Epoch: 46; Loss 1.361227 \n",
            "Step 20 of Epoch: 46; Loss 1.362574 \n",
            "Step 21 of Epoch: 46; Loss 1.363707 \n",
            "Step 22 of Epoch: 46; Loss 1.364046 \n",
            "Step 23 of Epoch: 46; Loss 1.364173 \n",
            "Step 24 of Epoch: 46; Loss 1.364840 \n",
            "Step 25 of Epoch: 46; Loss 1.365080 \n",
            "Step 26 of Epoch: 46; Loss 1.365607 \n",
            "Step 27 of Epoch: 46; Loss 1.366427 \n",
            "Step 28 of Epoch: 46; Loss 1.366332 \n",
            "Step 29 of Epoch: 46; Loss 1.367030 \n",
            "Step 30 of Epoch: 46; Loss 1.367797 \n",
            "Step 31 of Epoch: 46; Loss 1.367995 \n",
            "Step 32 of Epoch: 46; Loss 1.367991 \n",
            "Step 33 of Epoch: 46; Loss 1.368339 \n",
            "Step 34 of Epoch: 46; Loss 1.368802 \n",
            "Step 35 of Epoch: 46; Loss 1.369361 \n",
            "Step 36 of Epoch: 46; Loss 1.369637 \n",
            "Step 37 of Epoch: 46; Loss 1.370108 \n",
            "Step 38 of Epoch: 46; Loss 1.370383 \n",
            "Step 39 of Epoch: 46; Loss 1.370618 \n",
            "Step 40 of Epoch: 46; Loss 1.370806 \n",
            "Final Result for Epoch 46: Loss 1.370853; Val Acc 0.295969; Train Acc 0.449024\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 47; Loss 1.337411 \n",
            "Step 2 of Epoch: 47; Loss 1.337034 \n",
            "Step 3 of Epoch: 47; Loss 1.334159 \n",
            "Step 4 of Epoch: 47; Loss 1.338252 \n",
            "Step 5 of Epoch: 47; Loss 1.338658 \n",
            "Step 6 of Epoch: 47; Loss 1.341560 \n",
            "Step 7 of Epoch: 47; Loss 1.345189 \n",
            "Step 8 of Epoch: 47; Loss 1.347595 \n",
            "Step 9 of Epoch: 47; Loss 1.351515 \n",
            "Step 10 of Epoch: 47; Loss 1.353299 \n",
            "Step 11 of Epoch: 47; Loss 1.353957 \n",
            "Step 12 of Epoch: 47; Loss 1.353674 \n",
            "Step 13 of Epoch: 47; Loss 1.354440 \n",
            "Step 14 of Epoch: 47; Loss 1.353554 \n",
            "Step 15 of Epoch: 47; Loss 1.354704 \n",
            "Step 16 of Epoch: 47; Loss 1.354668 \n",
            "Step 17 of Epoch: 47; Loss 1.354858 \n",
            "Step 18 of Epoch: 47; Loss 1.356600 \n",
            "Step 19 of Epoch: 47; Loss 1.357194 \n",
            "Step 20 of Epoch: 47; Loss 1.358149 \n",
            "Step 21 of Epoch: 47; Loss 1.359072 \n",
            "Step 22 of Epoch: 47; Loss 1.359651 \n",
            "Step 23 of Epoch: 47; Loss 1.359866 \n",
            "Step 24 of Epoch: 47; Loss 1.360253 \n",
            "Step 25 of Epoch: 47; Loss 1.360985 \n",
            "Step 26 of Epoch: 47; Loss 1.361750 \n",
            "Step 27 of Epoch: 47; Loss 1.362584 \n",
            "Step 28 of Epoch: 47; Loss 1.363765 \n",
            "Step 29 of Epoch: 47; Loss 1.363954 \n",
            "Step 30 of Epoch: 47; Loss 1.364025 \n",
            "Step 31 of Epoch: 47; Loss 1.364016 \n",
            "Step 32 of Epoch: 47; Loss 1.364460 \n",
            "Step 33 of Epoch: 47; Loss 1.365175 \n",
            "Step 34 of Epoch: 47; Loss 1.365525 \n",
            "Step 35 of Epoch: 47; Loss 1.365653 \n",
            "Step 36 of Epoch: 47; Loss 1.365789 \n",
            "Step 37 of Epoch: 47; Loss 1.365895 \n",
            "Step 38 of Epoch: 47; Loss 1.366143 \n",
            "Step 39 of Epoch: 47; Loss 1.366586 \n",
            "Step 40 of Epoch: 47; Loss 1.367195 \n",
            "Final Result for Epoch 47: Loss 1.367194; Val Acc 0.294812; Train Acc 0.449353\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 48; Loss 1.344115 \n",
            "Step 2 of Epoch: 48; Loss 1.345728 \n",
            "Step 3 of Epoch: 48; Loss 1.345044 \n",
            "Step 4 of Epoch: 48; Loss 1.349756 \n",
            "Step 5 of Epoch: 48; Loss 1.345636 \n",
            "Step 6 of Epoch: 48; Loss 1.345645 \n",
            "Step 7 of Epoch: 48; Loss 1.345132 \n",
            "Step 8 of Epoch: 48; Loss 1.346221 \n",
            "Step 9 of Epoch: 48; Loss 1.347933 \n",
            "Step 10 of Epoch: 48; Loss 1.348759 \n",
            "Step 11 of Epoch: 48; Loss 1.349666 \n",
            "Step 12 of Epoch: 48; Loss 1.350602 \n",
            "Step 13 of Epoch: 48; Loss 1.351746 \n",
            "Step 14 of Epoch: 48; Loss 1.353443 \n",
            "Step 15 of Epoch: 48; Loss 1.354189 \n",
            "Step 16 of Epoch: 48; Loss 1.354000 \n",
            "Step 17 of Epoch: 48; Loss 1.354644 \n",
            "Step 18 of Epoch: 48; Loss 1.355257 \n",
            "Step 19 of Epoch: 48; Loss 1.355689 \n",
            "Step 20 of Epoch: 48; Loss 1.356031 \n",
            "Step 21 of Epoch: 48; Loss 1.356851 \n",
            "Step 22 of Epoch: 48; Loss 1.356938 \n",
            "Step 23 of Epoch: 48; Loss 1.358489 \n",
            "Step 24 of Epoch: 48; Loss 1.358929 \n",
            "Step 25 of Epoch: 48; Loss 1.359047 \n",
            "Step 26 of Epoch: 48; Loss 1.359827 \n",
            "Step 27 of Epoch: 48; Loss 1.360157 \n",
            "Step 28 of Epoch: 48; Loss 1.360064 \n",
            "Step 29 of Epoch: 48; Loss 1.360531 \n",
            "Step 30 of Epoch: 48; Loss 1.360511 \n",
            "Step 31 of Epoch: 48; Loss 1.360786 \n",
            "Step 32 of Epoch: 48; Loss 1.361277 \n",
            "Step 33 of Epoch: 48; Loss 1.360979 \n",
            "Step 34 of Epoch: 48; Loss 1.361273 \n",
            "Step 35 of Epoch: 48; Loss 1.362235 \n",
            "Step 36 of Epoch: 48; Loss 1.362456 \n",
            "Step 37 of Epoch: 48; Loss 1.362714 \n",
            "Step 38 of Epoch: 48; Loss 1.362980 \n",
            "Step 39 of Epoch: 48; Loss 1.363738 \n",
            "Step 40 of Epoch: 48; Loss 1.364248 \n",
            "Final Result for Epoch 48: Loss 1.364291; Val Acc 0.293500; Train Acc 0.451688\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 49; Loss 1.322201 \n",
            "Step 2 of Epoch: 49; Loss 1.332512 \n",
            "Step 3 of Epoch: 49; Loss 1.335448 \n",
            "Step 4 of Epoch: 49; Loss 1.338536 \n",
            "Step 5 of Epoch: 49; Loss 1.341808 \n",
            "Step 6 of Epoch: 49; Loss 1.342970 \n",
            "Step 7 of Epoch: 49; Loss 1.344603 \n",
            "Step 8 of Epoch: 49; Loss 1.346515 \n",
            "Step 9 of Epoch: 49; Loss 1.347041 \n",
            "Step 10 of Epoch: 49; Loss 1.347986 \n",
            "Step 11 of Epoch: 49; Loss 1.348453 \n",
            "Step 12 of Epoch: 49; Loss 1.348048 \n",
            "Step 13 of Epoch: 49; Loss 1.348193 \n",
            "Step 14 of Epoch: 49; Loss 1.348881 \n",
            "Step 15 of Epoch: 49; Loss 1.349122 \n",
            "Step 16 of Epoch: 49; Loss 1.349856 \n",
            "Step 17 of Epoch: 49; Loss 1.349632 \n",
            "Step 18 of Epoch: 49; Loss 1.350071 \n",
            "Step 19 of Epoch: 49; Loss 1.350956 \n",
            "Step 20 of Epoch: 49; Loss 1.351256 \n",
            "Step 21 of Epoch: 49; Loss 1.352677 \n",
            "Step 22 of Epoch: 49; Loss 1.352707 \n",
            "Step 23 of Epoch: 49; Loss 1.353480 \n",
            "Step 24 of Epoch: 49; Loss 1.354023 \n",
            "Step 25 of Epoch: 49; Loss 1.355816 \n",
            "Step 26 of Epoch: 49; Loss 1.356490 \n",
            "Step 27 of Epoch: 49; Loss 1.356551 \n",
            "Step 28 of Epoch: 49; Loss 1.357301 \n",
            "Step 29 of Epoch: 49; Loss 1.357228 \n",
            "Step 30 of Epoch: 49; Loss 1.357454 \n",
            "Step 31 of Epoch: 49; Loss 1.357380 \n",
            "Step 32 of Epoch: 49; Loss 1.357741 \n",
            "Step 33 of Epoch: 49; Loss 1.358723 \n",
            "Step 34 of Epoch: 49; Loss 1.358665 \n",
            "Step 35 of Epoch: 49; Loss 1.358710 \n",
            "Step 36 of Epoch: 49; Loss 1.358944 \n",
            "Step 37 of Epoch: 49; Loss 1.359221 \n",
            "Step 38 of Epoch: 49; Loss 1.359948 \n",
            "Step 39 of Epoch: 49; Loss 1.360540 \n",
            "Step 40 of Epoch: 49; Loss 1.361095 \n",
            "Final Result for Epoch 49: Loss 1.361127; Val Acc 0.293344; Train Acc 0.451301\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 50; Loss 1.352428 \n",
            "Step 2 of Epoch: 50; Loss 1.347432 \n",
            "Step 3 of Epoch: 50; Loss 1.348230 \n",
            "Step 4 of Epoch: 50; Loss 1.345975 \n",
            "Step 5 of Epoch: 50; Loss 1.347129 \n",
            "Step 6 of Epoch: 50; Loss 1.346696 \n",
            "Step 7 of Epoch: 50; Loss 1.345658 \n",
            "Step 8 of Epoch: 50; Loss 1.345825 \n",
            "Step 9 of Epoch: 50; Loss 1.345478 \n",
            "Step 10 of Epoch: 50; Loss 1.346538 \n",
            "Step 11 of Epoch: 50; Loss 1.347537 \n",
            "Step 12 of Epoch: 50; Loss 1.346961 \n",
            "Step 13 of Epoch: 50; Loss 1.347243 \n",
            "Step 14 of Epoch: 50; Loss 1.348579 \n",
            "Step 15 of Epoch: 50; Loss 1.348376 \n",
            "Step 16 of Epoch: 50; Loss 1.347587 \n",
            "Step 17 of Epoch: 50; Loss 1.348225 \n",
            "Step 18 of Epoch: 50; Loss 1.348851 \n",
            "Step 19 of Epoch: 50; Loss 1.350203 \n",
            "Step 20 of Epoch: 50; Loss 1.351330 \n",
            "Step 21 of Epoch: 50; Loss 1.351592 \n",
            "Step 22 of Epoch: 50; Loss 1.351765 \n",
            "Step 23 of Epoch: 50; Loss 1.352631 \n",
            "Step 24 of Epoch: 50; Loss 1.353632 \n",
            "Step 25 of Epoch: 50; Loss 1.353579 \n",
            "Step 26 of Epoch: 50; Loss 1.353134 \n",
            "Step 27 of Epoch: 50; Loss 1.353883 \n",
            "Step 28 of Epoch: 50; Loss 1.353690 \n",
            "Step 29 of Epoch: 50; Loss 1.353770 \n",
            "Step 30 of Epoch: 50; Loss 1.354204 \n",
            "Step 31 of Epoch: 50; Loss 1.354582 \n",
            "Step 32 of Epoch: 50; Loss 1.355417 \n",
            "Step 33 of Epoch: 50; Loss 1.356144 \n",
            "Step 34 of Epoch: 50; Loss 1.356781 \n",
            "Step 35 of Epoch: 50; Loss 1.356937 \n",
            "Step 36 of Epoch: 50; Loss 1.357512 \n",
            "Step 37 of Epoch: 50; Loss 1.357920 \n",
            "Step 38 of Epoch: 50; Loss 1.358103 \n",
            "Step 39 of Epoch: 50; Loss 1.358267 \n",
            "Step 40 of Epoch: 50; Loss 1.358993 \n",
            "Final Result for Epoch 50: Loss 1.358985; Val Acc 0.297063; Train Acc 0.451602\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 51; Loss 1.333285 \n",
            "Step 2 of Epoch: 51; Loss 1.334193 \n",
            "Step 3 of Epoch: 51; Loss 1.332282 \n",
            "Step 4 of Epoch: 51; Loss 1.333362 \n",
            "Step 5 of Epoch: 51; Loss 1.335949 \n",
            "Step 6 of Epoch: 51; Loss 1.337732 \n",
            "Step 7 of Epoch: 51; Loss 1.336761 \n",
            "Step 8 of Epoch: 51; Loss 1.337196 \n",
            "Step 9 of Epoch: 51; Loss 1.337449 \n",
            "Step 10 of Epoch: 51; Loss 1.338195 \n",
            "Step 11 of Epoch: 51; Loss 1.338953 \n",
            "Step 12 of Epoch: 51; Loss 1.339956 \n",
            "Step 13 of Epoch: 51; Loss 1.341012 \n",
            "Step 14 of Epoch: 51; Loss 1.341600 \n",
            "Step 15 of Epoch: 51; Loss 1.342908 \n",
            "Step 16 of Epoch: 51; Loss 1.343339 \n",
            "Step 17 of Epoch: 51; Loss 1.344417 \n",
            "Step 18 of Epoch: 51; Loss 1.345840 \n",
            "Step 19 of Epoch: 51; Loss 1.346078 \n",
            "Step 20 of Epoch: 51; Loss 1.346347 \n",
            "Step 21 of Epoch: 51; Loss 1.347562 \n",
            "Step 22 of Epoch: 51; Loss 1.346542 \n",
            "Step 23 of Epoch: 51; Loss 1.347752 \n",
            "Step 24 of Epoch: 51; Loss 1.348363 \n",
            "Step 25 of Epoch: 51; Loss 1.349460 \n",
            "Step 26 of Epoch: 51; Loss 1.350087 \n",
            "Step 27 of Epoch: 51; Loss 1.350333 \n",
            "Step 28 of Epoch: 51; Loss 1.350991 \n",
            "Step 29 of Epoch: 51; Loss 1.350849 \n",
            "Step 30 of Epoch: 51; Loss 1.351756 \n",
            "Step 31 of Epoch: 51; Loss 1.352262 \n",
            "Step 32 of Epoch: 51; Loss 1.352240 \n",
            "Step 33 of Epoch: 51; Loss 1.353076 \n",
            "Step 34 of Epoch: 51; Loss 1.353672 \n",
            "Step 35 of Epoch: 51; Loss 1.353834 \n",
            "Step 36 of Epoch: 51; Loss 1.354126 \n",
            "Step 37 of Epoch: 51; Loss 1.354425 \n",
            "Step 38 of Epoch: 51; Loss 1.354571 \n",
            "Step 39 of Epoch: 51; Loss 1.355213 \n",
            "Step 40 of Epoch: 51; Loss 1.355591 \n",
            "Final Result for Epoch 51: Loss 1.355595; Val Acc 0.292250; Train Acc 0.452965\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 52; Loss 1.328401 \n",
            "Step 2 of Epoch: 52; Loss 1.326867 \n",
            "Step 3 of Epoch: 52; Loss 1.327228 \n",
            "Step 4 of Epoch: 52; Loss 1.326248 \n",
            "Step 5 of Epoch: 52; Loss 1.327990 \n",
            "Step 6 of Epoch: 52; Loss 1.330920 \n",
            "Step 7 of Epoch: 52; Loss 1.332315 \n",
            "Step 8 of Epoch: 52; Loss 1.331881 \n",
            "Step 9 of Epoch: 52; Loss 1.333379 \n",
            "Step 10 of Epoch: 52; Loss 1.334628 \n",
            "Step 11 of Epoch: 52; Loss 1.334312 \n",
            "Step 12 of Epoch: 52; Loss 1.334397 \n",
            "Step 13 of Epoch: 52; Loss 1.334839 \n",
            "Step 14 of Epoch: 52; Loss 1.336735 \n",
            "Step 15 of Epoch: 52; Loss 1.337238 \n",
            "Step 16 of Epoch: 52; Loss 1.338285 \n",
            "Step 17 of Epoch: 52; Loss 1.338992 \n",
            "Step 18 of Epoch: 52; Loss 1.340237 \n",
            "Step 19 of Epoch: 52; Loss 1.340244 \n",
            "Step 20 of Epoch: 52; Loss 1.342416 \n",
            "Step 21 of Epoch: 52; Loss 1.342115 \n",
            "Step 22 of Epoch: 52; Loss 1.342968 \n",
            "Step 23 of Epoch: 52; Loss 1.344298 \n",
            "Step 24 of Epoch: 52; Loss 1.345267 \n",
            "Step 25 of Epoch: 52; Loss 1.345608 \n",
            "Step 26 of Epoch: 52; Loss 1.346322 \n",
            "Step 27 of Epoch: 52; Loss 1.346622 \n",
            "Step 28 of Epoch: 52; Loss 1.347382 \n",
            "Step 29 of Epoch: 52; Loss 1.347668 \n",
            "Step 30 of Epoch: 52; Loss 1.348281 \n",
            "Step 31 of Epoch: 52; Loss 1.348717 \n",
            "Step 32 of Epoch: 52; Loss 1.349826 \n",
            "Step 33 of Epoch: 52; Loss 1.350156 \n",
            "Step 34 of Epoch: 52; Loss 1.350474 \n",
            "Step 35 of Epoch: 52; Loss 1.351041 \n",
            "Step 36 of Epoch: 52; Loss 1.351917 \n",
            "Step 37 of Epoch: 52; Loss 1.352548 \n",
            "Step 38 of Epoch: 52; Loss 1.352921 \n",
            "Step 39 of Epoch: 52; Loss 1.353634 \n",
            "Step 40 of Epoch: 52; Loss 1.353469 \n",
            "Final Result for Epoch 52: Loss 1.353517; Val Acc 0.292063; Train Acc 0.455468\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 53; Loss 1.332290 \n",
            "Step 2 of Epoch: 53; Loss 1.327709 \n",
            "Step 3 of Epoch: 53; Loss 1.327466 \n",
            "Step 4 of Epoch: 53; Loss 1.330701 \n",
            "Step 5 of Epoch: 53; Loss 1.331343 \n",
            "Step 6 of Epoch: 53; Loss 1.329422 \n",
            "Step 7 of Epoch: 53; Loss 1.330623 \n",
            "Step 8 of Epoch: 53; Loss 1.332095 \n",
            "Step 9 of Epoch: 53; Loss 1.331026 \n",
            "Step 10 of Epoch: 53; Loss 1.331327 \n",
            "Step 11 of Epoch: 53; Loss 1.330892 \n",
            "Step 12 of Epoch: 53; Loss 1.331854 \n",
            "Step 13 of Epoch: 53; Loss 1.332237 \n",
            "Step 14 of Epoch: 53; Loss 1.332226 \n",
            "Step 15 of Epoch: 53; Loss 1.332810 \n",
            "Step 16 of Epoch: 53; Loss 1.333403 \n",
            "Step 17 of Epoch: 53; Loss 1.333674 \n",
            "Step 18 of Epoch: 53; Loss 1.335282 \n",
            "Step 19 of Epoch: 53; Loss 1.336571 \n",
            "Step 20 of Epoch: 53; Loss 1.336985 \n",
            "Step 21 of Epoch: 53; Loss 1.338631 \n",
            "Step 22 of Epoch: 53; Loss 1.339722 \n",
            "Step 23 of Epoch: 53; Loss 1.340465 \n",
            "Step 24 of Epoch: 53; Loss 1.341199 \n",
            "Step 25 of Epoch: 53; Loss 1.341982 \n",
            "Step 26 of Epoch: 53; Loss 1.342181 \n",
            "Step 27 of Epoch: 53; Loss 1.343225 \n",
            "Step 28 of Epoch: 53; Loss 1.343763 \n",
            "Step 29 of Epoch: 53; Loss 1.344284 \n",
            "Step 30 of Epoch: 53; Loss 1.345388 \n",
            "Step 31 of Epoch: 53; Loss 1.346206 \n",
            "Step 32 of Epoch: 53; Loss 1.346840 \n",
            "Step 33 of Epoch: 53; Loss 1.347242 \n",
            "Step 34 of Epoch: 53; Loss 1.348144 \n",
            "Step 35 of Epoch: 53; Loss 1.348124 \n",
            "Step 36 of Epoch: 53; Loss 1.348113 \n",
            "Step 37 of Epoch: 53; Loss 1.348910 \n",
            "Step 38 of Epoch: 53; Loss 1.349567 \n",
            "Step 39 of Epoch: 53; Loss 1.350303 \n",
            "Step 40 of Epoch: 53; Loss 1.350976 \n",
            "Final Result for Epoch 53: Loss 1.350993; Val Acc 0.294438; Train Acc 0.453336\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 54; Loss 1.325694 \n",
            "Step 2 of Epoch: 54; Loss 1.322429 \n",
            "Step 3 of Epoch: 54; Loss 1.323082 \n",
            "Step 4 of Epoch: 54; Loss 1.325897 \n",
            "Step 5 of Epoch: 54; Loss 1.329978 \n",
            "Step 6 of Epoch: 54; Loss 1.330942 \n",
            "Step 7 of Epoch: 54; Loss 1.330994 \n",
            "Step 8 of Epoch: 54; Loss 1.332179 \n",
            "Step 9 of Epoch: 54; Loss 1.332764 \n",
            "Step 10 of Epoch: 54; Loss 1.333211 \n",
            "Step 11 of Epoch: 54; Loss 1.333336 \n",
            "Step 12 of Epoch: 54; Loss 1.334630 \n",
            "Step 13 of Epoch: 54; Loss 1.335426 \n",
            "Step 14 of Epoch: 54; Loss 1.336579 \n",
            "Step 15 of Epoch: 54; Loss 1.337140 \n",
            "Step 16 of Epoch: 54; Loss 1.338223 \n",
            "Step 17 of Epoch: 54; Loss 1.338312 \n",
            "Step 18 of Epoch: 54; Loss 1.338085 \n",
            "Step 19 of Epoch: 54; Loss 1.337702 \n",
            "Step 20 of Epoch: 54; Loss 1.338426 \n",
            "Step 21 of Epoch: 54; Loss 1.338476 \n",
            "Step 22 of Epoch: 54; Loss 1.338965 \n",
            "Step 23 of Epoch: 54; Loss 1.339802 \n",
            "Step 24 of Epoch: 54; Loss 1.340119 \n",
            "Step 25 of Epoch: 54; Loss 1.340015 \n",
            "Step 26 of Epoch: 54; Loss 1.340463 \n",
            "Step 27 of Epoch: 54; Loss 1.341506 \n",
            "Step 28 of Epoch: 54; Loss 1.342443 \n",
            "Step 29 of Epoch: 54; Loss 1.343289 \n",
            "Step 30 of Epoch: 54; Loss 1.343556 \n",
            "Step 31 of Epoch: 54; Loss 1.344295 \n",
            "Step 32 of Epoch: 54; Loss 1.344937 \n",
            "Step 33 of Epoch: 54; Loss 1.345251 \n",
            "Step 34 of Epoch: 54; Loss 1.345531 \n",
            "Step 35 of Epoch: 54; Loss 1.345847 \n",
            "Step 36 of Epoch: 54; Loss 1.346000 \n",
            "Step 37 of Epoch: 54; Loss 1.346257 \n",
            "Step 38 of Epoch: 54; Loss 1.346361 \n",
            "Step 39 of Epoch: 54; Loss 1.347226 \n",
            "Step 40 of Epoch: 54; Loss 1.347554 \n",
            "Final Result for Epoch 54: Loss 1.347556; Val Acc 0.293875; Train Acc 0.456019\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 55; Loss 1.299220 \n",
            "Step 2 of Epoch: 55; Loss 1.315179 \n",
            "Step 3 of Epoch: 55; Loss 1.316591 \n",
            "Step 4 of Epoch: 55; Loss 1.319447 \n",
            "Step 5 of Epoch: 55; Loss 1.318801 \n",
            "Step 6 of Epoch: 55; Loss 1.318650 \n",
            "Step 7 of Epoch: 55; Loss 1.323276 \n",
            "Step 8 of Epoch: 55; Loss 1.324594 \n",
            "Step 9 of Epoch: 55; Loss 1.326779 \n",
            "Step 10 of Epoch: 55; Loss 1.329550 \n",
            "Step 11 of Epoch: 55; Loss 1.330839 \n",
            "Step 12 of Epoch: 55; Loss 1.330797 \n",
            "Step 13 of Epoch: 55; Loss 1.331971 \n",
            "Step 14 of Epoch: 55; Loss 1.331436 \n",
            "Step 15 of Epoch: 55; Loss 1.332472 \n",
            "Step 16 of Epoch: 55; Loss 1.332725 \n",
            "Step 17 of Epoch: 55; Loss 1.334235 \n",
            "Step 18 of Epoch: 55; Loss 1.335063 \n",
            "Step 19 of Epoch: 55; Loss 1.337363 \n",
            "Step 20 of Epoch: 55; Loss 1.337621 \n",
            "Step 21 of Epoch: 55; Loss 1.337725 \n",
            "Step 22 of Epoch: 55; Loss 1.338579 \n",
            "Step 23 of Epoch: 55; Loss 1.339322 \n",
            "Step 24 of Epoch: 55; Loss 1.340288 \n",
            "Step 25 of Epoch: 55; Loss 1.340470 \n",
            "Step 26 of Epoch: 55; Loss 1.339778 \n",
            "Step 27 of Epoch: 55; Loss 1.340309 \n",
            "Step 28 of Epoch: 55; Loss 1.340629 \n",
            "Step 29 of Epoch: 55; Loss 1.340835 \n",
            "Step 30 of Epoch: 55; Loss 1.340692 \n",
            "Step 31 of Epoch: 55; Loss 1.340974 \n",
            "Step 32 of Epoch: 55; Loss 1.341415 \n",
            "Step 33 of Epoch: 55; Loss 1.341671 \n",
            "Step 34 of Epoch: 55; Loss 1.342174 \n",
            "Step 35 of Epoch: 55; Loss 1.342657 \n",
            "Step 36 of Epoch: 55; Loss 1.343063 \n",
            "Step 37 of Epoch: 55; Loss 1.343468 \n",
            "Step 38 of Epoch: 55; Loss 1.344192 \n",
            "Step 39 of Epoch: 55; Loss 1.344826 \n",
            "Step 40 of Epoch: 55; Loss 1.345222 \n",
            "Final Result for Epoch 55: Loss 1.345250; Val Acc 0.293375; Train Acc 0.457788\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 56; Loss 1.316269 \n",
            "Step 2 of Epoch: 56; Loss 1.325920 \n",
            "Step 3 of Epoch: 56; Loss 1.322044 \n",
            "Step 4 of Epoch: 56; Loss 1.323023 \n",
            "Step 5 of Epoch: 56; Loss 1.321850 \n",
            "Step 6 of Epoch: 56; Loss 1.319682 \n",
            "Step 7 of Epoch: 56; Loss 1.320451 \n",
            "Step 8 of Epoch: 56; Loss 1.322539 \n",
            "Step 9 of Epoch: 56; Loss 1.324890 \n",
            "Step 10 of Epoch: 56; Loss 1.326145 \n",
            "Step 11 of Epoch: 56; Loss 1.326558 \n",
            "Step 12 of Epoch: 56; Loss 1.327790 \n",
            "Step 13 of Epoch: 56; Loss 1.328960 \n",
            "Step 14 of Epoch: 56; Loss 1.329981 \n",
            "Step 15 of Epoch: 56; Loss 1.330829 \n",
            "Step 16 of Epoch: 56; Loss 1.330855 \n",
            "Step 17 of Epoch: 56; Loss 1.330888 \n",
            "Step 18 of Epoch: 56; Loss 1.330682 \n",
            "Step 19 of Epoch: 56; Loss 1.331546 \n",
            "Step 20 of Epoch: 56; Loss 1.333185 \n",
            "Step 21 of Epoch: 56; Loss 1.333114 \n",
            "Step 22 of Epoch: 56; Loss 1.333009 \n",
            "Step 23 of Epoch: 56; Loss 1.332858 \n",
            "Step 24 of Epoch: 56; Loss 1.332511 \n",
            "Step 25 of Epoch: 56; Loss 1.333384 \n",
            "Step 26 of Epoch: 56; Loss 1.334169 \n",
            "Step 27 of Epoch: 56; Loss 1.334783 \n",
            "Step 28 of Epoch: 56; Loss 1.335230 \n",
            "Step 29 of Epoch: 56; Loss 1.336066 \n",
            "Step 30 of Epoch: 56; Loss 1.336967 \n",
            "Step 31 of Epoch: 56; Loss 1.337779 \n",
            "Step 32 of Epoch: 56; Loss 1.338732 \n",
            "Step 33 of Epoch: 56; Loss 1.339035 \n",
            "Step 34 of Epoch: 56; Loss 1.339510 \n",
            "Step 35 of Epoch: 56; Loss 1.340842 \n",
            "Step 36 of Epoch: 56; Loss 1.341550 \n",
            "Step 37 of Epoch: 56; Loss 1.342239 \n",
            "Step 38 of Epoch: 56; Loss 1.342571 \n",
            "Step 39 of Epoch: 56; Loss 1.343093 \n",
            "Step 40 of Epoch: 56; Loss 1.343509 \n",
            "Final Result for Epoch 56: Loss 1.343495; Val Acc 0.293187; Train Acc 0.458479\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 57; Loss 1.318388 \n",
            "Step 2 of Epoch: 57; Loss 1.316179 \n",
            "Step 3 of Epoch: 57; Loss 1.319797 \n",
            "Step 4 of Epoch: 57; Loss 1.319740 \n",
            "Step 5 of Epoch: 57; Loss 1.321463 \n",
            "Step 6 of Epoch: 57; Loss 1.322567 \n",
            "Step 7 of Epoch: 57; Loss 1.323830 \n",
            "Step 8 of Epoch: 57; Loss 1.324136 \n",
            "Step 9 of Epoch: 57; Loss 1.326768 \n",
            "Step 10 of Epoch: 57; Loss 1.328499 \n",
            "Step 11 of Epoch: 57; Loss 1.328621 \n",
            "Step 12 of Epoch: 57; Loss 1.328673 \n",
            "Step 13 of Epoch: 57; Loss 1.328040 \n",
            "Step 14 of Epoch: 57; Loss 1.328986 \n",
            "Step 15 of Epoch: 57; Loss 1.329094 \n",
            "Step 16 of Epoch: 57; Loss 1.329167 \n",
            "Step 17 of Epoch: 57; Loss 1.329745 \n",
            "Step 18 of Epoch: 57; Loss 1.330520 \n",
            "Step 19 of Epoch: 57; Loss 1.331776 \n",
            "Step 20 of Epoch: 57; Loss 1.331883 \n",
            "Step 21 of Epoch: 57; Loss 1.332279 \n",
            "Step 22 of Epoch: 57; Loss 1.332977 \n",
            "Step 23 of Epoch: 57; Loss 1.333262 \n",
            "Step 24 of Epoch: 57; Loss 1.333884 \n",
            "Step 25 of Epoch: 57; Loss 1.334503 \n",
            "Step 26 of Epoch: 57; Loss 1.335251 \n",
            "Step 27 of Epoch: 57; Loss 1.335450 \n",
            "Step 28 of Epoch: 57; Loss 1.336510 \n",
            "Step 29 of Epoch: 57; Loss 1.336868 \n",
            "Step 30 of Epoch: 57; Loss 1.336950 \n",
            "Step 31 of Epoch: 57; Loss 1.337782 \n",
            "Step 32 of Epoch: 57; Loss 1.338238 \n",
            "Step 33 of Epoch: 57; Loss 1.338678 \n",
            "Step 34 of Epoch: 57; Loss 1.338664 \n",
            "Step 35 of Epoch: 57; Loss 1.339480 \n",
            "Step 36 of Epoch: 57; Loss 1.339744 \n",
            "Step 37 of Epoch: 57; Loss 1.340131 \n",
            "Step 38 of Epoch: 57; Loss 1.340915 \n",
            "Step 39 of Epoch: 57; Loss 1.341111 \n",
            "Step 40 of Epoch: 57; Loss 1.341716 \n",
            "Final Result for Epoch 57: Loss 1.341713; Val Acc 0.293719; Train Acc 0.455882\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 58; Loss 1.319071 \n",
            "Step 2 of Epoch: 58; Loss 1.324883 \n",
            "Step 3 of Epoch: 58; Loss 1.322998 \n",
            "Step 4 of Epoch: 58; Loss 1.321379 \n",
            "Step 5 of Epoch: 58; Loss 1.320821 \n",
            "Step 6 of Epoch: 58; Loss 1.323192 \n",
            "Step 7 of Epoch: 58; Loss 1.320973 \n",
            "Step 8 of Epoch: 58; Loss 1.323060 \n",
            "Step 9 of Epoch: 58; Loss 1.324138 \n",
            "Step 10 of Epoch: 58; Loss 1.324747 \n",
            "Step 11 of Epoch: 58; Loss 1.327202 \n",
            "Step 12 of Epoch: 58; Loss 1.328264 \n",
            "Step 13 of Epoch: 58; Loss 1.328565 \n",
            "Step 14 of Epoch: 58; Loss 1.329178 \n",
            "Step 15 of Epoch: 58; Loss 1.328574 \n",
            "Step 16 of Epoch: 58; Loss 1.328775 \n",
            "Step 17 of Epoch: 58; Loss 1.329286 \n",
            "Step 18 of Epoch: 58; Loss 1.329005 \n",
            "Step 19 of Epoch: 58; Loss 1.330622 \n",
            "Step 20 of Epoch: 58; Loss 1.330809 \n",
            "Step 21 of Epoch: 58; Loss 1.331001 \n",
            "Step 22 of Epoch: 58; Loss 1.331035 \n",
            "Step 23 of Epoch: 58; Loss 1.332355 \n",
            "Step 24 of Epoch: 58; Loss 1.333503 \n",
            "Step 25 of Epoch: 58; Loss 1.334245 \n",
            "Step 26 of Epoch: 58; Loss 1.334334 \n",
            "Step 27 of Epoch: 58; Loss 1.335561 \n",
            "Step 28 of Epoch: 58; Loss 1.335777 \n",
            "Step 29 of Epoch: 58; Loss 1.335798 \n",
            "Step 30 of Epoch: 58; Loss 1.336110 \n",
            "Step 31 of Epoch: 58; Loss 1.336224 \n",
            "Step 32 of Epoch: 58; Loss 1.336445 \n",
            "Step 33 of Epoch: 58; Loss 1.337196 \n",
            "Step 34 of Epoch: 58; Loss 1.337460 \n",
            "Step 35 of Epoch: 58; Loss 1.337839 \n",
            "Step 36 of Epoch: 58; Loss 1.338039 \n",
            "Step 37 of Epoch: 58; Loss 1.338285 \n",
            "Step 38 of Epoch: 58; Loss 1.338516 \n",
            "Step 39 of Epoch: 58; Loss 1.338907 \n",
            "Step 40 of Epoch: 58; Loss 1.339122 \n",
            "Final Result for Epoch 58: Loss 1.339107; Val Acc 0.291687; Train Acc 0.458456\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 59; Loss 1.305730 \n",
            "Step 2 of Epoch: 59; Loss 1.314839 \n",
            "Step 3 of Epoch: 59; Loss 1.314708 \n",
            "Step 4 of Epoch: 59; Loss 1.318926 \n",
            "Step 5 of Epoch: 59; Loss 1.320174 \n",
            "Step 6 of Epoch: 59; Loss 1.321813 \n",
            "Step 7 of Epoch: 59; Loss 1.324264 \n",
            "Step 8 of Epoch: 59; Loss 1.322491 \n",
            "Step 9 of Epoch: 59; Loss 1.323548 \n",
            "Step 10 of Epoch: 59; Loss 1.324057 \n",
            "Step 11 of Epoch: 59; Loss 1.324257 \n",
            "Step 12 of Epoch: 59; Loss 1.325139 \n",
            "Step 13 of Epoch: 59; Loss 1.325755 \n",
            "Step 14 of Epoch: 59; Loss 1.325348 \n",
            "Step 15 of Epoch: 59; Loss 1.325761 \n",
            "Step 16 of Epoch: 59; Loss 1.325713 \n",
            "Step 17 of Epoch: 59; Loss 1.327039 \n",
            "Step 18 of Epoch: 59; Loss 1.326650 \n",
            "Step 19 of Epoch: 59; Loss 1.326867 \n",
            "Step 20 of Epoch: 59; Loss 1.327767 \n",
            "Step 21 of Epoch: 59; Loss 1.328394 \n",
            "Step 22 of Epoch: 59; Loss 1.328828 \n",
            "Step 23 of Epoch: 59; Loss 1.328459 \n",
            "Step 24 of Epoch: 59; Loss 1.329223 \n",
            "Step 25 of Epoch: 59; Loss 1.329679 \n",
            "Step 26 of Epoch: 59; Loss 1.330251 \n",
            "Step 27 of Epoch: 59; Loss 1.330491 \n",
            "Step 28 of Epoch: 59; Loss 1.331201 \n",
            "Step 29 of Epoch: 59; Loss 1.331372 \n",
            "Step 30 of Epoch: 59; Loss 1.331501 \n",
            "Step 31 of Epoch: 59; Loss 1.331927 \n",
            "Step 32 of Epoch: 59; Loss 1.332159 \n",
            "Step 33 of Epoch: 59; Loss 1.333364 \n",
            "Step 34 of Epoch: 59; Loss 1.333980 \n",
            "Step 35 of Epoch: 59; Loss 1.334606 \n",
            "Step 36 of Epoch: 59; Loss 1.335041 \n",
            "Step 37 of Epoch: 59; Loss 1.335385 \n",
            "Step 38 of Epoch: 59; Loss 1.335889 \n",
            "Step 39 of Epoch: 59; Loss 1.336197 \n",
            "Step 40 of Epoch: 59; Loss 1.336899 \n",
            "Final Result for Epoch 59: Loss 1.336873; Val Acc 0.291156; Train Acc 0.459412\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 60; Loss 1.311566 \n",
            "Step 2 of Epoch: 60; Loss 1.317165 \n",
            "Step 3 of Epoch: 60; Loss 1.315265 \n",
            "Step 4 of Epoch: 60; Loss 1.314152 \n",
            "Step 5 of Epoch: 60; Loss 1.313923 \n",
            "Step 6 of Epoch: 60; Loss 1.315154 \n",
            "Step 7 of Epoch: 60; Loss 1.315217 \n",
            "Step 8 of Epoch: 60; Loss 1.316305 \n",
            "Step 9 of Epoch: 60; Loss 1.317328 \n",
            "Step 10 of Epoch: 60; Loss 1.318846 \n",
            "Step 11 of Epoch: 60; Loss 1.320030 \n",
            "Step 12 of Epoch: 60; Loss 1.322028 \n",
            "Step 13 of Epoch: 60; Loss 1.322447 \n",
            "Step 14 of Epoch: 60; Loss 1.324110 \n",
            "Step 15 of Epoch: 60; Loss 1.324339 \n",
            "Step 16 of Epoch: 60; Loss 1.323791 \n",
            "Step 17 of Epoch: 60; Loss 1.324836 \n",
            "Step 18 of Epoch: 60; Loss 1.325538 \n",
            "Step 19 of Epoch: 60; Loss 1.326946 \n",
            "Step 20 of Epoch: 60; Loss 1.327147 \n",
            "Step 21 of Epoch: 60; Loss 1.327839 \n",
            "Step 22 of Epoch: 60; Loss 1.328562 \n",
            "Step 23 of Epoch: 60; Loss 1.329583 \n",
            "Step 24 of Epoch: 60; Loss 1.330330 \n",
            "Step 25 of Epoch: 60; Loss 1.330730 \n",
            "Step 26 of Epoch: 60; Loss 1.330646 \n",
            "Step 27 of Epoch: 60; Loss 1.330596 \n",
            "Step 28 of Epoch: 60; Loss 1.330703 \n",
            "Step 29 of Epoch: 60; Loss 1.331106 \n",
            "Step 30 of Epoch: 60; Loss 1.331127 \n",
            "Step 31 of Epoch: 60; Loss 1.331336 \n",
            "Step 32 of Epoch: 60; Loss 1.331432 \n",
            "Step 33 of Epoch: 60; Loss 1.331975 \n",
            "Step 34 of Epoch: 60; Loss 1.332454 \n",
            "Step 35 of Epoch: 60; Loss 1.332484 \n",
            "Step 36 of Epoch: 60; Loss 1.333073 \n",
            "Step 37 of Epoch: 60; Loss 1.333479 \n",
            "Step 38 of Epoch: 60; Loss 1.333928 \n",
            "Step 39 of Epoch: 60; Loss 1.334355 \n",
            "Step 40 of Epoch: 60; Loss 1.334587 \n",
            "Final Result for Epoch 60: Loss 1.334558; Val Acc 0.291187; Train Acc 0.462006\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 61; Loss 1.319171 \n",
            "Step 2 of Epoch: 61; Loss 1.320662 \n",
            "Step 3 of Epoch: 61; Loss 1.320275 \n",
            "Step 4 of Epoch: 61; Loss 1.313878 \n",
            "Step 5 of Epoch: 61; Loss 1.315571 \n",
            "Step 6 of Epoch: 61; Loss 1.319056 \n",
            "Step 7 of Epoch: 61; Loss 1.321873 \n",
            "Step 8 of Epoch: 61; Loss 1.320259 \n",
            "Step 9 of Epoch: 61; Loss 1.318716 \n",
            "Step 10 of Epoch: 61; Loss 1.319591 \n",
            "Step 11 of Epoch: 61; Loss 1.319024 \n",
            "Step 12 of Epoch: 61; Loss 1.319566 \n",
            "Step 13 of Epoch: 61; Loss 1.320660 \n",
            "Step 14 of Epoch: 61; Loss 1.322000 \n",
            "Step 15 of Epoch: 61; Loss 1.322821 \n",
            "Step 16 of Epoch: 61; Loss 1.323700 \n",
            "Step 17 of Epoch: 61; Loss 1.323299 \n",
            "Step 18 of Epoch: 61; Loss 1.324955 \n",
            "Step 19 of Epoch: 61; Loss 1.324435 \n",
            "Step 20 of Epoch: 61; Loss 1.324794 \n",
            "Step 21 of Epoch: 61; Loss 1.326251 \n",
            "Step 22 of Epoch: 61; Loss 1.326366 \n",
            "Step 23 of Epoch: 61; Loss 1.327453 \n",
            "Step 24 of Epoch: 61; Loss 1.327841 \n",
            "Step 25 of Epoch: 61; Loss 1.328113 \n",
            "Step 26 of Epoch: 61; Loss 1.328424 \n",
            "Step 27 of Epoch: 61; Loss 1.329737 \n",
            "Step 28 of Epoch: 61; Loss 1.329596 \n",
            "Step 29 of Epoch: 61; Loss 1.329978 \n",
            "Step 30 of Epoch: 61; Loss 1.330613 \n",
            "Step 31 of Epoch: 61; Loss 1.330656 \n",
            "Step 32 of Epoch: 61; Loss 1.330683 \n",
            "Step 33 of Epoch: 61; Loss 1.331356 \n",
            "Step 34 of Epoch: 61; Loss 1.331696 \n",
            "Step 35 of Epoch: 61; Loss 1.331745 \n",
            "Step 36 of Epoch: 61; Loss 1.332025 \n",
            "Step 37 of Epoch: 61; Loss 1.332391 \n",
            "Step 38 of Epoch: 61; Loss 1.332572 \n",
            "Step 39 of Epoch: 61; Loss 1.333092 \n",
            "Step 40 of Epoch: 61; Loss 1.333401 \n",
            "Final Result for Epoch 61: Loss 1.333418; Val Acc 0.289281; Train Acc 0.462634\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 62; Loss 1.300228 \n",
            "Step 2 of Epoch: 62; Loss 1.301504 \n",
            "Step 3 of Epoch: 62; Loss 1.307297 \n",
            "Step 4 of Epoch: 62; Loss 1.311621 \n",
            "Step 5 of Epoch: 62; Loss 1.313924 \n",
            "Step 6 of Epoch: 62; Loss 1.313480 \n",
            "Step 7 of Epoch: 62; Loss 1.314168 \n",
            "Step 8 of Epoch: 62; Loss 1.316063 \n",
            "Step 9 of Epoch: 62; Loss 1.316673 \n",
            "Step 10 of Epoch: 62; Loss 1.316273 \n",
            "Step 11 of Epoch: 62; Loss 1.316607 \n",
            "Step 12 of Epoch: 62; Loss 1.315808 \n",
            "Step 13 of Epoch: 62; Loss 1.317039 \n",
            "Step 14 of Epoch: 62; Loss 1.316396 \n",
            "Step 15 of Epoch: 62; Loss 1.317139 \n",
            "Step 16 of Epoch: 62; Loss 1.317677 \n",
            "Step 17 of Epoch: 62; Loss 1.319355 \n",
            "Step 18 of Epoch: 62; Loss 1.319343 \n",
            "Step 19 of Epoch: 62; Loss 1.319881 \n",
            "Step 20 of Epoch: 62; Loss 1.319916 \n",
            "Step 21 of Epoch: 62; Loss 1.320628 \n",
            "Step 22 of Epoch: 62; Loss 1.321344 \n",
            "Step 23 of Epoch: 62; Loss 1.323004 \n",
            "Step 24 of Epoch: 62; Loss 1.323155 \n",
            "Step 25 of Epoch: 62; Loss 1.324026 \n",
            "Step 26 of Epoch: 62; Loss 1.324023 \n",
            "Step 27 of Epoch: 62; Loss 1.325209 \n",
            "Step 28 of Epoch: 62; Loss 1.325476 \n",
            "Step 29 of Epoch: 62; Loss 1.325881 \n",
            "Step 30 of Epoch: 62; Loss 1.326507 \n",
            "Step 31 of Epoch: 62; Loss 1.326258 \n",
            "Step 32 of Epoch: 62; Loss 1.326764 \n",
            "Step 33 of Epoch: 62; Loss 1.327321 \n",
            "Step 34 of Epoch: 62; Loss 1.327878 \n",
            "Step 35 of Epoch: 62; Loss 1.328728 \n",
            "Step 36 of Epoch: 62; Loss 1.329258 \n",
            "Step 37 of Epoch: 62; Loss 1.329639 \n",
            "Step 38 of Epoch: 62; Loss 1.330447 \n",
            "Step 39 of Epoch: 62; Loss 1.330384 \n",
            "Step 40 of Epoch: 62; Loss 1.330791 \n",
            "Final Result for Epoch 62: Loss 1.330797; Val Acc 0.292000; Train Acc 0.462310\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 63; Loss 1.295902 \n",
            "Step 2 of Epoch: 63; Loss 1.306297 \n",
            "Step 3 of Epoch: 63; Loss 1.304636 \n",
            "Step 4 of Epoch: 63; Loss 1.307067 \n",
            "Step 5 of Epoch: 63; Loss 1.305596 \n",
            "Step 6 of Epoch: 63; Loss 1.308325 \n",
            "Step 7 of Epoch: 63; Loss 1.310690 \n",
            "Step 8 of Epoch: 63; Loss 1.311984 \n",
            "Step 9 of Epoch: 63; Loss 1.312737 \n",
            "Step 10 of Epoch: 63; Loss 1.314173 \n",
            "Step 11 of Epoch: 63; Loss 1.315108 \n",
            "Step 12 of Epoch: 63; Loss 1.315840 \n",
            "Step 13 of Epoch: 63; Loss 1.317356 \n",
            "Step 14 of Epoch: 63; Loss 1.317273 \n",
            "Step 15 of Epoch: 63; Loss 1.317773 \n",
            "Step 16 of Epoch: 63; Loss 1.318251 \n",
            "Step 17 of Epoch: 63; Loss 1.319328 \n",
            "Step 18 of Epoch: 63; Loss 1.320137 \n",
            "Step 19 of Epoch: 63; Loss 1.320757 \n",
            "Step 20 of Epoch: 63; Loss 1.321678 \n",
            "Step 21 of Epoch: 63; Loss 1.322188 \n",
            "Step 22 of Epoch: 63; Loss 1.323079 \n",
            "Step 23 of Epoch: 63; Loss 1.324308 \n",
            "Step 24 of Epoch: 63; Loss 1.324329 \n",
            "Step 25 of Epoch: 63; Loss 1.324576 \n",
            "Step 26 of Epoch: 63; Loss 1.325392 \n",
            "Step 27 of Epoch: 63; Loss 1.325346 \n",
            "Step 28 of Epoch: 63; Loss 1.325141 \n",
            "Step 29 of Epoch: 63; Loss 1.325407 \n",
            "Step 30 of Epoch: 63; Loss 1.325379 \n",
            "Step 31 of Epoch: 63; Loss 1.325823 \n",
            "Step 32 of Epoch: 63; Loss 1.326040 \n",
            "Step 33 of Epoch: 63; Loss 1.326801 \n",
            "Step 34 of Epoch: 63; Loss 1.327198 \n",
            "Step 35 of Epoch: 63; Loss 1.327622 \n",
            "Step 36 of Epoch: 63; Loss 1.328114 \n",
            "Step 37 of Epoch: 63; Loss 1.328605 \n",
            "Step 38 of Epoch: 63; Loss 1.328958 \n",
            "Step 39 of Epoch: 63; Loss 1.329187 \n",
            "Step 40 of Epoch: 63; Loss 1.329663 \n",
            "Final Result for Epoch 63: Loss 1.329676; Val Acc 0.290656; Train Acc 0.462693\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 64; Loss 1.304696 \n",
            "Step 2 of Epoch: 64; Loss 1.307784 \n",
            "Step 3 of Epoch: 64; Loss 1.334922 \n",
            "Step 4 of Epoch: 64; Loss 1.336750 \n",
            "Step 5 of Epoch: 64; Loss 1.335404 \n",
            "Step 6 of Epoch: 64; Loss 1.333495 \n",
            "Step 7 of Epoch: 64; Loss 1.336108 \n",
            "Step 8 of Epoch: 64; Loss 1.338075 \n",
            "Step 9 of Epoch: 64; Loss 1.335915 \n",
            "Step 10 of Epoch: 64; Loss 1.334036 \n",
            "Step 11 of Epoch: 64; Loss 1.331658 \n",
            "Step 12 of Epoch: 64; Loss 1.331753 \n",
            "Step 13 of Epoch: 64; Loss 1.330720 \n",
            "Step 14 of Epoch: 64; Loss 1.329607 \n",
            "Step 15 of Epoch: 64; Loss 1.329284 \n",
            "Step 16 of Epoch: 64; Loss 1.328924 \n",
            "Step 17 of Epoch: 64; Loss 1.327990 \n",
            "Step 18 of Epoch: 64; Loss 1.328201 \n",
            "Step 19 of Epoch: 64; Loss 1.327552 \n",
            "Step 20 of Epoch: 64; Loss 1.327920 \n",
            "Step 21 of Epoch: 64; Loss 1.328041 \n",
            "Step 22 of Epoch: 64; Loss 1.328443 \n",
            "Step 23 of Epoch: 64; Loss 1.327483 \n",
            "Step 24 of Epoch: 64; Loss 1.327831 \n",
            "Step 25 of Epoch: 64; Loss 1.327856 \n",
            "Step 26 of Epoch: 64; Loss 1.328246 \n",
            "Step 27 of Epoch: 64; Loss 1.328800 \n",
            "Step 28 of Epoch: 64; Loss 1.328509 \n",
            "Step 29 of Epoch: 64; Loss 1.328367 \n",
            "Step 30 of Epoch: 64; Loss 1.328373 \n",
            "Step 31 of Epoch: 64; Loss 1.328493 \n",
            "Step 32 of Epoch: 64; Loss 1.328723 \n",
            "Step 33 of Epoch: 64; Loss 1.329025 \n",
            "Step 34 of Epoch: 64; Loss 1.329255 \n",
            "Step 35 of Epoch: 64; Loss 1.328782 \n",
            "Step 36 of Epoch: 64; Loss 1.329157 \n",
            "Step 37 of Epoch: 64; Loss 1.329281 \n",
            "Step 38 of Epoch: 64; Loss 1.329974 \n",
            "Step 39 of Epoch: 64; Loss 1.330181 \n",
            "Step 40 of Epoch: 64; Loss 1.330228 \n",
            "Final Result for Epoch 64: Loss 1.330261; Val Acc 0.288969; Train Acc 0.461310\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 65; Loss 1.301327 \n",
            "Step 2 of Epoch: 65; Loss 1.310509 \n",
            "Step 3 of Epoch: 65; Loss 1.312084 \n",
            "Step 4 of Epoch: 65; Loss 1.311177 \n",
            "Step 5 of Epoch: 65; Loss 1.312374 \n",
            "Step 6 of Epoch: 65; Loss 1.313257 \n",
            "Step 7 of Epoch: 65; Loss 1.313304 \n",
            "Step 8 of Epoch: 65; Loss 1.312631 \n",
            "Step 9 of Epoch: 65; Loss 1.312694 \n",
            "Step 10 of Epoch: 65; Loss 1.311837 \n",
            "Step 11 of Epoch: 65; Loss 1.312463 \n",
            "Step 12 of Epoch: 65; Loss 1.311495 \n",
            "Step 13 of Epoch: 65; Loss 1.312475 \n",
            "Step 14 of Epoch: 65; Loss 1.312152 \n",
            "Step 15 of Epoch: 65; Loss 1.312682 \n",
            "Step 16 of Epoch: 65; Loss 1.312753 \n",
            "Step 17 of Epoch: 65; Loss 1.314765 \n",
            "Step 18 of Epoch: 65; Loss 1.316160 \n",
            "Step 19 of Epoch: 65; Loss 1.316708 \n",
            "Step 20 of Epoch: 65; Loss 1.317501 \n",
            "Step 21 of Epoch: 65; Loss 1.318534 \n",
            "Step 22 of Epoch: 65; Loss 1.318389 \n",
            "Step 23 of Epoch: 65; Loss 1.318737 \n",
            "Step 24 of Epoch: 65; Loss 1.318726 \n",
            "Step 25 of Epoch: 65; Loss 1.319033 \n",
            "Step 26 of Epoch: 65; Loss 1.319856 \n",
            "Step 27 of Epoch: 65; Loss 1.320641 \n",
            "Step 28 of Epoch: 65; Loss 1.320716 \n",
            "Step 29 of Epoch: 65; Loss 1.321827 \n",
            "Step 30 of Epoch: 65; Loss 1.322154 \n",
            "Step 31 of Epoch: 65; Loss 1.322252 \n",
            "Step 32 of Epoch: 65; Loss 1.322557 \n",
            "Step 33 of Epoch: 65; Loss 1.323433 \n",
            "Step 34 of Epoch: 65; Loss 1.323724 \n",
            "Step 35 of Epoch: 65; Loss 1.323978 \n",
            "Step 36 of Epoch: 65; Loss 1.324444 \n",
            "Step 37 of Epoch: 65; Loss 1.324235 \n",
            "Step 38 of Epoch: 65; Loss 1.324896 \n",
            "Step 39 of Epoch: 65; Loss 1.325491 \n",
            "Step 40 of Epoch: 65; Loss 1.325678 \n",
            "Final Result for Epoch 65: Loss 1.325692; Val Acc 0.291531; Train Acc 0.463806\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 66; Loss 1.291803 \n",
            "Step 2 of Epoch: 66; Loss 1.304728 \n",
            "Step 3 of Epoch: 66; Loss 1.303994 \n",
            "Step 4 of Epoch: 66; Loss 1.308582 \n",
            "Step 5 of Epoch: 66; Loss 1.308619 \n",
            "Step 6 of Epoch: 66; Loss 1.310004 \n",
            "Step 7 of Epoch: 66; Loss 1.307455 \n",
            "Step 8 of Epoch: 66; Loss 1.307905 \n",
            "Step 9 of Epoch: 66; Loss 1.307910 \n",
            "Step 10 of Epoch: 66; Loss 1.308722 \n",
            "Step 11 of Epoch: 66; Loss 1.306760 \n",
            "Step 12 of Epoch: 66; Loss 1.308277 \n",
            "Step 13 of Epoch: 66; Loss 1.310023 \n",
            "Step 14 of Epoch: 66; Loss 1.310176 \n",
            "Step 15 of Epoch: 66; Loss 1.311187 \n",
            "Step 16 of Epoch: 66; Loss 1.312038 \n",
            "Step 17 of Epoch: 66; Loss 1.313396 \n",
            "Step 18 of Epoch: 66; Loss 1.314529 \n",
            "Step 19 of Epoch: 66; Loss 1.315426 \n",
            "Step 20 of Epoch: 66; Loss 1.315552 \n",
            "Step 21 of Epoch: 66; Loss 1.315811 \n",
            "Step 22 of Epoch: 66; Loss 1.316324 \n",
            "Step 23 of Epoch: 66; Loss 1.317455 \n",
            "Step 24 of Epoch: 66; Loss 1.317641 \n",
            "Step 25 of Epoch: 66; Loss 1.318294 \n",
            "Step 26 of Epoch: 66; Loss 1.318643 \n",
            "Step 27 of Epoch: 66; Loss 1.318625 \n",
            "Step 28 of Epoch: 66; Loss 1.318302 \n",
            "Step 29 of Epoch: 66; Loss 1.319275 \n",
            "Step 30 of Epoch: 66; Loss 1.319550 \n",
            "Step 31 of Epoch: 66; Loss 1.320291 \n",
            "Step 32 of Epoch: 66; Loss 1.320464 \n",
            "Step 33 of Epoch: 66; Loss 1.320874 \n",
            "Step 34 of Epoch: 66; Loss 1.321588 \n",
            "Step 35 of Epoch: 66; Loss 1.321590 \n",
            "Step 36 of Epoch: 66; Loss 1.322204 \n",
            "Step 37 of Epoch: 66; Loss 1.323108 \n",
            "Step 38 of Epoch: 66; Loss 1.324067 \n",
            "Step 39 of Epoch: 66; Loss 1.324460 \n",
            "Step 40 of Epoch: 66; Loss 1.325221 \n",
            "Final Result for Epoch 66: Loss 1.325170; Val Acc 0.291250; Train Acc 0.462892\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 67; Loss 1.295550 \n",
            "Step 2 of Epoch: 67; Loss 1.294690 \n",
            "Step 3 of Epoch: 67; Loss 1.298803 \n",
            "Step 4 of Epoch: 67; Loss 1.299823 \n",
            "Step 5 of Epoch: 67; Loss 1.302824 \n",
            "Step 6 of Epoch: 67; Loss 1.306784 \n",
            "Step 7 of Epoch: 67; Loss 1.306206 \n",
            "Step 8 of Epoch: 67; Loss 1.306814 \n",
            "Step 9 of Epoch: 67; Loss 1.306587 \n",
            "Step 10 of Epoch: 67; Loss 1.308478 \n",
            "Step 11 of Epoch: 67; Loss 1.308407 \n",
            "Step 12 of Epoch: 67; Loss 1.309204 \n",
            "Step 13 of Epoch: 67; Loss 1.308478 \n",
            "Step 14 of Epoch: 67; Loss 1.308249 \n",
            "Step 15 of Epoch: 67; Loss 1.308312 \n",
            "Step 16 of Epoch: 67; Loss 1.309546 \n",
            "Step 17 of Epoch: 67; Loss 1.309292 \n",
            "Step 18 of Epoch: 67; Loss 1.309375 \n",
            "Step 19 of Epoch: 67; Loss 1.311455 \n",
            "Step 20 of Epoch: 67; Loss 1.311552 \n",
            "Step 21 of Epoch: 67; Loss 1.311476 \n",
            "Step 22 of Epoch: 67; Loss 1.312975 \n",
            "Step 23 of Epoch: 67; Loss 1.313225 \n",
            "Step 24 of Epoch: 67; Loss 1.314046 \n",
            "Step 25 of Epoch: 67; Loss 1.314655 \n",
            "Step 26 of Epoch: 67; Loss 1.315120 \n",
            "Step 27 of Epoch: 67; Loss 1.315511 \n",
            "Step 28 of Epoch: 67; Loss 1.316007 \n",
            "Step 29 of Epoch: 67; Loss 1.316444 \n",
            "Step 30 of Epoch: 67; Loss 1.317360 \n",
            "Step 31 of Epoch: 67; Loss 1.317837 \n",
            "Step 32 of Epoch: 67; Loss 1.318749 \n",
            "Step 33 of Epoch: 67; Loss 1.319527 \n",
            "Step 34 of Epoch: 67; Loss 1.319952 \n",
            "Step 35 of Epoch: 67; Loss 1.320464 \n",
            "Step 36 of Epoch: 67; Loss 1.321094 \n",
            "Step 37 of Epoch: 67; Loss 1.321688 \n",
            "Step 38 of Epoch: 67; Loss 1.322584 \n",
            "Step 39 of Epoch: 67; Loss 1.322686 \n",
            "Step 40 of Epoch: 67; Loss 1.322978 \n",
            "Final Result for Epoch 67: Loss 1.322999; Val Acc 0.291281; Train Acc 0.466786\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 68; Loss 1.299841 \n",
            "Step 2 of Epoch: 68; Loss 1.297721 \n",
            "Step 3 of Epoch: 68; Loss 1.297590 \n",
            "Step 4 of Epoch: 68; Loss 1.299957 \n",
            "Step 5 of Epoch: 68; Loss 1.299922 \n",
            "Step 6 of Epoch: 68; Loss 1.299620 \n",
            "Step 7 of Epoch: 68; Loss 1.301526 \n",
            "Step 8 of Epoch: 68; Loss 1.301337 \n",
            "Step 9 of Epoch: 68; Loss 1.301040 \n",
            "Step 10 of Epoch: 68; Loss 1.302903 \n",
            "Step 11 of Epoch: 68; Loss 1.303831 \n",
            "Step 12 of Epoch: 68; Loss 1.304105 \n",
            "Step 13 of Epoch: 68; Loss 1.306611 \n",
            "Step 14 of Epoch: 68; Loss 1.307642 \n",
            "Step 15 of Epoch: 68; Loss 1.308791 \n",
            "Step 16 of Epoch: 68; Loss 1.309533 \n",
            "Step 17 of Epoch: 68; Loss 1.310666 \n",
            "Step 18 of Epoch: 68; Loss 1.311457 \n",
            "Step 19 of Epoch: 68; Loss 1.311882 \n",
            "Step 20 of Epoch: 68; Loss 1.311829 \n",
            "Step 21 of Epoch: 68; Loss 1.311282 \n",
            "Step 22 of Epoch: 68; Loss 1.311738 \n",
            "Step 23 of Epoch: 68; Loss 1.313154 \n",
            "Step 24 of Epoch: 68; Loss 1.313831 \n",
            "Step 25 of Epoch: 68; Loss 1.314626 \n",
            "Step 26 of Epoch: 68; Loss 1.314860 \n",
            "Step 27 of Epoch: 68; Loss 1.315319 \n",
            "Step 28 of Epoch: 68; Loss 1.316236 \n",
            "Step 29 of Epoch: 68; Loss 1.316750 \n",
            "Step 30 of Epoch: 68; Loss 1.317187 \n",
            "Step 31 of Epoch: 68; Loss 1.317689 \n",
            "Step 32 of Epoch: 68; Loss 1.318122 \n",
            "Step 33 of Epoch: 68; Loss 1.319406 \n",
            "Step 34 of Epoch: 68; Loss 1.319995 \n",
            "Step 35 of Epoch: 68; Loss 1.320443 \n",
            "Step 36 of Epoch: 68; Loss 1.320612 \n",
            "Step 37 of Epoch: 68; Loss 1.320759 \n",
            "Step 38 of Epoch: 68; Loss 1.320662 \n",
            "Step 39 of Epoch: 68; Loss 1.321232 \n",
            "Step 40 of Epoch: 68; Loss 1.321466 \n",
            "Final Result for Epoch 68: Loss 1.321489; Val Acc 0.292563; Train Acc 0.465407\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 69; Loss 1.295032 \n",
            "Step 2 of Epoch: 69; Loss 1.303034 \n",
            "Step 3 of Epoch: 69; Loss 1.302453 \n",
            "Step 4 of Epoch: 69; Loss 1.299652 \n",
            "Step 5 of Epoch: 69; Loss 1.302221 \n",
            "Step 6 of Epoch: 69; Loss 1.301623 \n",
            "Step 7 of Epoch: 69; Loss 1.303023 \n",
            "Step 8 of Epoch: 69; Loss 1.303572 \n",
            "Step 9 of Epoch: 69; Loss 1.303479 \n",
            "Step 10 of Epoch: 69; Loss 1.305288 \n",
            "Step 11 of Epoch: 69; Loss 1.306695 \n",
            "Step 12 of Epoch: 69; Loss 1.306039 \n",
            "Step 13 of Epoch: 69; Loss 1.307309 \n",
            "Step 14 of Epoch: 69; Loss 1.307878 \n",
            "Step 15 of Epoch: 69; Loss 1.308276 \n",
            "Step 16 of Epoch: 69; Loss 1.309595 \n",
            "Step 17 of Epoch: 69; Loss 1.310180 \n",
            "Step 18 of Epoch: 69; Loss 1.310806 \n",
            "Step 19 of Epoch: 69; Loss 1.310981 \n",
            "Step 20 of Epoch: 69; Loss 1.311453 \n",
            "Step 21 of Epoch: 69; Loss 1.311887 \n",
            "Step 22 of Epoch: 69; Loss 1.312491 \n",
            "Step 23 of Epoch: 69; Loss 1.312568 \n",
            "Step 24 of Epoch: 69; Loss 1.312868 \n",
            "Step 25 of Epoch: 69; Loss 1.312538 \n",
            "Step 26 of Epoch: 69; Loss 1.313045 \n",
            "Step 27 of Epoch: 69; Loss 1.313310 \n",
            "Step 28 of Epoch: 69; Loss 1.313763 \n",
            "Step 29 of Epoch: 69; Loss 1.314462 \n",
            "Step 30 of Epoch: 69; Loss 1.315009 \n",
            "Step 31 of Epoch: 69; Loss 1.315747 \n",
            "Step 32 of Epoch: 69; Loss 1.316110 \n",
            "Step 33 of Epoch: 69; Loss 1.316940 \n",
            "Step 34 of Epoch: 69; Loss 1.317441 \n",
            "Step 35 of Epoch: 69; Loss 1.317807 \n",
            "Step 36 of Epoch: 69; Loss 1.318227 \n",
            "Step 37 of Epoch: 69; Loss 1.318656 \n",
            "Step 38 of Epoch: 69; Loss 1.318698 \n",
            "Step 39 of Epoch: 69; Loss 1.319047 \n",
            "Step 40 of Epoch: 69; Loss 1.319861 \n",
            "Final Result for Epoch 69: Loss 1.319828; Val Acc 0.290469; Train Acc 0.465860\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 70; Loss 1.279636 \n",
            "Step 2 of Epoch: 70; Loss 1.289075 \n",
            "Step 3 of Epoch: 70; Loss 1.295846 \n",
            "Step 4 of Epoch: 70; Loss 1.292624 \n",
            "Step 5 of Epoch: 70; Loss 1.295626 \n",
            "Step 6 of Epoch: 70; Loss 1.294971 \n",
            "Step 7 of Epoch: 70; Loss 1.296157 \n",
            "Step 8 of Epoch: 70; Loss 1.297483 \n",
            "Step 9 of Epoch: 70; Loss 1.297224 \n",
            "Step 10 of Epoch: 70; Loss 1.296590 \n",
            "Step 11 of Epoch: 70; Loss 1.298217 \n",
            "Step 12 of Epoch: 70; Loss 1.299580 \n",
            "Step 13 of Epoch: 70; Loss 1.300998 \n",
            "Step 14 of Epoch: 70; Loss 1.301921 \n",
            "Step 15 of Epoch: 70; Loss 1.302855 \n",
            "Step 16 of Epoch: 70; Loss 1.303951 \n",
            "Step 17 of Epoch: 70; Loss 1.305217 \n",
            "Step 18 of Epoch: 70; Loss 1.306392 \n",
            "Step 19 of Epoch: 70; Loss 1.307116 \n",
            "Step 20 of Epoch: 70; Loss 1.307332 \n",
            "Step 21 of Epoch: 70; Loss 1.308440 \n",
            "Step 22 of Epoch: 70; Loss 1.309989 \n",
            "Step 23 of Epoch: 70; Loss 1.310885 \n",
            "Step 24 of Epoch: 70; Loss 1.310889 \n",
            "Step 25 of Epoch: 70; Loss 1.311271 \n",
            "Step 26 of Epoch: 70; Loss 1.311782 \n",
            "Step 27 of Epoch: 70; Loss 1.312636 \n",
            "Step 28 of Epoch: 70; Loss 1.313139 \n",
            "Step 29 of Epoch: 70; Loss 1.313866 \n",
            "Step 30 of Epoch: 70; Loss 1.314302 \n",
            "Step 31 of Epoch: 70; Loss 1.314672 \n",
            "Step 32 of Epoch: 70; Loss 1.314793 \n",
            "Step 33 of Epoch: 70; Loss 1.315565 \n",
            "Step 34 of Epoch: 70; Loss 1.316211 \n",
            "Step 35 of Epoch: 70; Loss 1.316416 \n",
            "Step 36 of Epoch: 70; Loss 1.317049 \n",
            "Step 37 of Epoch: 70; Loss 1.317725 \n",
            "Step 38 of Epoch: 70; Loss 1.317747 \n",
            "Step 39 of Epoch: 70; Loss 1.318329 \n",
            "Step 40 of Epoch: 70; Loss 1.318574 \n",
            "Final Result for Epoch 70: Loss 1.318584; Val Acc 0.288281; Train Acc 0.465415\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 71; Loss 1.283459 \n",
            "Step 2 of Epoch: 71; Loss 1.286880 \n",
            "Step 3 of Epoch: 71; Loss 1.286967 \n",
            "Step 4 of Epoch: 71; Loss 1.289975 \n",
            "Step 5 of Epoch: 71; Loss 1.292224 \n",
            "Step 6 of Epoch: 71; Loss 1.295805 \n",
            "Step 7 of Epoch: 71; Loss 1.296663 \n",
            "Step 8 of Epoch: 71; Loss 1.300233 \n",
            "Step 9 of Epoch: 71; Loss 1.298853 \n",
            "Step 10 of Epoch: 71; Loss 1.301557 \n",
            "Step 11 of Epoch: 71; Loss 1.302720 \n",
            "Step 12 of Epoch: 71; Loss 1.302955 \n",
            "Step 13 of Epoch: 71; Loss 1.303984 \n",
            "Step 14 of Epoch: 71; Loss 1.304148 \n",
            "Step 15 of Epoch: 71; Loss 1.304141 \n",
            "Step 16 of Epoch: 71; Loss 1.304037 \n",
            "Step 17 of Epoch: 71; Loss 1.304463 \n",
            "Step 18 of Epoch: 71; Loss 1.304978 \n",
            "Step 19 of Epoch: 71; Loss 1.305868 \n",
            "Step 20 of Epoch: 71; Loss 1.306810 \n",
            "Step 21 of Epoch: 71; Loss 1.307336 \n",
            "Step 22 of Epoch: 71; Loss 1.308035 \n",
            "Step 23 of Epoch: 71; Loss 1.308291 \n",
            "Step 24 of Epoch: 71; Loss 1.308732 \n",
            "Step 25 of Epoch: 71; Loss 1.309547 \n",
            "Step 26 of Epoch: 71; Loss 1.310273 \n",
            "Step 27 of Epoch: 71; Loss 1.311208 \n",
            "Step 28 of Epoch: 71; Loss 1.311288 \n",
            "Step 29 of Epoch: 71; Loss 1.311427 \n",
            "Step 30 of Epoch: 71; Loss 1.312036 \n",
            "Step 31 of Epoch: 71; Loss 1.312677 \n",
            "Step 32 of Epoch: 71; Loss 1.313251 \n",
            "Step 33 of Epoch: 71; Loss 1.313806 \n",
            "Step 34 of Epoch: 71; Loss 1.314153 \n",
            "Step 35 of Epoch: 71; Loss 1.314219 \n",
            "Step 36 of Epoch: 71; Loss 1.314835 \n",
            "Step 37 of Epoch: 71; Loss 1.315335 \n",
            "Step 38 of Epoch: 71; Loss 1.315697 \n",
            "Step 39 of Epoch: 71; Loss 1.316043 \n",
            "Step 40 of Epoch: 71; Loss 1.316116 \n",
            "Final Result for Epoch 71: Loss 1.316086; Val Acc 0.291656; Train Acc 0.468235\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 72; Loss 1.289262 \n",
            "Step 2 of Epoch: 72; Loss 1.299172 \n",
            "Step 3 of Epoch: 72; Loss 1.297877 \n",
            "Step 4 of Epoch: 72; Loss 1.301108 \n",
            "Step 5 of Epoch: 72; Loss 1.303844 \n",
            "Step 6 of Epoch: 72; Loss 1.302338 \n",
            "Step 7 of Epoch: 72; Loss 1.301524 \n",
            "Step 8 of Epoch: 72; Loss 1.302107 \n",
            "Step 9 of Epoch: 72; Loss 1.302474 \n",
            "Step 10 of Epoch: 72; Loss 1.301268 \n",
            "Step 11 of Epoch: 72; Loss 1.302495 \n",
            "Step 12 of Epoch: 72; Loss 1.303319 \n",
            "Step 13 of Epoch: 72; Loss 1.303718 \n",
            "Step 14 of Epoch: 72; Loss 1.302984 \n",
            "Step 15 of Epoch: 72; Loss 1.303972 \n",
            "Step 16 of Epoch: 72; Loss 1.304432 \n",
            "Step 17 of Epoch: 72; Loss 1.303767 \n",
            "Step 18 of Epoch: 72; Loss 1.303382 \n",
            "Step 19 of Epoch: 72; Loss 1.303896 \n",
            "Step 20 of Epoch: 72; Loss 1.303846 \n",
            "Step 21 of Epoch: 72; Loss 1.304838 \n",
            "Step 22 of Epoch: 72; Loss 1.305589 \n",
            "Step 23 of Epoch: 72; Loss 1.305714 \n",
            "Step 24 of Epoch: 72; Loss 1.306423 \n",
            "Step 25 of Epoch: 72; Loss 1.306556 \n",
            "Step 26 of Epoch: 72; Loss 1.307143 \n",
            "Step 27 of Epoch: 72; Loss 1.307979 \n",
            "Step 28 of Epoch: 72; Loss 1.309050 \n",
            "Step 29 of Epoch: 72; Loss 1.309548 \n",
            "Step 30 of Epoch: 72; Loss 1.310599 \n",
            "Step 31 of Epoch: 72; Loss 1.312061 \n",
            "Step 32 of Epoch: 72; Loss 1.312139 \n",
            "Step 33 of Epoch: 72; Loss 1.312502 \n",
            "Step 34 of Epoch: 72; Loss 1.313191 \n",
            "Step 35 of Epoch: 72; Loss 1.313612 \n",
            "Step 36 of Epoch: 72; Loss 1.313716 \n",
            "Step 37 of Epoch: 72; Loss 1.314451 \n",
            "Step 38 of Epoch: 72; Loss 1.314513 \n",
            "Step 39 of Epoch: 72; Loss 1.314932 \n",
            "Step 40 of Epoch: 72; Loss 1.315014 \n",
            "Final Result for Epoch 72: Loss 1.314990; Val Acc 0.287531; Train Acc 0.468930\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 73; Loss 1.307087 \n",
            "Step 2 of Epoch: 73; Loss 1.294737 \n",
            "Step 3 of Epoch: 73; Loss 1.293450 \n",
            "Step 4 of Epoch: 73; Loss 1.292105 \n",
            "Step 5 of Epoch: 73; Loss 1.293345 \n",
            "Step 6 of Epoch: 73; Loss 1.292788 \n",
            "Step 7 of Epoch: 73; Loss 1.293524 \n",
            "Step 8 of Epoch: 73; Loss 1.296261 \n",
            "Step 9 of Epoch: 73; Loss 1.296961 \n",
            "Step 10 of Epoch: 73; Loss 1.297250 \n",
            "Step 11 of Epoch: 73; Loss 1.297295 \n",
            "Step 12 of Epoch: 73; Loss 1.297407 \n",
            "Step 13 of Epoch: 73; Loss 1.298543 \n",
            "Step 14 of Epoch: 73; Loss 1.298565 \n",
            "Step 15 of Epoch: 73; Loss 1.299777 \n",
            "Step 16 of Epoch: 73; Loss 1.299961 \n",
            "Step 17 of Epoch: 73; Loss 1.301336 \n",
            "Step 18 of Epoch: 73; Loss 1.301608 \n",
            "Step 19 of Epoch: 73; Loss 1.302411 \n",
            "Step 20 of Epoch: 73; Loss 1.303039 \n",
            "Step 21 of Epoch: 73; Loss 1.303292 \n",
            "Step 22 of Epoch: 73; Loss 1.303919 \n",
            "Step 23 of Epoch: 73; Loss 1.304335 \n",
            "Step 24 of Epoch: 73; Loss 1.305717 \n",
            "Step 25 of Epoch: 73; Loss 1.306407 \n",
            "Step 26 of Epoch: 73; Loss 1.307123 \n",
            "Step 27 of Epoch: 73; Loss 1.307564 \n",
            "Step 28 of Epoch: 73; Loss 1.307543 \n",
            "Step 29 of Epoch: 73; Loss 1.307822 \n",
            "Step 30 of Epoch: 73; Loss 1.307686 \n",
            "Step 31 of Epoch: 73; Loss 1.308225 \n",
            "Step 32 of Epoch: 73; Loss 1.309121 \n",
            "Step 33 of Epoch: 73; Loss 1.309843 \n",
            "Step 34 of Epoch: 73; Loss 1.310720 \n",
            "Step 35 of Epoch: 73; Loss 1.311639 \n",
            "Step 36 of Epoch: 73; Loss 1.312221 \n",
            "Step 37 of Epoch: 73; Loss 1.312767 \n",
            "Step 38 of Epoch: 73; Loss 1.312411 \n",
            "Step 39 of Epoch: 73; Loss 1.312810 \n",
            "Step 40 of Epoch: 73; Loss 1.313351 \n",
            "Final Result for Epoch 73: Loss 1.313407; Val Acc 0.289406; Train Acc 0.467938\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 74; Loss 1.274144 \n",
            "Step 2 of Epoch: 74; Loss 1.282749 \n",
            "Step 3 of Epoch: 74; Loss 1.287159 \n",
            "Step 4 of Epoch: 74; Loss 1.292307 \n",
            "Step 5 of Epoch: 74; Loss 1.289560 \n",
            "Step 6 of Epoch: 74; Loss 1.290241 \n",
            "Step 7 of Epoch: 74; Loss 1.290632 \n",
            "Step 8 of Epoch: 74; Loss 1.292327 \n",
            "Step 9 of Epoch: 74; Loss 1.291834 \n",
            "Step 10 of Epoch: 74; Loss 1.292645 \n",
            "Step 11 of Epoch: 74; Loss 1.293714 \n",
            "Step 12 of Epoch: 74; Loss 1.293259 \n",
            "Step 13 of Epoch: 74; Loss 1.293569 \n",
            "Step 14 of Epoch: 74; Loss 1.294037 \n",
            "Step 15 of Epoch: 74; Loss 1.295305 \n",
            "Step 16 of Epoch: 74; Loss 1.296429 \n",
            "Step 17 of Epoch: 74; Loss 1.297572 \n",
            "Step 18 of Epoch: 74; Loss 1.298857 \n",
            "Step 19 of Epoch: 74; Loss 1.299479 \n",
            "Step 20 of Epoch: 74; Loss 1.300300 \n",
            "Step 21 of Epoch: 74; Loss 1.299784 \n",
            "Step 22 of Epoch: 74; Loss 1.300577 \n",
            "Step 23 of Epoch: 74; Loss 1.300936 \n",
            "Step 24 of Epoch: 74; Loss 1.302568 \n",
            "Step 25 of Epoch: 74; Loss 1.303521 \n",
            "Step 26 of Epoch: 74; Loss 1.303894 \n",
            "Step 27 of Epoch: 74; Loss 1.304698 \n",
            "Step 28 of Epoch: 74; Loss 1.306271 \n",
            "Step 29 of Epoch: 74; Loss 1.307459 \n",
            "Step 30 of Epoch: 74; Loss 1.308178 \n",
            "Step 31 of Epoch: 74; Loss 1.308830 \n",
            "Step 32 of Epoch: 74; Loss 1.309073 \n",
            "Step 33 of Epoch: 74; Loss 1.309379 \n",
            "Step 34 of Epoch: 74; Loss 1.310000 \n",
            "Step 35 of Epoch: 74; Loss 1.310280 \n",
            "Step 36 of Epoch: 74; Loss 1.311138 \n",
            "Step 37 of Epoch: 74; Loss 1.312129 \n",
            "Step 38 of Epoch: 74; Loss 1.312055 \n",
            "Step 39 of Epoch: 74; Loss 1.312441 \n",
            "Step 40 of Epoch: 74; Loss 1.312843 \n",
            "Final Result for Epoch 74: Loss 1.312794; Val Acc 0.287344; Train Acc 0.470738\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 75; Loss 1.297792 \n",
            "Step 2 of Epoch: 75; Loss 1.296742 \n",
            "Step 3 of Epoch: 75; Loss 1.301841 \n",
            "Step 4 of Epoch: 75; Loss 1.300837 \n",
            "Step 5 of Epoch: 75; Loss 1.298940 \n",
            "Step 6 of Epoch: 75; Loss 1.297971 \n",
            "Step 7 of Epoch: 75; Loss 1.296444 \n",
            "Step 8 of Epoch: 75; Loss 1.296167 \n",
            "Step 9 of Epoch: 75; Loss 1.297936 \n",
            "Step 10 of Epoch: 75; Loss 1.297702 \n",
            "Step 11 of Epoch: 75; Loss 1.297705 \n",
            "Step 12 of Epoch: 75; Loss 1.298608 \n",
            "Step 13 of Epoch: 75; Loss 1.299728 \n",
            "Step 14 of Epoch: 75; Loss 1.299186 \n",
            "Step 15 of Epoch: 75; Loss 1.300512 \n",
            "Step 16 of Epoch: 75; Loss 1.302052 \n",
            "Step 17 of Epoch: 75; Loss 1.301841 \n",
            "Step 18 of Epoch: 75; Loss 1.302505 \n",
            "Step 19 of Epoch: 75; Loss 1.302730 \n",
            "Step 20 of Epoch: 75; Loss 1.303474 \n",
            "Step 21 of Epoch: 75; Loss 1.304300 \n",
            "Step 22 of Epoch: 75; Loss 1.304608 \n",
            "Step 23 of Epoch: 75; Loss 1.304739 \n",
            "Step 24 of Epoch: 75; Loss 1.305218 \n",
            "Step 25 of Epoch: 75; Loss 1.305714 \n",
            "Step 26 of Epoch: 75; Loss 1.306259 \n",
            "Step 27 of Epoch: 75; Loss 1.306485 \n",
            "Step 28 of Epoch: 75; Loss 1.306540 \n",
            "Step 29 of Epoch: 75; Loss 1.307279 \n",
            "Step 30 of Epoch: 75; Loss 1.307733 \n",
            "Step 31 of Epoch: 75; Loss 1.308033 \n",
            "Step 32 of Epoch: 75; Loss 1.308780 \n",
            "Step 33 of Epoch: 75; Loss 1.308861 \n",
            "Step 34 of Epoch: 75; Loss 1.309555 \n",
            "Step 35 of Epoch: 75; Loss 1.309505 \n",
            "Step 36 of Epoch: 75; Loss 1.309140 \n",
            "Step 37 of Epoch: 75; Loss 1.309490 \n",
            "Step 38 of Epoch: 75; Loss 1.309938 \n",
            "Step 39 of Epoch: 75; Loss 1.310238 \n",
            "Step 40 of Epoch: 75; Loss 1.310287 \n",
            "Final Result for Epoch 75: Loss 1.310329; Val Acc 0.288531; Train Acc 0.468945\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 76; Loss 1.288737 \n",
            "Step 2 of Epoch: 76; Loss 1.287272 \n",
            "Step 3 of Epoch: 76; Loss 1.289094 \n",
            "Step 4 of Epoch: 76; Loss 1.287893 \n",
            "Step 5 of Epoch: 76; Loss 1.288968 \n",
            "Step 6 of Epoch: 76; Loss 1.291636 \n",
            "Step 7 of Epoch: 76; Loss 1.291663 \n",
            "Step 8 of Epoch: 76; Loss 1.292193 \n",
            "Step 9 of Epoch: 76; Loss 1.292677 \n",
            "Step 10 of Epoch: 76; Loss 1.293300 \n",
            "Step 11 of Epoch: 76; Loss 1.292929 \n",
            "Step 12 of Epoch: 76; Loss 1.292502 \n",
            "Step 13 of Epoch: 76; Loss 1.291917 \n",
            "Step 14 of Epoch: 76; Loss 1.293223 \n",
            "Step 15 of Epoch: 76; Loss 1.293759 \n",
            "Step 16 of Epoch: 76; Loss 1.294595 \n",
            "Step 17 of Epoch: 76; Loss 1.295257 \n",
            "Step 18 of Epoch: 76; Loss 1.296661 \n",
            "Step 19 of Epoch: 76; Loss 1.297405 \n",
            "Step 20 of Epoch: 76; Loss 1.298154 \n",
            "Step 21 of Epoch: 76; Loss 1.299017 \n",
            "Step 22 of Epoch: 76; Loss 1.299947 \n",
            "Step 23 of Epoch: 76; Loss 1.301621 \n",
            "Step 24 of Epoch: 76; Loss 1.301659 \n",
            "Step 25 of Epoch: 76; Loss 1.302498 \n",
            "Step 26 of Epoch: 76; Loss 1.302614 \n",
            "Step 27 of Epoch: 76; Loss 1.303019 \n",
            "Step 28 of Epoch: 76; Loss 1.303439 \n",
            "Step 29 of Epoch: 76; Loss 1.303606 \n",
            "Step 30 of Epoch: 76; Loss 1.304517 \n",
            "Step 31 of Epoch: 76; Loss 1.304749 \n",
            "Step 32 of Epoch: 76; Loss 1.305537 \n",
            "Step 33 of Epoch: 76; Loss 1.305911 \n",
            "Step 34 of Epoch: 76; Loss 1.306349 \n",
            "Step 35 of Epoch: 76; Loss 1.306676 \n",
            "Step 36 of Epoch: 76; Loss 1.307280 \n",
            "Step 37 of Epoch: 76; Loss 1.307627 \n",
            "Step 38 of Epoch: 76; Loss 1.308019 \n",
            "Step 39 of Epoch: 76; Loss 1.308345 \n",
            "Step 40 of Epoch: 76; Loss 1.308628 \n",
            "Final Result for Epoch 76: Loss 1.308644; Val Acc 0.287625; Train Acc 0.470515\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 77; Loss 1.290233 \n",
            "Step 2 of Epoch: 77; Loss 1.290761 \n",
            "Step 3 of Epoch: 77; Loss 1.293678 \n",
            "Step 4 of Epoch: 77; Loss 1.293386 \n",
            "Step 5 of Epoch: 77; Loss 1.289029 \n",
            "Step 6 of Epoch: 77; Loss 1.288910 \n",
            "Step 7 of Epoch: 77; Loss 1.289688 \n",
            "Step 8 of Epoch: 77; Loss 1.288726 \n",
            "Step 9 of Epoch: 77; Loss 1.289530 \n",
            "Step 10 of Epoch: 77; Loss 1.288751 \n",
            "Step 11 of Epoch: 77; Loss 1.289748 \n",
            "Step 12 of Epoch: 77; Loss 1.289049 \n",
            "Step 13 of Epoch: 77; Loss 1.290671 \n",
            "Step 14 of Epoch: 77; Loss 1.292327 \n",
            "Step 15 of Epoch: 77; Loss 1.293129 \n",
            "Step 16 of Epoch: 77; Loss 1.294806 \n",
            "Step 17 of Epoch: 77; Loss 1.294309 \n",
            "Step 18 of Epoch: 77; Loss 1.295659 \n",
            "Step 19 of Epoch: 77; Loss 1.296336 \n",
            "Step 20 of Epoch: 77; Loss 1.297469 \n",
            "Step 21 of Epoch: 77; Loss 1.297963 \n",
            "Step 22 of Epoch: 77; Loss 1.298217 \n",
            "Step 23 of Epoch: 77; Loss 1.299104 \n",
            "Step 24 of Epoch: 77; Loss 1.299609 \n",
            "Step 25 of Epoch: 77; Loss 1.299759 \n",
            "Step 26 of Epoch: 77; Loss 1.300690 \n",
            "Step 27 of Epoch: 77; Loss 1.301157 \n",
            "Step 28 of Epoch: 77; Loss 1.302167 \n",
            "Step 29 of Epoch: 77; Loss 1.303003 \n",
            "Step 30 of Epoch: 77; Loss 1.304043 \n",
            "Step 31 of Epoch: 77; Loss 1.304881 \n",
            "Step 32 of Epoch: 77; Loss 1.305207 \n",
            "Step 33 of Epoch: 77; Loss 1.305529 \n",
            "Step 34 of Epoch: 77; Loss 1.305823 \n",
            "Step 35 of Epoch: 77; Loss 1.306530 \n",
            "Step 36 of Epoch: 77; Loss 1.306355 \n",
            "Step 37 of Epoch: 77; Loss 1.307178 \n",
            "Step 38 of Epoch: 77; Loss 1.307586 \n",
            "Step 39 of Epoch: 77; Loss 1.307994 \n",
            "Step 40 of Epoch: 77; Loss 1.308028 \n",
            "Final Result for Epoch 77: Loss 1.308031; Val Acc 0.289219; Train Acc 0.469914\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 78; Loss 1.285035 \n",
            "Step 2 of Epoch: 78; Loss 1.287233 \n",
            "Step 3 of Epoch: 78; Loss 1.289439 \n",
            "Step 4 of Epoch: 78; Loss 1.284973 \n",
            "Step 5 of Epoch: 78; Loss 1.285163 \n",
            "Step 6 of Epoch: 78; Loss 1.288933 \n",
            "Step 7 of Epoch: 78; Loss 1.287503 \n",
            "Step 8 of Epoch: 78; Loss 1.287986 \n",
            "Step 9 of Epoch: 78; Loss 1.288305 \n",
            "Step 10 of Epoch: 78; Loss 1.287510 \n",
            "Step 11 of Epoch: 78; Loss 1.289139 \n",
            "Step 12 of Epoch: 78; Loss 1.291352 \n",
            "Step 13 of Epoch: 78; Loss 1.292488 \n",
            "Step 14 of Epoch: 78; Loss 1.294295 \n",
            "Step 15 of Epoch: 78; Loss 1.295167 \n",
            "Step 16 of Epoch: 78; Loss 1.296000 \n",
            "Step 17 of Epoch: 78; Loss 1.297251 \n",
            "Step 18 of Epoch: 78; Loss 1.298349 \n",
            "Step 19 of Epoch: 78; Loss 1.298912 \n",
            "Step 20 of Epoch: 78; Loss 1.299787 \n",
            "Step 21 of Epoch: 78; Loss 1.300936 \n",
            "Step 22 of Epoch: 78; Loss 1.301826 \n",
            "Step 23 of Epoch: 78; Loss 1.301933 \n",
            "Step 24 of Epoch: 78; Loss 1.301733 \n",
            "Step 25 of Epoch: 78; Loss 1.301997 \n",
            "Step 26 of Epoch: 78; Loss 1.302989 \n",
            "Step 27 of Epoch: 78; Loss 1.302989 \n",
            "Step 28 of Epoch: 78; Loss 1.303772 \n",
            "Step 29 of Epoch: 78; Loss 1.304744 \n",
            "Step 30 of Epoch: 78; Loss 1.305573 \n",
            "Step 31 of Epoch: 78; Loss 1.306097 \n",
            "Step 32 of Epoch: 78; Loss 1.306847 \n",
            "Step 33 of Epoch: 78; Loss 1.306693 \n",
            "Step 34 of Epoch: 78; Loss 1.306327 \n",
            "Step 35 of Epoch: 78; Loss 1.306937 \n",
            "Step 36 of Epoch: 78; Loss 1.307029 \n",
            "Step 37 of Epoch: 78; Loss 1.307498 \n",
            "Step 38 of Epoch: 78; Loss 1.308168 \n",
            "Step 39 of Epoch: 78; Loss 1.308516 \n",
            "Step 40 of Epoch: 78; Loss 1.308931 \n",
            "Final Result for Epoch 78: Loss 1.308914; Val Acc 0.288500; Train Acc 0.471609\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 79; Loss 1.273741 \n",
            "Step 2 of Epoch: 79; Loss 1.277762 \n",
            "Step 3 of Epoch: 79; Loss 1.287285 \n",
            "Step 4 of Epoch: 79; Loss 1.287214 \n",
            "Step 5 of Epoch: 79; Loss 1.287087 \n",
            "Step 6 of Epoch: 79; Loss 1.287534 \n",
            "Step 7 of Epoch: 79; Loss 1.286687 \n",
            "Step 8 of Epoch: 79; Loss 1.288650 \n",
            "Step 9 of Epoch: 79; Loss 1.289368 \n",
            "Step 10 of Epoch: 79; Loss 1.289949 \n",
            "Step 11 of Epoch: 79; Loss 1.291255 \n",
            "Step 12 of Epoch: 79; Loss 1.292498 \n",
            "Step 13 of Epoch: 79; Loss 1.292061 \n",
            "Step 14 of Epoch: 79; Loss 1.293560 \n",
            "Step 15 of Epoch: 79; Loss 1.293935 \n",
            "Step 16 of Epoch: 79; Loss 1.294404 \n",
            "Step 17 of Epoch: 79; Loss 1.295289 \n",
            "Step 18 of Epoch: 79; Loss 1.296790 \n",
            "Step 19 of Epoch: 79; Loss 1.296469 \n",
            "Step 20 of Epoch: 79; Loss 1.296568 \n",
            "Step 21 of Epoch: 79; Loss 1.296731 \n",
            "Step 22 of Epoch: 79; Loss 1.296700 \n",
            "Step 23 of Epoch: 79; Loss 1.297156 \n",
            "Step 24 of Epoch: 79; Loss 1.297389 \n",
            "Step 25 of Epoch: 79; Loss 1.298328 \n",
            "Step 26 of Epoch: 79; Loss 1.299498 \n",
            "Step 27 of Epoch: 79; Loss 1.300240 \n",
            "Step 28 of Epoch: 79; Loss 1.300757 \n",
            "Step 29 of Epoch: 79; Loss 1.301124 \n",
            "Step 30 of Epoch: 79; Loss 1.301943 \n",
            "Step 31 of Epoch: 79; Loss 1.302314 \n",
            "Step 32 of Epoch: 79; Loss 1.303037 \n",
            "Step 33 of Epoch: 79; Loss 1.303214 \n",
            "Step 34 of Epoch: 79; Loss 1.303648 \n",
            "Step 35 of Epoch: 79; Loss 1.303792 \n",
            "Step 36 of Epoch: 79; Loss 1.304188 \n",
            "Step 37 of Epoch: 79; Loss 1.304481 \n",
            "Step 38 of Epoch: 79; Loss 1.304691 \n",
            "Step 39 of Epoch: 79; Loss 1.305238 \n",
            "Step 40 of Epoch: 79; Loss 1.305999 \n",
            "Final Result for Epoch 79: Loss 1.306036; Val Acc 0.287156; Train Acc 0.471476\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 80; Loss 1.282564 \n",
            "Step 2 of Epoch: 80; Loss 1.283699 \n",
            "Step 3 of Epoch: 80; Loss 1.281887 \n",
            "Step 4 of Epoch: 80; Loss 1.281391 \n",
            "Step 5 of Epoch: 80; Loss 1.280253 \n",
            "Step 6 of Epoch: 80; Loss 1.276785 \n",
            "Step 7 of Epoch: 80; Loss 1.280546 \n",
            "Step 8 of Epoch: 80; Loss 1.281353 \n",
            "Step 9 of Epoch: 80; Loss 1.281516 \n",
            "Step 10 of Epoch: 80; Loss 1.283168 \n",
            "Step 11 of Epoch: 80; Loss 1.284513 \n",
            "Step 12 of Epoch: 80; Loss 1.285152 \n",
            "Step 13 of Epoch: 80; Loss 1.285931 \n",
            "Step 14 of Epoch: 80; Loss 1.287203 \n",
            "Step 15 of Epoch: 80; Loss 1.288529 \n",
            "Step 16 of Epoch: 80; Loss 1.290056 \n",
            "Step 17 of Epoch: 80; Loss 1.290876 \n",
            "Step 18 of Epoch: 80; Loss 1.292454 \n",
            "Step 19 of Epoch: 80; Loss 1.292773 \n",
            "Step 20 of Epoch: 80; Loss 1.294153 \n",
            "Step 21 of Epoch: 80; Loss 1.294594 \n",
            "Step 22 of Epoch: 80; Loss 1.295073 \n",
            "Step 23 of Epoch: 80; Loss 1.296674 \n",
            "Step 24 of Epoch: 80; Loss 1.296720 \n",
            "Step 25 of Epoch: 80; Loss 1.296886 \n",
            "Step 26 of Epoch: 80; Loss 1.297957 \n",
            "Step 27 of Epoch: 80; Loss 1.298563 \n",
            "Step 28 of Epoch: 80; Loss 1.299186 \n",
            "Step 29 of Epoch: 80; Loss 1.299569 \n",
            "Step 30 of Epoch: 80; Loss 1.299867 \n",
            "Step 31 of Epoch: 80; Loss 1.300398 \n",
            "Step 32 of Epoch: 80; Loss 1.300944 \n",
            "Step 33 of Epoch: 80; Loss 1.301654 \n",
            "Step 34 of Epoch: 80; Loss 1.302120 \n",
            "Step 35 of Epoch: 80; Loss 1.302266 \n",
            "Step 36 of Epoch: 80; Loss 1.303009 \n",
            "Step 37 of Epoch: 80; Loss 1.303774 \n",
            "Step 38 of Epoch: 80; Loss 1.304871 \n",
            "Step 39 of Epoch: 80; Loss 1.305294 \n",
            "Step 40 of Epoch: 80; Loss 1.305810 \n",
            "Final Result for Epoch 80: Loss 1.305835; Val Acc 0.288562; Train Acc 0.471097\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 81; Loss 1.268271 \n",
            "Step 2 of Epoch: 81; Loss 1.267358 \n",
            "Step 3 of Epoch: 81; Loss 1.273842 \n",
            "Step 4 of Epoch: 81; Loss 1.279476 \n",
            "Step 5 of Epoch: 81; Loss 1.280192 \n",
            "Step 6 of Epoch: 81; Loss 1.283116 \n",
            "Step 7 of Epoch: 81; Loss 1.280707 \n",
            "Step 8 of Epoch: 81; Loss 1.283105 \n",
            "Step 9 of Epoch: 81; Loss 1.284893 \n",
            "Step 10 of Epoch: 81; Loss 1.285704 \n",
            "Step 11 of Epoch: 81; Loss 1.286840 \n",
            "Step 12 of Epoch: 81; Loss 1.287854 \n",
            "Step 13 of Epoch: 81; Loss 1.289864 \n",
            "Step 14 of Epoch: 81; Loss 1.290869 \n",
            "Step 15 of Epoch: 81; Loss 1.291382 \n",
            "Step 16 of Epoch: 81; Loss 1.291977 \n",
            "Step 17 of Epoch: 81; Loss 1.292776 \n",
            "Step 18 of Epoch: 81; Loss 1.294397 \n",
            "Step 19 of Epoch: 81; Loss 1.294295 \n",
            "Step 20 of Epoch: 81; Loss 1.295711 \n",
            "Step 21 of Epoch: 81; Loss 1.296377 \n",
            "Step 22 of Epoch: 81; Loss 1.297017 \n",
            "Step 23 of Epoch: 81; Loss 1.297903 \n",
            "Step 24 of Epoch: 81; Loss 1.298780 \n",
            "Step 25 of Epoch: 81; Loss 1.299426 \n",
            "Step 26 of Epoch: 81; Loss 1.300161 \n",
            "Step 27 of Epoch: 81; Loss 1.300824 \n",
            "Step 28 of Epoch: 81; Loss 1.300734 \n",
            "Step 29 of Epoch: 81; Loss 1.300898 \n",
            "Step 30 of Epoch: 81; Loss 1.301052 \n",
            "Step 31 of Epoch: 81; Loss 1.301437 \n",
            "Step 32 of Epoch: 81; Loss 1.301811 \n",
            "Step 33 of Epoch: 81; Loss 1.302245 \n",
            "Step 34 of Epoch: 81; Loss 1.302632 \n",
            "Step 35 of Epoch: 81; Loss 1.303077 \n",
            "Step 36 of Epoch: 81; Loss 1.303425 \n",
            "Step 37 of Epoch: 81; Loss 1.303940 \n",
            "Step 38 of Epoch: 81; Loss 1.304211 \n",
            "Step 39 of Epoch: 81; Loss 1.304317 \n",
            "Step 40 of Epoch: 81; Loss 1.304716 \n",
            "Final Result for Epoch 81: Loss 1.304745; Val Acc 0.289219; Train Acc 0.470703\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 82; Loss 1.270655 \n",
            "Step 2 of Epoch: 82; Loss 1.275985 \n",
            "Step 3 of Epoch: 82; Loss 1.286169 \n",
            "Step 4 of Epoch: 82; Loss 1.286024 \n",
            "Step 5 of Epoch: 82; Loss 1.284371 \n",
            "Step 6 of Epoch: 82; Loss 1.286032 \n",
            "Step 7 of Epoch: 82; Loss 1.284983 \n",
            "Step 8 of Epoch: 82; Loss 1.288079 \n",
            "Step 9 of Epoch: 82; Loss 1.287792 \n",
            "Step 10 of Epoch: 82; Loss 1.288644 \n",
            "Step 11 of Epoch: 82; Loss 1.290099 \n",
            "Step 12 of Epoch: 82; Loss 1.289741 \n",
            "Step 13 of Epoch: 82; Loss 1.291428 \n",
            "Step 14 of Epoch: 82; Loss 1.290988 \n",
            "Step 15 of Epoch: 82; Loss 1.290187 \n",
            "Step 16 of Epoch: 82; Loss 1.290213 \n",
            "Step 17 of Epoch: 82; Loss 1.291357 \n",
            "Step 18 of Epoch: 82; Loss 1.290992 \n",
            "Step 19 of Epoch: 82; Loss 1.291968 \n",
            "Step 20 of Epoch: 82; Loss 1.292486 \n",
            "Step 21 of Epoch: 82; Loss 1.293332 \n",
            "Step 22 of Epoch: 82; Loss 1.294460 \n",
            "Step 23 of Epoch: 82; Loss 1.294352 \n",
            "Step 24 of Epoch: 82; Loss 1.295016 \n",
            "Step 25 of Epoch: 82; Loss 1.295310 \n",
            "Step 26 of Epoch: 82; Loss 1.295670 \n",
            "Step 27 of Epoch: 82; Loss 1.296363 \n",
            "Step 28 of Epoch: 82; Loss 1.296852 \n",
            "Step 29 of Epoch: 82; Loss 1.298007 \n",
            "Step 30 of Epoch: 82; Loss 1.298105 \n",
            "Step 31 of Epoch: 82; Loss 1.297547 \n",
            "Step 32 of Epoch: 82; Loss 1.298342 \n",
            "Step 33 of Epoch: 82; Loss 1.298835 \n",
            "Step 34 of Epoch: 82; Loss 1.299305 \n",
            "Step 35 of Epoch: 82; Loss 1.299464 \n",
            "Step 36 of Epoch: 82; Loss 1.299602 \n",
            "Step 37 of Epoch: 82; Loss 1.299965 \n",
            "Step 38 of Epoch: 82; Loss 1.300585 \n",
            "Step 39 of Epoch: 82; Loss 1.301282 \n",
            "Step 40 of Epoch: 82; Loss 1.301899 \n",
            "Final Result for Epoch 82: Loss 1.301880; Val Acc 0.288938; Train Acc 0.473499\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 83; Loss 1.288404 \n",
            "Step 2 of Epoch: 83; Loss 1.284552 \n",
            "Step 3 of Epoch: 83; Loss 1.279138 \n",
            "Step 4 of Epoch: 83; Loss 1.283018 \n",
            "Step 5 of Epoch: 83; Loss 1.284287 \n",
            "Step 6 of Epoch: 83; Loss 1.286398 \n",
            "Step 7 of Epoch: 83; Loss 1.284896 \n",
            "Step 8 of Epoch: 83; Loss 1.287040 \n",
            "Step 9 of Epoch: 83; Loss 1.287173 \n",
            "Step 10 of Epoch: 83; Loss 1.285908 \n",
            "Step 11 of Epoch: 83; Loss 1.286418 \n",
            "Step 12 of Epoch: 83; Loss 1.286211 \n",
            "Step 13 of Epoch: 83; Loss 1.286335 \n",
            "Step 14 of Epoch: 83; Loss 1.288702 \n",
            "Step 15 of Epoch: 83; Loss 1.290275 \n",
            "Step 16 of Epoch: 83; Loss 1.291289 \n",
            "Step 17 of Epoch: 83; Loss 1.290797 \n",
            "Step 18 of Epoch: 83; Loss 1.292672 \n",
            "Step 19 of Epoch: 83; Loss 1.293352 \n",
            "Step 20 of Epoch: 83; Loss 1.293471 \n",
            "Step 21 of Epoch: 83; Loss 1.294008 \n",
            "Step 22 of Epoch: 83; Loss 1.295140 \n",
            "Step 23 of Epoch: 83; Loss 1.295215 \n",
            "Step 24 of Epoch: 83; Loss 1.295862 \n",
            "Step 25 of Epoch: 83; Loss 1.296240 \n",
            "Step 26 of Epoch: 83; Loss 1.296847 \n",
            "Step 27 of Epoch: 83; Loss 1.297116 \n",
            "Step 28 of Epoch: 83; Loss 1.297478 \n",
            "Step 29 of Epoch: 83; Loss 1.298354 \n",
            "Step 30 of Epoch: 83; Loss 1.298434 \n",
            "Step 31 of Epoch: 83; Loss 1.298829 \n",
            "Step 32 of Epoch: 83; Loss 1.299198 \n",
            "Step 33 of Epoch: 83; Loss 1.299779 \n",
            "Step 34 of Epoch: 83; Loss 1.299995 \n",
            "Step 35 of Epoch: 83; Loss 1.300190 \n",
            "Step 36 of Epoch: 83; Loss 1.300066 \n",
            "Step 37 of Epoch: 83; Loss 1.300555 \n",
            "Step 38 of Epoch: 83; Loss 1.301081 \n",
            "Step 39 of Epoch: 83; Loss 1.301458 \n",
            "Step 40 of Epoch: 83; Loss 1.301613 \n",
            "Final Result for Epoch 83: Loss 1.301629; Val Acc 0.287406; Train Acc 0.471706\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 84; Loss 1.280905 \n",
            "Step 2 of Epoch: 84; Loss 1.279097 \n",
            "Step 3 of Epoch: 84; Loss 1.280156 \n",
            "Step 4 of Epoch: 84; Loss 1.280849 \n",
            "Step 5 of Epoch: 84; Loss 1.285283 \n",
            "Step 6 of Epoch: 84; Loss 1.285209 \n",
            "Step 7 of Epoch: 84; Loss 1.286339 \n",
            "Step 8 of Epoch: 84; Loss 1.287618 \n",
            "Step 9 of Epoch: 84; Loss 1.287811 \n",
            "Step 10 of Epoch: 84; Loss 1.288096 \n",
            "Step 11 of Epoch: 84; Loss 1.287819 \n",
            "Step 12 of Epoch: 84; Loss 1.288355 \n",
            "Step 13 of Epoch: 84; Loss 1.289900 \n",
            "Step 14 of Epoch: 84; Loss 1.290681 \n",
            "Step 15 of Epoch: 84; Loss 1.292094 \n",
            "Step 16 of Epoch: 84; Loss 1.292631 \n",
            "Step 17 of Epoch: 84; Loss 1.293180 \n",
            "Step 18 of Epoch: 84; Loss 1.292592 \n",
            "Step 19 of Epoch: 84; Loss 1.292908 \n",
            "Step 20 of Epoch: 84; Loss 1.293299 \n",
            "Step 21 of Epoch: 84; Loss 1.293815 \n",
            "Step 22 of Epoch: 84; Loss 1.293936 \n",
            "Step 23 of Epoch: 84; Loss 1.294022 \n",
            "Step 24 of Epoch: 84; Loss 1.294814 \n",
            "Step 25 of Epoch: 84; Loss 1.294976 \n",
            "Step 26 of Epoch: 84; Loss 1.295054 \n",
            "Step 27 of Epoch: 84; Loss 1.295295 \n",
            "Step 28 of Epoch: 84; Loss 1.295606 \n",
            "Step 29 of Epoch: 84; Loss 1.295733 \n",
            "Step 30 of Epoch: 84; Loss 1.296547 \n",
            "Step 31 of Epoch: 84; Loss 1.296894 \n",
            "Step 32 of Epoch: 84; Loss 1.297199 \n",
            "Step 33 of Epoch: 84; Loss 1.297727 \n",
            "Step 34 of Epoch: 84; Loss 1.298016 \n",
            "Step 35 of Epoch: 84; Loss 1.298033 \n",
            "Step 36 of Epoch: 84; Loss 1.298531 \n",
            "Step 37 of Epoch: 84; Loss 1.298831 \n",
            "Step 38 of Epoch: 84; Loss 1.298893 \n",
            "Step 39 of Epoch: 84; Loss 1.299323 \n",
            "Step 40 of Epoch: 84; Loss 1.299851 \n",
            "Final Result for Epoch 84: Loss 1.299859; Val Acc 0.290375; Train Acc 0.473108\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 85; Loss 1.260785 \n",
            "Step 2 of Epoch: 85; Loss 1.271757 \n",
            "Step 3 of Epoch: 85; Loss 1.278255 \n",
            "Step 4 of Epoch: 85; Loss 1.281043 \n",
            "Step 5 of Epoch: 85; Loss 1.281071 \n",
            "Step 6 of Epoch: 85; Loss 1.280431 \n",
            "Step 7 of Epoch: 85; Loss 1.280849 \n",
            "Step 8 of Epoch: 85; Loss 1.285688 \n",
            "Step 9 of Epoch: 85; Loss 1.285574 \n",
            "Step 10 of Epoch: 85; Loss 1.284883 \n",
            "Step 11 of Epoch: 85; Loss 1.285173 \n",
            "Step 12 of Epoch: 85; Loss 1.284949 \n",
            "Step 13 of Epoch: 85; Loss 1.286359 \n",
            "Step 14 of Epoch: 85; Loss 1.286683 \n",
            "Step 15 of Epoch: 85; Loss 1.288228 \n",
            "Step 16 of Epoch: 85; Loss 1.289484 \n",
            "Step 17 of Epoch: 85; Loss 1.289087 \n",
            "Step 18 of Epoch: 85; Loss 1.289234 \n",
            "Step 19 of Epoch: 85; Loss 1.290642 \n",
            "Step 20 of Epoch: 85; Loss 1.290565 \n",
            "Step 21 of Epoch: 85; Loss 1.291184 \n",
            "Step 22 of Epoch: 85; Loss 1.291128 \n",
            "Step 23 of Epoch: 85; Loss 1.291429 \n",
            "Step 24 of Epoch: 85; Loss 1.292914 \n",
            "Step 25 of Epoch: 85; Loss 1.292863 \n",
            "Step 26 of Epoch: 85; Loss 1.293581 \n",
            "Step 27 of Epoch: 85; Loss 1.293757 \n",
            "Step 28 of Epoch: 85; Loss 1.294095 \n",
            "Step 29 of Epoch: 85; Loss 1.294706 \n",
            "Step 30 of Epoch: 85; Loss 1.294701 \n",
            "Step 31 of Epoch: 85; Loss 1.295243 \n",
            "Step 32 of Epoch: 85; Loss 1.295846 \n",
            "Step 33 of Epoch: 85; Loss 1.296318 \n",
            "Step 34 of Epoch: 85; Loss 1.296749 \n",
            "Step 35 of Epoch: 85; Loss 1.297434 \n",
            "Step 36 of Epoch: 85; Loss 1.298104 \n",
            "Step 37 of Epoch: 85; Loss 1.298625 \n",
            "Step 38 of Epoch: 85; Loss 1.299481 \n",
            "Step 39 of Epoch: 85; Loss 1.299730 \n",
            "Step 40 of Epoch: 85; Loss 1.299883 \n",
            "Final Result for Epoch 85: Loss 1.299874; Val Acc 0.287969; Train Acc 0.475795\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 86; Loss 1.278039 \n",
            "Step 2 of Epoch: 86; Loss 1.273196 \n",
            "Step 3 of Epoch: 86; Loss 1.276603 \n",
            "Step 4 of Epoch: 86; Loss 1.279459 \n",
            "Step 5 of Epoch: 86; Loss 1.280387 \n",
            "Step 6 of Epoch: 86; Loss 1.278331 \n",
            "Step 7 of Epoch: 86; Loss 1.276186 \n",
            "Step 8 of Epoch: 86; Loss 1.278146 \n",
            "Step 9 of Epoch: 86; Loss 1.278591 \n",
            "Step 10 of Epoch: 86; Loss 1.279739 \n",
            "Step 11 of Epoch: 86; Loss 1.280509 \n",
            "Step 12 of Epoch: 86; Loss 1.280174 \n",
            "Step 13 of Epoch: 86; Loss 1.280960 \n",
            "Step 14 of Epoch: 86; Loss 1.282090 \n",
            "Step 15 of Epoch: 86; Loss 1.283949 \n",
            "Step 16 of Epoch: 86; Loss 1.284881 \n",
            "Step 17 of Epoch: 86; Loss 1.285807 \n",
            "Step 18 of Epoch: 86; Loss 1.286701 \n",
            "Step 19 of Epoch: 86; Loss 1.287593 \n",
            "Step 20 of Epoch: 86; Loss 1.288291 \n",
            "Step 21 of Epoch: 86; Loss 1.288761 \n",
            "Step 22 of Epoch: 86; Loss 1.290726 \n",
            "Step 23 of Epoch: 86; Loss 1.290929 \n",
            "Step 24 of Epoch: 86; Loss 1.290815 \n",
            "Step 25 of Epoch: 86; Loss 1.290863 \n",
            "Step 26 of Epoch: 86; Loss 1.291759 \n",
            "Step 27 of Epoch: 86; Loss 1.291990 \n",
            "Step 28 of Epoch: 86; Loss 1.293200 \n",
            "Step 29 of Epoch: 86; Loss 1.293253 \n",
            "Step 30 of Epoch: 86; Loss 1.293559 \n",
            "Step 31 of Epoch: 86; Loss 1.294012 \n",
            "Step 32 of Epoch: 86; Loss 1.293932 \n",
            "Step 33 of Epoch: 86; Loss 1.294198 \n",
            "Step 34 of Epoch: 86; Loss 1.294831 \n",
            "Step 35 of Epoch: 86; Loss 1.295012 \n",
            "Step 36 of Epoch: 86; Loss 1.295360 \n",
            "Step 37 of Epoch: 86; Loss 1.295944 \n",
            "Step 38 of Epoch: 86; Loss 1.296555 \n",
            "Step 39 of Epoch: 86; Loss 1.297116 \n",
            "Step 40 of Epoch: 86; Loss 1.297542 \n",
            "Final Result for Epoch 86: Loss 1.297549; Val Acc 0.289531; Train Acc 0.474190\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 87; Loss 1.279350 \n",
            "Step 2 of Epoch: 87; Loss 1.276924 \n",
            "Step 3 of Epoch: 87; Loss 1.278415 \n",
            "Step 4 of Epoch: 87; Loss 1.279032 \n",
            "Step 5 of Epoch: 87; Loss 1.282314 \n",
            "Step 6 of Epoch: 87; Loss 1.280636 \n",
            "Step 7 of Epoch: 87; Loss 1.280596 \n",
            "Step 8 of Epoch: 87; Loss 1.279906 \n",
            "Step 9 of Epoch: 87; Loss 1.280881 \n",
            "Step 10 of Epoch: 87; Loss 1.282722 \n",
            "Step 11 of Epoch: 87; Loss 1.282675 \n",
            "Step 12 of Epoch: 87; Loss 1.284087 \n",
            "Step 13 of Epoch: 87; Loss 1.284283 \n",
            "Step 14 of Epoch: 87; Loss 1.284867 \n",
            "Step 15 of Epoch: 87; Loss 1.285239 \n",
            "Step 16 of Epoch: 87; Loss 1.286647 \n",
            "Step 17 of Epoch: 87; Loss 1.287672 \n",
            "Step 18 of Epoch: 87; Loss 1.287036 \n",
            "Step 19 of Epoch: 87; Loss 1.287884 \n",
            "Step 20 of Epoch: 87; Loss 1.288071 \n",
            "Step 21 of Epoch: 87; Loss 1.288854 \n",
            "Step 22 of Epoch: 87; Loss 1.289330 \n",
            "Step 23 of Epoch: 87; Loss 1.289750 \n",
            "Step 24 of Epoch: 87; Loss 1.290352 \n",
            "Step 25 of Epoch: 87; Loss 1.289981 \n",
            "Step 26 of Epoch: 87; Loss 1.290939 \n",
            "Step 27 of Epoch: 87; Loss 1.291287 \n",
            "Step 28 of Epoch: 87; Loss 1.291435 \n",
            "Step 29 of Epoch: 87; Loss 1.291748 \n",
            "Step 30 of Epoch: 87; Loss 1.292245 \n",
            "Step 31 of Epoch: 87; Loss 1.292762 \n",
            "Step 32 of Epoch: 87; Loss 1.293690 \n",
            "Step 33 of Epoch: 87; Loss 1.293893 \n",
            "Step 34 of Epoch: 87; Loss 1.294236 \n",
            "Step 35 of Epoch: 87; Loss 1.294642 \n",
            "Step 36 of Epoch: 87; Loss 1.294976 \n",
            "Step 37 of Epoch: 87; Loss 1.295671 \n",
            "Step 38 of Epoch: 87; Loss 1.296122 \n",
            "Step 39 of Epoch: 87; Loss 1.296629 \n",
            "Step 40 of Epoch: 87; Loss 1.296612 \n",
            "Final Result for Epoch 87: Loss 1.296626; Val Acc 0.287906; Train Acc 0.476506\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 88; Loss 1.270713 \n",
            "Step 2 of Epoch: 88; Loss 1.276056 \n",
            "Step 3 of Epoch: 88; Loss 1.274015 \n",
            "Step 4 of Epoch: 88; Loss 1.270870 \n",
            "Step 5 of Epoch: 88; Loss 1.272673 \n",
            "Step 6 of Epoch: 88; Loss 1.272387 \n",
            "Step 7 of Epoch: 88; Loss 1.270594 \n",
            "Step 8 of Epoch: 88; Loss 1.270495 \n",
            "Step 9 of Epoch: 88; Loss 1.271638 \n",
            "Step 10 of Epoch: 88; Loss 1.273191 \n",
            "Step 11 of Epoch: 88; Loss 1.273991 \n",
            "Step 12 of Epoch: 88; Loss 1.274178 \n",
            "Step 13 of Epoch: 88; Loss 1.275265 \n",
            "Step 14 of Epoch: 88; Loss 1.276152 \n",
            "Step 15 of Epoch: 88; Loss 1.277437 \n",
            "Step 16 of Epoch: 88; Loss 1.279674 \n",
            "Step 17 of Epoch: 88; Loss 1.280768 \n",
            "Step 18 of Epoch: 88; Loss 1.281905 \n",
            "Step 19 of Epoch: 88; Loss 1.283040 \n",
            "Step 20 of Epoch: 88; Loss 1.283204 \n",
            "Step 21 of Epoch: 88; Loss 1.283777 \n",
            "Step 22 of Epoch: 88; Loss 1.284840 \n",
            "Step 23 of Epoch: 88; Loss 1.285799 \n",
            "Step 24 of Epoch: 88; Loss 1.287053 \n",
            "Step 25 of Epoch: 88; Loss 1.287693 \n",
            "Step 26 of Epoch: 88; Loss 1.288240 \n",
            "Step 27 of Epoch: 88; Loss 1.288257 \n",
            "Step 28 of Epoch: 88; Loss 1.289283 \n",
            "Step 29 of Epoch: 88; Loss 1.289935 \n",
            "Step 30 of Epoch: 88; Loss 1.290373 \n",
            "Step 31 of Epoch: 88; Loss 1.291501 \n",
            "Step 32 of Epoch: 88; Loss 1.291627 \n",
            "Step 33 of Epoch: 88; Loss 1.291473 \n",
            "Step 34 of Epoch: 88; Loss 1.292061 \n",
            "Step 35 of Epoch: 88; Loss 1.292891 \n",
            "Step 36 of Epoch: 88; Loss 1.293181 \n",
            "Step 37 of Epoch: 88; Loss 1.293743 \n",
            "Step 38 of Epoch: 88; Loss 1.294163 \n",
            "Step 39 of Epoch: 88; Loss 1.294671 \n",
            "Step 40 of Epoch: 88; Loss 1.294982 \n",
            "Final Result for Epoch 88: Loss 1.294952; Val Acc 0.289312; Train Acc 0.474776\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 89; Loss 1.267159 \n",
            "Step 2 of Epoch: 89; Loss 1.274740 \n",
            "Step 3 of Epoch: 89; Loss 1.269665 \n",
            "Step 4 of Epoch: 89; Loss 1.274280 \n",
            "Step 5 of Epoch: 89; Loss 1.274969 \n",
            "Step 6 of Epoch: 89; Loss 1.275600 \n",
            "Step 7 of Epoch: 89; Loss 1.276244 \n",
            "Step 8 of Epoch: 89; Loss 1.277614 \n",
            "Step 9 of Epoch: 89; Loss 1.277613 \n",
            "Step 10 of Epoch: 89; Loss 1.276970 \n",
            "Step 11 of Epoch: 89; Loss 1.277780 \n",
            "Step 12 of Epoch: 89; Loss 1.278148 \n",
            "Step 13 of Epoch: 89; Loss 1.276375 \n",
            "Step 14 of Epoch: 89; Loss 1.278416 \n",
            "Step 15 of Epoch: 89; Loss 1.278896 \n",
            "Step 16 of Epoch: 89; Loss 1.280247 \n",
            "Step 17 of Epoch: 89; Loss 1.281989 \n",
            "Step 18 of Epoch: 89; Loss 1.282084 \n",
            "Step 19 of Epoch: 89; Loss 1.282592 \n",
            "Step 20 of Epoch: 89; Loss 1.283017 \n",
            "Step 21 of Epoch: 89; Loss 1.283944 \n",
            "Step 22 of Epoch: 89; Loss 1.284479 \n",
            "Step 23 of Epoch: 89; Loss 1.286071 \n",
            "Step 24 of Epoch: 89; Loss 1.287230 \n",
            "Step 25 of Epoch: 89; Loss 1.287242 \n",
            "Step 26 of Epoch: 89; Loss 1.287837 \n",
            "Step 27 of Epoch: 89; Loss 1.288288 \n",
            "Step 28 of Epoch: 89; Loss 1.288667 \n",
            "Step 29 of Epoch: 89; Loss 1.289150 \n",
            "Step 30 of Epoch: 89; Loss 1.289789 \n",
            "Step 31 of Epoch: 89; Loss 1.290369 \n",
            "Step 32 of Epoch: 89; Loss 1.291204 \n",
            "Step 33 of Epoch: 89; Loss 1.291781 \n",
            "Step 34 of Epoch: 89; Loss 1.292601 \n",
            "Step 35 of Epoch: 89; Loss 1.292629 \n",
            "Step 36 of Epoch: 89; Loss 1.293013 \n",
            "Step 37 of Epoch: 89; Loss 1.293505 \n",
            "Step 38 of Epoch: 89; Loss 1.294169 \n",
            "Step 39 of Epoch: 89; Loss 1.294838 \n",
            "Step 40 of Epoch: 89; Loss 1.295762 \n",
            "Final Result for Epoch 89: Loss 1.295776; Val Acc 0.289875; Train Acc 0.473897\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 90; Loss 1.260739 \n",
            "Step 2 of Epoch: 90; Loss 1.266461 \n",
            "Step 3 of Epoch: 90; Loss 1.271074 \n",
            "Step 4 of Epoch: 90; Loss 1.270964 \n",
            "Step 5 of Epoch: 90; Loss 1.270897 \n",
            "Step 6 of Epoch: 90; Loss 1.270095 \n",
            "Step 7 of Epoch: 90; Loss 1.271610 \n",
            "Step 8 of Epoch: 90; Loss 1.272559 \n",
            "Step 9 of Epoch: 90; Loss 1.271841 \n",
            "Step 10 of Epoch: 90; Loss 1.274255 \n",
            "Step 11 of Epoch: 90; Loss 1.275348 \n",
            "Step 12 of Epoch: 90; Loss 1.276599 \n",
            "Step 13 of Epoch: 90; Loss 1.276864 \n",
            "Step 14 of Epoch: 90; Loss 1.277336 \n",
            "Step 15 of Epoch: 90; Loss 1.278920 \n",
            "Step 16 of Epoch: 90; Loss 1.280958 \n",
            "Step 17 of Epoch: 90; Loss 1.280793 \n",
            "Step 18 of Epoch: 90; Loss 1.279978 \n",
            "Step 19 of Epoch: 90; Loss 1.280956 \n",
            "Step 20 of Epoch: 90; Loss 1.282390 \n",
            "Step 21 of Epoch: 90; Loss 1.283243 \n",
            "Step 22 of Epoch: 90; Loss 1.284377 \n",
            "Step 23 of Epoch: 90; Loss 1.283731 \n",
            "Step 24 of Epoch: 90; Loss 1.284500 \n",
            "Step 25 of Epoch: 90; Loss 1.284586 \n",
            "Step 26 of Epoch: 90; Loss 1.285268 \n",
            "Step 27 of Epoch: 90; Loss 1.285997 \n",
            "Step 28 of Epoch: 90; Loss 1.286531 \n",
            "Step 29 of Epoch: 90; Loss 1.287161 \n",
            "Step 30 of Epoch: 90; Loss 1.287593 \n",
            "Step 31 of Epoch: 90; Loss 1.288292 \n",
            "Step 32 of Epoch: 90; Loss 1.289250 \n",
            "Step 33 of Epoch: 90; Loss 1.290037 \n",
            "Step 34 of Epoch: 90; Loss 1.290650 \n",
            "Step 35 of Epoch: 90; Loss 1.291283 \n",
            "Step 36 of Epoch: 90; Loss 1.292095 \n",
            "Step 37 of Epoch: 90; Loss 1.292481 \n",
            "Step 38 of Epoch: 90; Loss 1.293759 \n",
            "Step 39 of Epoch: 90; Loss 1.294347 \n",
            "Step 40 of Epoch: 90; Loss 1.294220 \n",
            "Final Result for Epoch 90: Loss 1.294200; Val Acc 0.291781; Train Acc 0.477330\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 91; Loss 1.268326 \n",
            "Step 2 of Epoch: 91; Loss 1.264989 \n",
            "Step 3 of Epoch: 91; Loss 1.267146 \n",
            "Step 4 of Epoch: 91; Loss 1.264893 \n",
            "Step 5 of Epoch: 91; Loss 1.270209 \n",
            "Step 6 of Epoch: 91; Loss 1.271752 \n",
            "Step 7 of Epoch: 91; Loss 1.271935 \n",
            "Step 8 of Epoch: 91; Loss 1.271457 \n",
            "Step 9 of Epoch: 91; Loss 1.274800 \n",
            "Step 10 of Epoch: 91; Loss 1.274279 \n",
            "Step 11 of Epoch: 91; Loss 1.275128 \n",
            "Step 12 of Epoch: 91; Loss 1.275740 \n",
            "Step 13 of Epoch: 91; Loss 1.275654 \n",
            "Step 14 of Epoch: 91; Loss 1.274557 \n",
            "Step 15 of Epoch: 91; Loss 1.274727 \n",
            "Step 16 of Epoch: 91; Loss 1.275764 \n",
            "Step 17 of Epoch: 91; Loss 1.277391 \n",
            "Step 18 of Epoch: 91; Loss 1.278280 \n",
            "Step 19 of Epoch: 91; Loss 1.279856 \n",
            "Step 20 of Epoch: 91; Loss 1.279918 \n",
            "Step 21 of Epoch: 91; Loss 1.280958 \n",
            "Step 22 of Epoch: 91; Loss 1.281610 \n",
            "Step 23 of Epoch: 91; Loss 1.282624 \n",
            "Step 24 of Epoch: 91; Loss 1.284024 \n",
            "Step 25 of Epoch: 91; Loss 1.284551 \n",
            "Step 26 of Epoch: 91; Loss 1.285578 \n",
            "Step 27 of Epoch: 91; Loss 1.286175 \n",
            "Step 28 of Epoch: 91; Loss 1.287055 \n",
            "Step 29 of Epoch: 91; Loss 1.287720 \n",
            "Step 30 of Epoch: 91; Loss 1.288340 \n",
            "Step 31 of Epoch: 91; Loss 1.288908 \n",
            "Step 32 of Epoch: 91; Loss 1.289633 \n",
            "Step 33 of Epoch: 91; Loss 1.290316 \n",
            "Step 34 of Epoch: 91; Loss 1.290825 \n",
            "Step 35 of Epoch: 91; Loss 1.290995 \n",
            "Step 36 of Epoch: 91; Loss 1.291372 \n",
            "Step 37 of Epoch: 91; Loss 1.291825 \n",
            "Step 38 of Epoch: 91; Loss 1.292062 \n",
            "Step 39 of Epoch: 91; Loss 1.292209 \n",
            "Step 40 of Epoch: 91; Loss 1.292954 \n",
            "Final Result for Epoch 91: Loss 1.292954; Val Acc 0.290844; Train Acc 0.478236\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 92; Loss 1.269891 \n",
            "Step 2 of Epoch: 92; Loss 1.272667 \n",
            "Step 3 of Epoch: 92; Loss 1.268439 \n",
            "Step 4 of Epoch: 92; Loss 1.268754 \n",
            "Step 5 of Epoch: 92; Loss 1.268475 \n",
            "Step 6 of Epoch: 92; Loss 1.270059 \n",
            "Step 7 of Epoch: 92; Loss 1.272576 \n",
            "Step 8 of Epoch: 92; Loss 1.273935 \n",
            "Step 9 of Epoch: 92; Loss 1.275785 \n",
            "Step 10 of Epoch: 92; Loss 1.275127 \n",
            "Step 11 of Epoch: 92; Loss 1.276729 \n",
            "Step 12 of Epoch: 92; Loss 1.277460 \n",
            "Step 13 of Epoch: 92; Loss 1.277428 \n",
            "Step 14 of Epoch: 92; Loss 1.277199 \n",
            "Step 15 of Epoch: 92; Loss 1.279203 \n",
            "Step 16 of Epoch: 92; Loss 1.279706 \n",
            "Step 17 of Epoch: 92; Loss 1.281585 \n",
            "Step 18 of Epoch: 92; Loss 1.282232 \n",
            "Step 19 of Epoch: 92; Loss 1.283251 \n",
            "Step 20 of Epoch: 92; Loss 1.283199 \n",
            "Step 21 of Epoch: 92; Loss 1.283950 \n",
            "Step 22 of Epoch: 92; Loss 1.284295 \n",
            "Step 23 of Epoch: 92; Loss 1.285224 \n",
            "Step 24 of Epoch: 92; Loss 1.285777 \n",
            "Step 25 of Epoch: 92; Loss 1.286553 \n",
            "Step 26 of Epoch: 92; Loss 1.286554 \n",
            "Step 27 of Epoch: 92; Loss 1.286631 \n",
            "Step 28 of Epoch: 92; Loss 1.287065 \n",
            "Step 29 of Epoch: 92; Loss 1.288384 \n",
            "Step 30 of Epoch: 92; Loss 1.288314 \n",
            "Step 31 of Epoch: 92; Loss 1.288504 \n",
            "Step 32 of Epoch: 92; Loss 1.289520 \n",
            "Step 33 of Epoch: 92; Loss 1.290051 \n",
            "Step 34 of Epoch: 92; Loss 1.291016 \n",
            "Step 35 of Epoch: 92; Loss 1.291387 \n",
            "Step 36 of Epoch: 92; Loss 1.291669 \n",
            "Step 37 of Epoch: 92; Loss 1.292228 \n",
            "Step 38 of Epoch: 92; Loss 1.292376 \n",
            "Step 39 of Epoch: 92; Loss 1.292348 \n",
            "Step 40 of Epoch: 92; Loss 1.292787 \n",
            "Final Result for Epoch 92: Loss 1.292731; Val Acc 0.288719; Train Acc 0.478611\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 93; Loss 1.277451 \n",
            "Step 2 of Epoch: 93; Loss 1.268746 \n",
            "Step 3 of Epoch: 93; Loss 1.271906 \n",
            "Step 4 of Epoch: 93; Loss 1.272874 \n",
            "Step 5 of Epoch: 93; Loss 1.273314 \n",
            "Step 6 of Epoch: 93; Loss 1.272986 \n",
            "Step 7 of Epoch: 93; Loss 1.274212 \n",
            "Step 8 of Epoch: 93; Loss 1.275769 \n",
            "Step 9 of Epoch: 93; Loss 1.276953 \n",
            "Step 10 of Epoch: 93; Loss 1.278525 \n",
            "Step 11 of Epoch: 93; Loss 1.278003 \n",
            "Step 12 of Epoch: 93; Loss 1.279360 \n",
            "Step 13 of Epoch: 93; Loss 1.280340 \n",
            "Step 14 of Epoch: 93; Loss 1.280936 \n",
            "Step 15 of Epoch: 93; Loss 1.281515 \n",
            "Step 16 of Epoch: 93; Loss 1.281263 \n",
            "Step 17 of Epoch: 93; Loss 1.281623 \n",
            "Step 18 of Epoch: 93; Loss 1.281395 \n",
            "Step 19 of Epoch: 93; Loss 1.281586 \n",
            "Step 20 of Epoch: 93; Loss 1.281749 \n",
            "Step 21 of Epoch: 93; Loss 1.282333 \n",
            "Step 22 of Epoch: 93; Loss 1.282924 \n",
            "Step 23 of Epoch: 93; Loss 1.283074 \n",
            "Step 24 of Epoch: 93; Loss 1.283586 \n",
            "Step 25 of Epoch: 93; Loss 1.284397 \n",
            "Step 26 of Epoch: 93; Loss 1.284908 \n",
            "Step 27 of Epoch: 93; Loss 1.285762 \n",
            "Step 28 of Epoch: 93; Loss 1.286224 \n",
            "Step 29 of Epoch: 93; Loss 1.286839 \n",
            "Step 30 of Epoch: 93; Loss 1.286973 \n",
            "Step 31 of Epoch: 93; Loss 1.287355 \n",
            "Step 32 of Epoch: 93; Loss 1.287872 \n",
            "Step 33 of Epoch: 93; Loss 1.288361 \n",
            "Step 34 of Epoch: 93; Loss 1.288960 \n",
            "Step 35 of Epoch: 93; Loss 1.289598 \n",
            "Step 36 of Epoch: 93; Loss 1.289729 \n",
            "Step 37 of Epoch: 93; Loss 1.289727 \n",
            "Step 38 of Epoch: 93; Loss 1.290682 \n",
            "Step 39 of Epoch: 93; Loss 1.290948 \n",
            "Step 40 of Epoch: 93; Loss 1.291474 \n",
            "Final Result for Epoch 93: Loss 1.291450; Val Acc 0.287594; Train Acc 0.477693\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 94; Loss 1.263050 \n",
            "Step 2 of Epoch: 94; Loss 1.260673 \n",
            "Step 3 of Epoch: 94; Loss 1.262290 \n",
            "Step 4 of Epoch: 94; Loss 1.265643 \n",
            "Step 5 of Epoch: 94; Loss 1.268304 \n",
            "Step 6 of Epoch: 94; Loss 1.267049 \n",
            "Step 7 of Epoch: 94; Loss 1.268698 \n",
            "Step 8 of Epoch: 94; Loss 1.268034 \n",
            "Step 9 of Epoch: 94; Loss 1.269619 \n",
            "Step 10 of Epoch: 94; Loss 1.271874 \n",
            "Step 11 of Epoch: 94; Loss 1.271260 \n",
            "Step 12 of Epoch: 94; Loss 1.272937 \n",
            "Step 13 of Epoch: 94; Loss 1.274003 \n",
            "Step 14 of Epoch: 94; Loss 1.274475 \n",
            "Step 15 of Epoch: 94; Loss 1.275471 \n",
            "Step 16 of Epoch: 94; Loss 1.275097 \n",
            "Step 17 of Epoch: 94; Loss 1.276069 \n",
            "Step 18 of Epoch: 94; Loss 1.276538 \n",
            "Step 19 of Epoch: 94; Loss 1.277004 \n",
            "Step 20 of Epoch: 94; Loss 1.277419 \n",
            "Step 21 of Epoch: 94; Loss 1.278549 \n",
            "Step 22 of Epoch: 94; Loss 1.279426 \n",
            "Step 23 of Epoch: 94; Loss 1.280418 \n",
            "Step 24 of Epoch: 94; Loss 1.281208 \n",
            "Step 25 of Epoch: 94; Loss 1.282219 \n",
            "Step 26 of Epoch: 94; Loss 1.282449 \n",
            "Step 27 of Epoch: 94; Loss 1.282864 \n",
            "Step 28 of Epoch: 94; Loss 1.283294 \n",
            "Step 29 of Epoch: 94; Loss 1.283882 \n",
            "Step 30 of Epoch: 94; Loss 1.284884 \n",
            "Step 31 of Epoch: 94; Loss 1.285526 \n",
            "Step 32 of Epoch: 94; Loss 1.285788 \n",
            "Step 33 of Epoch: 94; Loss 1.286960 \n",
            "Step 34 of Epoch: 94; Loss 1.287611 \n",
            "Step 35 of Epoch: 94; Loss 1.287751 \n",
            "Step 36 of Epoch: 94; Loss 1.288368 \n",
            "Step 37 of Epoch: 94; Loss 1.289088 \n",
            "Step 38 of Epoch: 94; Loss 1.289737 \n",
            "Step 39 of Epoch: 94; Loss 1.290155 \n",
            "Step 40 of Epoch: 94; Loss 1.290751 \n",
            "Final Result for Epoch 94: Loss 1.290724; Val Acc 0.287313; Train Acc 0.477084\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 95; Loss 1.262836 \n",
            "Step 2 of Epoch: 95; Loss 1.258823 \n",
            "Step 3 of Epoch: 95; Loss 1.264361 \n",
            "Step 4 of Epoch: 95; Loss 1.262106 \n",
            "Step 5 of Epoch: 95; Loss 1.262440 \n",
            "Step 6 of Epoch: 95; Loss 1.262589 \n",
            "Step 7 of Epoch: 95; Loss 1.263625 \n",
            "Step 8 of Epoch: 95; Loss 1.264644 \n",
            "Step 9 of Epoch: 95; Loss 1.264281 \n",
            "Step 10 of Epoch: 95; Loss 1.265168 \n",
            "Step 11 of Epoch: 95; Loss 1.266455 \n",
            "Step 12 of Epoch: 95; Loss 1.268436 \n",
            "Step 13 of Epoch: 95; Loss 1.270261 \n",
            "Step 14 of Epoch: 95; Loss 1.271040 \n",
            "Step 15 of Epoch: 95; Loss 1.271530 \n",
            "Step 16 of Epoch: 95; Loss 1.272400 \n",
            "Step 17 of Epoch: 95; Loss 1.272827 \n",
            "Step 18 of Epoch: 95; Loss 1.274000 \n",
            "Step 19 of Epoch: 95; Loss 1.275007 \n",
            "Step 20 of Epoch: 95; Loss 1.275660 \n",
            "Step 21 of Epoch: 95; Loss 1.277235 \n",
            "Step 22 of Epoch: 95; Loss 1.277889 \n",
            "Step 23 of Epoch: 95; Loss 1.279204 \n",
            "Step 24 of Epoch: 95; Loss 1.279960 \n",
            "Step 25 of Epoch: 95; Loss 1.280957 \n",
            "Step 26 of Epoch: 95; Loss 1.281541 \n",
            "Step 27 of Epoch: 95; Loss 1.282482 \n",
            "Step 28 of Epoch: 95; Loss 1.283680 \n",
            "Step 29 of Epoch: 95; Loss 1.283774 \n",
            "Step 30 of Epoch: 95; Loss 1.284924 \n",
            "Step 31 of Epoch: 95; Loss 1.284725 \n",
            "Step 32 of Epoch: 95; Loss 1.285341 \n",
            "Step 33 of Epoch: 95; Loss 1.285983 \n",
            "Step 34 of Epoch: 95; Loss 1.286279 \n",
            "Step 35 of Epoch: 95; Loss 1.286663 \n",
            "Step 36 of Epoch: 95; Loss 1.287297 \n",
            "Step 37 of Epoch: 95; Loss 1.287847 \n",
            "Step 38 of Epoch: 95; Loss 1.288243 \n",
            "Step 39 of Epoch: 95; Loss 1.288792 \n",
            "Step 40 of Epoch: 95; Loss 1.289473 \n",
            "Final Result for Epoch 95: Loss 1.289514; Val Acc 0.288594; Train Acc 0.477494\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 96; Loss 1.273170 \n",
            "Step 2 of Epoch: 96; Loss 1.271225 \n",
            "Step 3 of Epoch: 96; Loss 1.275127 \n",
            "Step 4 of Epoch: 96; Loss 1.275661 \n",
            "Step 5 of Epoch: 96; Loss 1.276665 \n",
            "Step 6 of Epoch: 96; Loss 1.276115 \n",
            "Step 7 of Epoch: 96; Loss 1.274784 \n",
            "Step 8 of Epoch: 96; Loss 1.272757 \n",
            "Step 9 of Epoch: 96; Loss 1.272714 \n",
            "Step 10 of Epoch: 96; Loss 1.273360 \n",
            "Step 11 of Epoch: 96; Loss 1.273106 \n",
            "Step 12 of Epoch: 96; Loss 1.274008 \n",
            "Step 13 of Epoch: 96; Loss 1.273937 \n",
            "Step 14 of Epoch: 96; Loss 1.273976 \n",
            "Step 15 of Epoch: 96; Loss 1.275411 \n",
            "Step 16 of Epoch: 96; Loss 1.276226 \n",
            "Step 17 of Epoch: 96; Loss 1.276972 \n",
            "Step 18 of Epoch: 96; Loss 1.277379 \n",
            "Step 19 of Epoch: 96; Loss 1.277734 \n",
            "Step 20 of Epoch: 96; Loss 1.278590 \n",
            "Step 21 of Epoch: 96; Loss 1.279642 \n",
            "Step 22 of Epoch: 96; Loss 1.280382 \n",
            "Step 23 of Epoch: 96; Loss 1.281638 \n",
            "Step 24 of Epoch: 96; Loss 1.282388 \n",
            "Step 25 of Epoch: 96; Loss 1.283391 \n",
            "Step 26 of Epoch: 96; Loss 1.284640 \n",
            "Step 27 of Epoch: 96; Loss 1.285708 \n",
            "Step 28 of Epoch: 96; Loss 1.286363 \n",
            "Step 29 of Epoch: 96; Loss 1.286620 \n",
            "Step 30 of Epoch: 96; Loss 1.286402 \n",
            "Step 31 of Epoch: 96; Loss 1.286201 \n",
            "Step 32 of Epoch: 96; Loss 1.286860 \n",
            "Step 33 of Epoch: 96; Loss 1.287618 \n",
            "Step 34 of Epoch: 96; Loss 1.288400 \n",
            "Step 35 of Epoch: 96; Loss 1.288780 \n",
            "Step 36 of Epoch: 96; Loss 1.289283 \n",
            "Step 37 of Epoch: 96; Loss 1.290007 \n",
            "Step 38 of Epoch: 96; Loss 1.290268 \n",
            "Step 39 of Epoch: 96; Loss 1.290388 \n",
            "Step 40 of Epoch: 96; Loss 1.290630 \n",
            "Final Result for Epoch 96: Loss 1.290609; Val Acc 0.291594; Train Acc 0.477470\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 97; Loss 1.274705 \n",
            "Step 2 of Epoch: 97; Loss 1.273395 \n",
            "Step 3 of Epoch: 97; Loss 1.265298 \n",
            "Step 4 of Epoch: 97; Loss 1.265490 \n",
            "Step 5 of Epoch: 97; Loss 1.266242 \n",
            "Step 6 of Epoch: 97; Loss 1.269034 \n",
            "Step 7 of Epoch: 97; Loss 1.270488 \n",
            "Step 8 of Epoch: 97; Loss 1.269262 \n",
            "Step 9 of Epoch: 97; Loss 1.270396 \n",
            "Step 10 of Epoch: 97; Loss 1.269523 \n",
            "Step 11 of Epoch: 97; Loss 1.271439 \n",
            "Step 12 of Epoch: 97; Loss 1.272312 \n",
            "Step 13 of Epoch: 97; Loss 1.273056 \n",
            "Step 14 of Epoch: 97; Loss 1.273966 \n",
            "Step 15 of Epoch: 97; Loss 1.275151 \n",
            "Step 16 of Epoch: 97; Loss 1.274849 \n",
            "Step 17 of Epoch: 97; Loss 1.275935 \n",
            "Step 18 of Epoch: 97; Loss 1.275929 \n",
            "Step 19 of Epoch: 97; Loss 1.276941 \n",
            "Step 20 of Epoch: 97; Loss 1.277684 \n",
            "Step 21 of Epoch: 97; Loss 1.277312 \n",
            "Step 22 of Epoch: 97; Loss 1.278131 \n",
            "Step 23 of Epoch: 97; Loss 1.279289 \n",
            "Step 24 of Epoch: 97; Loss 1.279409 \n",
            "Step 25 of Epoch: 97; Loss 1.280904 \n",
            "Step 26 of Epoch: 97; Loss 1.281966 \n",
            "Step 27 of Epoch: 97; Loss 1.281769 \n",
            "Step 28 of Epoch: 97; Loss 1.281941 \n",
            "Step 29 of Epoch: 97; Loss 1.282631 \n",
            "Step 30 of Epoch: 97; Loss 1.283396 \n",
            "Step 31 of Epoch: 97; Loss 1.284004 \n",
            "Step 32 of Epoch: 97; Loss 1.284662 \n",
            "Step 33 of Epoch: 97; Loss 1.284881 \n",
            "Step 34 of Epoch: 97; Loss 1.285394 \n",
            "Step 35 of Epoch: 97; Loss 1.285376 \n",
            "Step 36 of Epoch: 97; Loss 1.285884 \n",
            "Step 37 of Epoch: 97; Loss 1.286270 \n",
            "Step 38 of Epoch: 97; Loss 1.286761 \n",
            "Step 39 of Epoch: 97; Loss 1.287608 \n",
            "Step 40 of Epoch: 97; Loss 1.287859 \n",
            "Final Result for Epoch 97: Loss 1.287861; Val Acc 0.290094; Train Acc 0.478884\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 98; Loss 1.254810 \n",
            "Step 2 of Epoch: 98; Loss 1.257123 \n",
            "Step 3 of Epoch: 98; Loss 1.268467 \n",
            "Step 4 of Epoch: 98; Loss 1.268227 \n",
            "Step 5 of Epoch: 98; Loss 1.267499 \n",
            "Step 6 of Epoch: 98; Loss 1.269928 \n",
            "Step 7 of Epoch: 98; Loss 1.270211 \n",
            "Step 8 of Epoch: 98; Loss 1.270372 \n",
            "Step 9 of Epoch: 98; Loss 1.270369 \n",
            "Step 10 of Epoch: 98; Loss 1.270781 \n",
            "Step 11 of Epoch: 98; Loss 1.271186 \n",
            "Step 12 of Epoch: 98; Loss 1.271410 \n",
            "Step 13 of Epoch: 98; Loss 1.272207 \n",
            "Step 14 of Epoch: 98; Loss 1.273699 \n",
            "Step 15 of Epoch: 98; Loss 1.273805 \n",
            "Step 16 of Epoch: 98; Loss 1.274427 \n",
            "Step 17 of Epoch: 98; Loss 1.274872 \n",
            "Step 18 of Epoch: 98; Loss 1.275567 \n",
            "Step 19 of Epoch: 98; Loss 1.277139 \n",
            "Step 20 of Epoch: 98; Loss 1.277595 \n",
            "Step 21 of Epoch: 98; Loss 1.279038 \n",
            "Step 22 of Epoch: 98; Loss 1.280379 \n",
            "Step 23 of Epoch: 98; Loss 1.281697 \n",
            "Step 24 of Epoch: 98; Loss 1.282453 \n",
            "Step 25 of Epoch: 98; Loss 1.282393 \n",
            "Step 26 of Epoch: 98; Loss 1.283271 \n",
            "Step 27 of Epoch: 98; Loss 1.283895 \n",
            "Step 28 of Epoch: 98; Loss 1.284195 \n",
            "Step 29 of Epoch: 98; Loss 1.284568 \n",
            "Step 30 of Epoch: 98; Loss 1.285277 \n",
            "Step 31 of Epoch: 98; Loss 1.285699 \n",
            "Step 32 of Epoch: 98; Loss 1.286068 \n",
            "Step 33 of Epoch: 98; Loss 1.286168 \n",
            "Step 34 of Epoch: 98; Loss 1.286903 \n",
            "Step 35 of Epoch: 98; Loss 1.287335 \n",
            "Step 36 of Epoch: 98; Loss 1.287999 \n",
            "Step 37 of Epoch: 98; Loss 1.288171 \n",
            "Step 38 of Epoch: 98; Loss 1.288154 \n",
            "Step 39 of Epoch: 98; Loss 1.288952 \n",
            "Step 40 of Epoch: 98; Loss 1.289457 \n",
            "Final Result for Epoch 98: Loss 1.289467; Val Acc 0.287219; Train Acc 0.478294\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 99; Loss 1.259509 \n",
            "Step 2 of Epoch: 99; Loss 1.263417 \n",
            "Step 3 of Epoch: 99; Loss 1.266323 \n",
            "Step 4 of Epoch: 99; Loss 1.271888 \n",
            "Step 5 of Epoch: 99; Loss 1.270870 \n",
            "Step 6 of Epoch: 99; Loss 1.270653 \n",
            "Step 7 of Epoch: 99; Loss 1.268942 \n",
            "Step 8 of Epoch: 99; Loss 1.271325 \n",
            "Step 9 of Epoch: 99; Loss 1.272515 \n",
            "Step 10 of Epoch: 99; Loss 1.271887 \n",
            "Step 11 of Epoch: 99; Loss 1.273107 \n",
            "Step 12 of Epoch: 99; Loss 1.273100 \n",
            "Step 13 of Epoch: 99; Loss 1.274025 \n",
            "Step 14 of Epoch: 99; Loss 1.273884 \n",
            "Step 15 of Epoch: 99; Loss 1.274096 \n",
            "Step 16 of Epoch: 99; Loss 1.274662 \n",
            "Step 17 of Epoch: 99; Loss 1.275005 \n",
            "Step 18 of Epoch: 99; Loss 1.274983 \n",
            "Step 19 of Epoch: 99; Loss 1.275324 \n",
            "Step 20 of Epoch: 99; Loss 1.276342 \n",
            "Step 21 of Epoch: 99; Loss 1.277460 \n",
            "Step 22 of Epoch: 99; Loss 1.277357 \n",
            "Step 23 of Epoch: 99; Loss 1.277799 \n",
            "Step 24 of Epoch: 99; Loss 1.278382 \n",
            "Step 25 of Epoch: 99; Loss 1.279635 \n",
            "Step 26 of Epoch: 99; Loss 1.280387 \n",
            "Step 27 of Epoch: 99; Loss 1.281041 \n",
            "Step 28 of Epoch: 99; Loss 1.281892 \n",
            "Step 29 of Epoch: 99; Loss 1.282406 \n",
            "Step 30 of Epoch: 99; Loss 1.282103 \n",
            "Step 31 of Epoch: 99; Loss 1.282216 \n",
            "Step 32 of Epoch: 99; Loss 1.282206 \n",
            "Step 33 of Epoch: 99; Loss 1.282015 \n",
            "Step 34 of Epoch: 99; Loss 1.282467 \n",
            "Step 35 of Epoch: 99; Loss 1.282756 \n",
            "Step 36 of Epoch: 99; Loss 1.283568 \n",
            "Step 37 of Epoch: 99; Loss 1.284233 \n",
            "Step 38 of Epoch: 99; Loss 1.284950 \n",
            "Step 39 of Epoch: 99; Loss 1.286044 \n",
            "Step 40 of Epoch: 99; Loss 1.286083 \n",
            "Final Result for Epoch 99: Loss 1.286062; Val Acc 0.289719; Train Acc 0.478517\n",
            "-------------------------------------------------------------\n",
            "Step 1 of Epoch: 100; Loss 1.267060 \n",
            "Step 2 of Epoch: 100; Loss 1.265305 \n",
            "Step 3 of Epoch: 100; Loss 1.262269 \n",
            "Step 4 of Epoch: 100; Loss 1.263001 \n",
            "Step 5 of Epoch: 100; Loss 1.264227 \n",
            "Step 6 of Epoch: 100; Loss 1.263067 \n",
            "Step 7 of Epoch: 100; Loss 1.265502 \n",
            "Step 8 of Epoch: 100; Loss 1.266705 \n",
            "Step 9 of Epoch: 100; Loss 1.267772 \n",
            "Step 10 of Epoch: 100; Loss 1.267919 \n",
            "Step 11 of Epoch: 100; Loss 1.269667 \n",
            "Step 12 of Epoch: 100; Loss 1.270868 \n",
            "Step 13 of Epoch: 100; Loss 1.270596 \n",
            "Step 14 of Epoch: 100; Loss 1.270313 \n",
            "Step 15 of Epoch: 100; Loss 1.270295 \n",
            "Step 16 of Epoch: 100; Loss 1.271171 \n",
            "Step 17 of Epoch: 100; Loss 1.272824 \n",
            "Step 18 of Epoch: 100; Loss 1.273547 \n",
            "Step 19 of Epoch: 100; Loss 1.275547 \n",
            "Step 20 of Epoch: 100; Loss 1.277072 \n",
            "Step 21 of Epoch: 100; Loss 1.278190 \n",
            "Step 22 of Epoch: 100; Loss 1.278509 \n",
            "Step 23 of Epoch: 100; Loss 1.279909 \n",
            "Step 24 of Epoch: 100; Loss 1.280806 \n",
            "Step 25 of Epoch: 100; Loss 1.280868 \n",
            "Step 26 of Epoch: 100; Loss 1.281156 \n",
            "Step 27 of Epoch: 100; Loss 1.281674 \n",
            "Step 28 of Epoch: 100; Loss 1.282019 \n",
            "Step 29 of Epoch: 100; Loss 1.282590 \n",
            "Step 30 of Epoch: 100; Loss 1.283046 \n",
            "Step 31 of Epoch: 100; Loss 1.283471 \n",
            "Step 32 of Epoch: 100; Loss 1.283821 \n",
            "Step 33 of Epoch: 100; Loss 1.284172 \n",
            "Step 34 of Epoch: 100; Loss 1.284883 \n",
            "Step 35 of Epoch: 100; Loss 1.285285 \n",
            "Step 36 of Epoch: 100; Loss 1.285617 \n",
            "Step 37 of Epoch: 100; Loss 1.286197 \n",
            "Step 38 of Epoch: 100; Loss 1.286429 \n",
            "Step 39 of Epoch: 100; Loss 1.286751 \n",
            "Step 40 of Epoch: 100; Loss 1.287012 \n",
            "Final Result for Epoch 100: Loss 1.287003; Val Acc 0.290000; Train Acc 0.476803\n",
            "-------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXYQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFnCAYAAAChL+DqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "total training time: 7213 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QA282W2hdZau"
      },
      "cell_type": "markdown",
      "source": [
        "### Saving the model"
      ]
    },
    {
      "metadata": {
        "id": "HdK0qz6ejj_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "0c2d6963-6c40-41a7-cf3d-817fb6a68cc0"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gLctz-w5jxmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/checkpoint_biLSTM_Asad.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iGIvu-ysukwk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'checkpoint_biLSTM_Asad.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "F0TSZ1O2dfo-"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "h9plkTXediGq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('checkpoint_biLSTM.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eGTAC6Wudkv3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FqbUdJgD0iec"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TMF8kPVVvh91",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tweet Test\n",
        "happy_tweet = 'Im happy'\n",
        "sad_tweet = 'Im sad'\n",
        "angry = 'Im angry'\n",
        "surprised_tweet = 'Im surprised'\n",
        "disgusted_tweet = 'Im disgusted'\n",
        "afraid_tweet = 'Im afraid'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eO3IhcoX0vqy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tweet_to_glove_index(tweet,glove_dict):\n",
        "    tweets_ints = []\n",
        "    tweet = inference_tweet_cleanup(tweet)\n",
        "    idxs = [glove_dict.stoi[w]        # lookup the index of word\n",
        "            for w in tweet.split()\n",
        "            if w in glove_dict.stoi] # keep words that has an embedding\n",
        "    tweets_ints.append(idxs)\n",
        "    return tweets_ints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eBnsPtrJ1NgH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_emotion = {0:'Happy', 1:'Sad' , 2:'Angry', 3:'Surprised', 4:'Disgusted', 5:'Afraid'}\n",
        "\n",
        "def predict(model, test_tweet, sequence_length=max(tweets_lens),use_gpu=True):\n",
        "    \n",
        "    \n",
        "    # tokenize tweet\n",
        "    test_ints = tweet_to_glove_index(test_tweet,glove)\n",
        "    \n",
        "    # pad tokenized sequence\n",
        "    seq_length=sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    # convert to tensor to pass into your model\n",
        "    if use_gpu:\n",
        "      feature_tensor = torch.from_numpy(features).cuda()\n",
        "    else:\n",
        "      feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    \n",
        "    # get the output from the model\n",
        "    output = model(feature_tensor)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    output_prob = nn.functional.softmax(output,dim=1)\n",
        "    top_n_pred = output_prob.topk(3,dim=1) ## top 3 preds\n",
        "    top_n_pred_prob, top_n_pred_index = top_n_pred[0].detach().cpu().numpy()[0], top_n_pred[1].detach().cpu().numpy()[0]\n",
        "    print(test_tweet)\n",
        "    print('Prediction:')\n",
        "    for prob,index in zip(top_n_pred_prob,top_n_pred_index):\n",
        "      print(int_to_emotion[index] , 'with' , str(int(prob*100))+\"%\", 'confidence')\n",
        "    print('---------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TyMRFkX43fhn",
        "outputId": "5ca81296-ce4f-422a-a19a-6c55fe2a5b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        }
      },
      "cell_type": "code",
      "source": [
        "predict(model,happy_tweet)\n",
        "predict(model,sad_tweet)\n",
        "predict(model,angry)\n",
        "predict(model,surprised_tweet)\n",
        "predict(model,disgusted_tweet)\n",
        "predict(model,afraid_tweet)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Im happy\n",
            "Prediction:\n",
            "Happy with 92% confidence\n",
            "Angry with 2% confidence\n",
            "Afraid with 1% confidence\n",
            "---------------\n",
            "Im sad\n",
            "Prediction:\n",
            "Sad with 87% confidence\n",
            "Happy with 3% confidence\n",
            "Disgusted with 3% confidence\n",
            "---------------\n",
            "Im angry\n",
            "Prediction:\n",
            "Disgusted with 35% confidence\n",
            "Angry with 31% confidence\n",
            "Sad with 20% confidence\n",
            "---------------\n",
            "Im surprised\n",
            "Prediction:\n",
            "Happy with 36% confidence\n",
            "Surprised with 25% confidence\n",
            "Afraid with 20% confidence\n",
            "---------------\n",
            "Im disgusted\n",
            "Prediction:\n",
            "Disgusted with 56% confidence\n",
            "Sad with 20% confidence\n",
            "Angry with 14% confidence\n",
            "---------------\n",
            "Im afraid\n",
            "Prediction:\n",
            "Happy with 46% confidence\n",
            "Disgusted with 16% confidence\n",
            "Angry with 12% confidence\n",
            "---------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AUhEs_I4hJ8M",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets = ['Im so happy',\n",
        "          'I feel so down today',\n",
        "          'It boils my blood to see you',\n",
        "         'wow what a nice car',\n",
        "         'im sick of this shit',\n",
        "         'theres a stranger at my home']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MollcketAEYZ",
        "outputId": "0fbed27b-7d64-41fc-ffd0-3d0f5c645ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        }
      },
      "cell_type": "code",
      "source": [
        "for tweet in tweets:\n",
        "  predict(model, tweet)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Im so happy\n",
            "Prediction:\n",
            "Happy with 59% confidence\n",
            "Sad with 14% confidence\n",
            "Afraid with 13% confidence\n",
            "---------------\n",
            "I feel so down today\n",
            "Prediction:\n",
            "Sad with 44% confidence\n",
            "Disgusted with 29% confidence\n",
            "Angry with 19% confidence\n",
            "---------------\n",
            "It boils my blood to see you\n",
            "Prediction:\n",
            "Surprised with 27% confidence\n",
            "Afraid with 20% confidence\n",
            "Happy with 19% confidence\n",
            "---------------\n",
            "wow what a nice car\n",
            "Prediction:\n",
            "Happy with 49% confidence\n",
            "Afraid with 24% confidence\n",
            "Surprised with 18% confidence\n",
            "---------------\n",
            "im sick of this shit\n",
            "Prediction:\n",
            "Disgusted with 59% confidence\n",
            "Angry with 38% confidence\n",
            "Sad with 2% confidence\n",
            "---------------\n",
            "theres a stranger at my home\n",
            "Prediction:\n",
            "Surprised with 48% confidence\n",
            "Afraid with 17% confidence\n",
            "Sad with 9% confidence\n",
            "---------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ttE3WT7qAGgM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}