{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Base_Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "S9NeXRWQgm5R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Todos:\n",
        "### 1. preprocess the data to make it suitable for glove embeding:\n",
        "### a. remove ' from words( don't -> dont)\n",
        "### b. convert all letters to lowercase ( Hoping -> hoping)\n",
        "### 2. find the frequency of each emoji to see if the dataset is balanced and balance the dataset(important)"
      ]
    },
    {
      "metadata": {
        "id": "kjt9iuwEgm5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGMIDRTHgm5b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "26_C6S4Ugm5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    header = ['tweet','label']\n",
        "    data_set = pandas.read_csv('cleaned_data.txt',delimiter='\\t',names = header)\n",
        "    return data_set\n",
        "\n",
        "def split_tweet(tweet):\n",
        "    return tweet.split()    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bmOzSIDpgm5h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_set = get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ey347C1Zgm5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "ce31101c-9ca9-4eeb-91a3-3e1cb342235e"
      },
      "cell_type": "code",
      "source": [
        "glove = torchtext.vocab.GloVe(name=\"twitter.27B\",dim=50)\n",
        "\n",
        "# inser padding character into glove embedding, we overwrite the first element and that's okay\n",
        "# because we don't use the first element in our vocab\n",
        "glove.vectors[0] = torch.tensor(np.zeros(50))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.twitter.27B.zip: 1.52GB [00:26, 57.7MB/s]                            \n",
            "100%|█████████▉| 1190290/1193514 [00:35<00:00, 34480.83it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "sQ8JIzR4gm5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40b72f39-e9d8-49a1-981f-a708cbe863d7"
      },
      "cell_type": "code",
      "source": [
        "len(data_set)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161893"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "tA4Ahaifgm53",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        },
        "outputId": "2b162c17-4e58-4162-d3b6-ae981a64e099"
      },
      "cell_type": "code",
      "source": [
        "data_set"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hoping i dont screw up this interview</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i feel like a baby kangaroo stuck in its mothe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>girl ppl should be happy i even remembered her...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>oh ,  the irony if misha wins the choice tv sc...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i miss you to  ,  you so fake now</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>i miss you to  ,  you so fake now</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>i know</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>bacolod please ? !</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>things can change so quickly</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>all me &amp;amp ;  vic do is laugh .  .  anybody o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>do u have to remind me i was bored right after...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>its ok love .  you are sweet ,  but i know you...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>nainai has said bitch twice in 4 minutes and i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>louis thank you for everything you do you dese...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>impractical jokers its almost always on in our...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>my eyes are in pain .  cant get over the movie...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>daydrinking</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>genitin</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>like like like</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>eye need to get these brows done asap</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>i almost choked on my retainer just now</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>ms .  mendiola works miracles! shes so great</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>oh and parking dont even get me started on the...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>oh and parking dont even get me started on the...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>trying to stay my ass in the burbs but im gett...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>oh my god riley is so cute</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>why doe</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>i cant wait to be on the beach tomorrow</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>- warrior basti also will have his claws cut .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>not bein selfish .  .  im only workin wit one arm</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161863</th>\n",
              "      <td>wish i knew her last name</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161864</th>\n",
              "      <td>lmao is it bad if my friends hit me up to tell...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161865</th>\n",
              "      <td>when he calls you his partner in crime</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161866</th>\n",
              "      <td>nigga .  .  you don watch the new episode ?</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161867</th>\n",
              "      <td>but he died cause of a car accident no one kil...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161868</th>\n",
              "      <td>what we looking for ?</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161869</th>\n",
              "      <td>all i do is share shit so that evens it out</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161870</th>\n",
              "      <td>sister goals let stop for people say im acting...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161871</th>\n",
              "      <td>sister goals let stop for people say im acting...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161872</th>\n",
              "      <td>is so cant wait to see him live at</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161873</th>\n",
              "      <td>i was chilling and i still am .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161874</th>\n",
              "      <td>tirs au but</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161875</th>\n",
              "      <td>yeaah im just tryin a new style</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161876</th>\n",
              "      <td>you off earlyyyy ?  lets do something please</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161877</th>\n",
              "      <td>thats how blew i am .  .  .  a nigga couldnt e...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161878</th>\n",
              "      <td>stressed is such an understatement the only th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161879</th>\n",
              "      <td>stressed is such an understatement the only th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161880</th>\n",
              "      <td>happy birthday to you my beautiful sk may god ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161881</th>\n",
              "      <td>thanks</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161882</th>\n",
              "      <td>carebear w/ the teeth</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161883</th>\n",
              "      <td>brooo .  its so frustrating .  theres no reaso...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161884</th>\n",
              "      <td>on that note ;  goodnight</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161885</th>\n",
              "      <td>i just realized something ive never been in a ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161886</th>\n",
              "      <td>i just realized something ive never been in a ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161887</th>\n",
              "      <td>im boutta go take a nap</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161888</th>\n",
              "      <td>closing sucked but its still been a good day</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161889</th>\n",
              "      <td>thank god my friends love me and will listen t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161890</th>\n",
              "      <td>blowed</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161891</th>\n",
              "      <td>london agent + ibs = full of shit</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161892</th>\n",
              "      <td>london agent + ibs = full of shit</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>161893 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    tweet  label\n",
              "0                   hoping i dont screw up this interview      0\n",
              "1       i feel like a baby kangaroo stuck in its mothe...      1\n",
              "2       girl ppl should be happy i even remembered her...      0\n",
              "3       oh ,  the irony if misha wins the choice tv sc...      2\n",
              "4                       i miss you to  ,  you so fake now      2\n",
              "5                       i miss you to  ,  you so fake now      4\n",
              "6                                                  i know      1\n",
              "7                                      bacolod please ? !      0\n",
              "8                            things can change so quickly      1\n",
              "9       all me &amp ;  vic do is laugh .  .  anybody o...      1\n",
              "10      do u have to remind me i was bored right after...      0\n",
              "11      its ok love .  you are sweet ,  but i know you...      0\n",
              "12      nainai has said bitch twice in 4 minutes and i...      0\n",
              "13      louis thank you for everything you do you dese...      0\n",
              "14      impractical jokers its almost always on in our...      0\n",
              "15      my eyes are in pain .  cant get over the movie...      1\n",
              "16                                            daydrinking      0\n",
              "17                                                genitin      0\n",
              "18                                         like like like      0\n",
              "19                  eye need to get these brows done asap      1\n",
              "20                i almost choked on my retainer just now      1\n",
              "21           ms .  mendiola works miracles! shes so great      1\n",
              "22      oh and parking dont even get me started on the...      2\n",
              "23      oh and parking dont even get me started on the...      4\n",
              "24      trying to stay my ass in the burbs but im gett...      1\n",
              "25                             oh my god riley is so cute      0\n",
              "26                                                why doe      0\n",
              "27                i cant wait to be on the beach tomorrow      0\n",
              "28        - warrior basti also will have his claws cut .       0\n",
              "29      not bein selfish .  .  im only workin wit one arm      0\n",
              "...                                                   ...    ...\n",
              "161863                          wish i knew her last name      4\n",
              "161864  lmao is it bad if my friends hit me up to tell...      0\n",
              "161865             when he calls you his partner in crime      0\n",
              "161866       nigga .  .  you don watch the new episode ?       5\n",
              "161867  but he died cause of a car accident no one kil...      0\n",
              "161868                             what we looking for ?       3\n",
              "161869        all i do is share shit so that evens it out      0\n",
              "161870  sister goals let stop for people say im acting...      0\n",
              "161871  sister goals let stop for people say im acting...      1\n",
              "161872                 is so cant wait to see him live at      1\n",
              "161873                   i was chilling and i still am .       0\n",
              "161874                                        tirs au but      1\n",
              "161875                    yeaah im just tryin a new style      0\n",
              "161876       you off earlyyyy ?  lets do something please      1\n",
              "161877  thats how blew i am .  .  .  a nigga couldnt e...      4\n",
              "161878  stressed is such an understatement the only th...      0\n",
              "161879  stressed is such an understatement the only th...      1\n",
              "161880  happy birthday to you my beautiful sk may god ...      0\n",
              "161881                                             thanks      0\n",
              "161882                              carebear w/ the teeth      0\n",
              "161883  brooo .  its so frustrating .  theres no reaso...      1\n",
              "161884                          on that note ;  goodnight      0\n",
              "161885  i just realized something ive never been in a ...      3\n",
              "161886  i just realized something ive never been in a ...      5\n",
              "161887                            im boutta go take a nap      0\n",
              "161888       closing sucked but its still been a good day      0\n",
              "161889  thank god my friends love me and will listen t...      0\n",
              "161890                                             blowed      0\n",
              "161891                  london agent + ibs = full of shit      0\n",
              "161892                  london agent + ibs = full of shit      4\n",
              "\n",
              "[161893 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "cWYeV80Sgm59",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_set_to_glove_index(glove_dict):\n",
        "    tweets_ints, encoded_labels = [],[]\n",
        "    for i in range(len(data_set)):\n",
        "        tweet = data_set['tweet'][i]\n",
        "        label = data_set['label'][i]\n",
        "        if(type(tweet) != str):\n",
        "            continue\n",
        "        idxs = [glove_dict.stoi[w]        # lookup the index of word\n",
        "            for w in tweet.split()\n",
        "            if w in glove_dict.stoi] # keep words that has an embedding\n",
        "        if not idxs: # ignore tweets without any word with an embedding\n",
        "            continue\n",
        "        tweets_ints.append(idxs)\n",
        "        encoded_labels.append(label)\n",
        "    return tweets_ints, encoded_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uci6Anmugm5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets_ints, encoded_labels = data_set_to_glove_index(glove)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sB-kS3issuZ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a4b8fcc2-942a-40d8-ef2f-8b425fce3eb4"
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# outlier review stats\n",
        "tweets_lens = Counter([len(x) for x in tweets_ints])\n",
        "print(\"Zero-length tweets: {}\".format(tweets_lens[0]))\n",
        "print(\"Maximum tweet length: {}\".format(max(tweets_lens)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length tweets: 0\n",
            "Maximum tweet length: 62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CxaLejWos_Tw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_features(tweets_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''\n",
        "    \n",
        "    # getting the correct rows x cols shape\n",
        "    features = np.zeros((len(tweets_ints), seq_length), dtype=int)\n",
        "\n",
        "    # for each review, I grab that review and \n",
        "    for i, row in enumerate(tweets_ints):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wy8MFokXs_9K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_length = max(tweets_lens)\n",
        "\n",
        "features = pad_features(tweets_ints, seq_length=seq_length)\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R9siA7B4tAc7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4312072b-231f-4a7b-bac7-33be22957c35"
      },
      "cell_type": "code",
      "source": [
        "print(features[:30,])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0 ...    85    53  2706]\n",
            " [    0     0     0 ...   221  9193 50306]\n",
            " [    0     0     0 ...   316   226   325]\n",
            " ...\n",
            " [    0     0     0 ...    13  1863   328]\n",
            " [    0     0     0 ... 58381  1465     1]\n",
            " [    0     0     0 ...  1104    96  4799]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gm-jUzQ8teCo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train, Validation, Test Split"
      ]
    },
    {
      "metadata": {
        "id": "8yZGBcHztBGC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "features, encoded_labels = shuffle(features, encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0nYglhndtgYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3ef824ac-f9a5-49ed-eb60-51ec0b1b6716"
      },
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "split_idx = int(len(features)*split_frac)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(128920, 62) \n",
            "Validation set: \t(16115, 62) \n",
            "Test set: \t\t(16115, 62)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BV-wQQmb5d7j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### DataLoader and Batching"
      ]
    },
    {
      "metadata": {
        "id": "FHgkWFzb5eRF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure the SHUFFLE your training data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DxNTXSGU5mpX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "316930c7-c867-487d-d91a-321a100241e2"
      },
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2578"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "AkI6wmX8gm6U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ]
    },
    {
      "metadata": {
        "id": "TdnI3kw0gm6V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TweetLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TweetLSTM, self).__init__()\n",
        "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Look up the embedding\n",
        "        x = self.emb(x)\n",
        "        # Set an initial hidden state and cell state\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        # Forward propagate the LSTM\n",
        "        out, _ = self.rnn(x, (h0, c0))\n",
        "        # Pass the output of the last time step to the classifier\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "num_classes = 6\n",
        "model = TweetLSTM(input_size=50, hidden_size=50, num_classes=num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "90sHCtA9gm6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd02dda3-c47a-4e2f-e615-4c66d33b8d13"
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "    correct, total = 0, 0\n",
        "    for tweets, labels in data_loader:\n",
        "        output = model(tweets)\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += labels.shape[0]\n",
        "    return float(correct) / float(total)\n",
        "\n",
        "get_accuracy(model, test_loader)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.30006211180124226"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "XUd3LjcAgm6g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_rnn_network(model, train, valid, num_epochs=5, learning_rate=1e-5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    losses, train_acc, valid_acc = [], [], []\n",
        "    epochs = []\n",
        "    counter = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        for tweets, labels in train:\n",
        "            counter += 1\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(tweets)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if counter % 100 == 0:\n",
        "              print(\"Step %d of Epoch: %d; Loss %f \" % ( counter/100 ,epoch+1,float(loss)))\n",
        "                \n",
        "        losses.append(float(loss))\n",
        "\n",
        "        epochs.append(epoch)\n",
        "        train_acc.append(get_accuracy(model, train_loader))\n",
        "        valid_acc.append(get_accuracy(model, valid_loader))\n",
        "        print(\"Final Result for Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
        "              epoch+1, loss, train_acc[-1], valid_acc[-1]))\n",
        "        \n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(losses, label=\"Train\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(epochs, train_acc, label=\"Train\")\n",
        "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aPz9Hioigm6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "outputId": "70124639-45ea-441f-9d9e-21d32f570d04"
      },
      "cell_type": "code",
      "source": [
        "model_lstm = TweetLSTM(input_size=50, hidden_size=50, num_classes=num_classes)\n",
        "train_rnn_network(model, train_loader, valid_loader, num_epochs=100, learning_rate=1e-3)\n",
        "get_accuracy(model, test_loader)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 of Epoch: 1; Loss 1.715364 \n",
            "Step 2 of Epoch: 1; Loss 1.720793 \n",
            "Step 3 of Epoch: 1; Loss 1.696173 \n",
            "Step 4 of Epoch: 1; Loss 1.804846 \n",
            "Step 5 of Epoch: 1; Loss 1.570959 \n",
            "Step 6 of Epoch: 1; Loss 1.712306 \n",
            "Step 7 of Epoch: 1; Loss 1.598416 \n",
            "Step 8 of Epoch: 1; Loss 1.623860 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2e34fcbf30bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_rnn_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-149c8d62fb65>\u001b[0m in \u001b[0;36mtrain_rnn_network\u001b[0;34m(model, train, valid, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "QA282W2hdZau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Saving the model"
      ]
    },
    {
      "metadata": {
        "id": "iGIvu-ysukwk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F0TSZ1O2dfo-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the model"
      ]
    },
    {
      "metadata": {
        "id": "h9plkTXediGq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eGTAC6Wudkv3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FqbUdJgD0iec",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ]
    },
    {
      "metadata": {
        "id": "TMF8kPVVvh91",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tweet Test\n",
        "happy_tweet = 'Im happy'\n",
        "sad_tweet = 'Im sad'\n",
        "angry = 'Im angry'\n",
        "surprised_tweet = 'Im surprised'\n",
        "disgusted_tweet = 'Im disgusted'\n",
        "afraid_tweet = 'Im afraid'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eO3IhcoX0vqy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tweet_to_glove_index(tweet,glove_dict):\n",
        "    tweets_ints = []\n",
        "    tweet = tweet.lower()\n",
        "    idxs = [glove_dict.stoi[w]        # lookup the index of word\n",
        "            for w in tweet.split()\n",
        "            if w in glove_dict.stoi] # keep words that has an embedding\n",
        "    tweets_ints.append(idxs)\n",
        "    return tweets_ints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eBnsPtrJ1NgH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_emotion = {0:'Happy', 1:'Sad' , 2:'Angry', 3:'Surprised', 4:'Disgusted', 5:'Afraid'}\n",
        "\n",
        "def predict(model, test_tweet, sequence_length=max(tweets_lens)):\n",
        "    \n",
        "    \n",
        "    # tokenize tweet\n",
        "    test_ints = tweet_to_glove_index(test_tweet,glove)\n",
        "    \n",
        "    # pad tokenized sequence\n",
        "    seq_length=sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    # convert to tensor to pass into your model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    \n",
        "    # get the output from the model\n",
        "    output = model(feature_tensor)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    output_prob = nn.functional.softmax(output,dim=1)\n",
        "    top_n_pred = output_prob.topk(3,dim=1) ## top 3 preds\n",
        "    top_n_pred_prob, top_n_pred_index = top_n_pred[0].detach().numpy()[0], top_n_pred[1].detach().numpy()[0]\n",
        "    print(test_tweet)\n",
        "    print('Prediction:')\n",
        "    for prob,index in zip(top_n_pred_prob,top_n_pred_index):\n",
        "      print(int_to_emotion[index] , 'with' , str(int(prob*100))+\"%\", 'confidence')\n",
        "    print('---------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TyMRFkX43fhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "329311cb-88ca-492a-ea66-182178665399"
      },
      "cell_type": "code",
      "source": [
        "predict(model,happy_tweet)\n",
        "predict(model,sad_tweet)\n",
        "predict(model,angry)\n",
        "predict(model,surprised_tweet)\n",
        "predict(model,disgusted_tweet)\n",
        "predict(model,afraid_tweet)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Im happy\n",
            "Prediction:\n",
            "Happy with 46% confidence\n",
            "Surprised with 24% confidence\n",
            "Afraid with 12% confidence\n",
            "---------------\n",
            "Im sad\n",
            "Prediction:\n",
            "Sad with 82% confidence\n",
            "Disgusted with 5% confidence\n",
            "Happy with 5% confidence\n",
            "---------------\n",
            "Im angry\n",
            "Prediction:\n",
            "Disgusted with 34% confidence\n",
            "Angry with 32% confidence\n",
            "Happy with 10% confidence\n",
            "---------------\n",
            "Im surprised\n",
            "Prediction:\n",
            "Happy with 68% confidence\n",
            "Sad with 8% confidence\n",
            "Disgusted with 8% confidence\n",
            "---------------\n",
            "Im disgusted\n",
            "Prediction:\n",
            "Disgusted with 30% confidence\n",
            "Angry with 25% confidence\n",
            "Sad with 21% confidence\n",
            "---------------\n",
            "Im afraid\n",
            "Prediction:\n",
            "Surprised with 27% confidence\n",
            "Sad with 23% confidence\n",
            "Afraid with 22% confidence\n",
            "---------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AUhEs_I4hJ8M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets = ['he said he will be there and he showed up','']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MollcketAEYZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "79a301b7-03a5-4593-f425-a79dba5a9135"
      },
      "cell_type": "code",
      "source": [
        "for tweet in tweets:\n",
        "  predict(model, tweet)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he said he will be there and he showed up\n",
            "Prediction:\n",
            "Happy with 32% confidence\n",
            "Surprised with 20% confidence\n",
            "Sad with 17% confidence\n",
            "---------------\n",
            "he said he will be there but he did not show up\n",
            "Prediction:\n",
            "Sad with 29% confidence\n",
            "Surprised with 19% confidence\n",
            "Disgusted with 18% confidence\n",
            "---------------\n",
            "he said he will be there and he showed up with his friends\n",
            "Prediction:\n",
            "Surprised with 24% confidence\n",
            "Happy with 20% confidence\n",
            "Sad with 16% confidence\n",
            "---------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ttE3WT7qAGgM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}