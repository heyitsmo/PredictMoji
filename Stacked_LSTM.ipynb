{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stacked_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kFbOl1oKNfAp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i3Od2_l9NfAw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmlhgXFPNfA0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    header = ['tweet','label']\n",
        "    data_set = pandas.read_csv('cleaned_data.txt',delimiter='\\t',names = header)\n",
        "    return data_set\n",
        "\n",
        "# def get_label_mapping():\n",
        "#     original_list = interesting_labels.wanted_list\n",
        "#     output_list = [i for i in range(len(original_list))]\n",
        "#     return dict(zip(original_list,output_list))\n",
        "  \n",
        "def split_tweet(tweet):\n",
        "    return tweet.split()    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G8jn8m8_NfA4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_set = get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HPZEG2URNfA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Create a vocabulary set that will contain all the uniue words in the tweets in our dataset\n",
        "'''\n",
        "all_words = []\n",
        "list_of_list_of_tweets = []\n",
        "for i in range(len(data_set['tweet'])):\n",
        "    words = data_set['tweet'][i].split()\n",
        "    list_of_list_of_tweets.append(words)\n",
        "    for word in words:\n",
        "        all_words.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zpvzBdr3NfBA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0194ba9-f66c-432e-9e9f-e8ad49100353"
      },
      "cell_type": "code",
      "source": [
        "len(list_of_list_of_tweets)\n",
        "list_of_list_of_tweets[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hoping', 'i', 'dont', 'screw', 'up', 'this', 'interview']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "rG9kdT_INfBF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tDAM5OtENfBI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(all_words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rNfOxWNNfBL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20240f80-064b-40d5-c556-53978feeb757"
      },
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62373"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "NLVcaDjxNfBQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ## use the dict to tokenize each review in reviews_split\n",
        "# ## store the tokenized reviews in reviews_ints\n",
        "tweets_ints = []\n",
        "for tweet in list_of_list_of_tweets:\n",
        "    tweets_ints.append([vocab_to_int[word] for word in tweet])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5yLelns0NfBT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecf6afdc-2d17-46e6-9f89-5e7a0529fcf1"
      },
      "cell_type": "code",
      "source": [
        "len(tweets_ints)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161893"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "NEMuXoVHNfBW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoded_labels = np.array(data_set['label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZegOWWP4NfBY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "17e6e2d2-40a8-4d81-aeb4-b585325dd635"
      },
      "cell_type": "code",
      "source": [
        "# outlier review stats\n",
        "tweets_lens = Counter([len(x) for x in tweets_ints])\n",
        "print(\"Zero-length tweets: {}\".format(tweets_lens[0]))\n",
        "print(\"Maximum tweet length: {}\".format(max(tweets_lens)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length tweets: 0\n",
            "Maximum tweet length: 62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uT2g7RFPNfBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "8ec99a54-c2c9-4285-b558-8faa6be2f403"
      },
      "cell_type": "code",
      "source": [
        "tweets_lens"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 3340,\n",
              "         2: 7871,\n",
              "         3: 9817,\n",
              "         4: 11462,\n",
              "         5: 11761,\n",
              "         6: 11545,\n",
              "         7: 10978,\n",
              "         8: 9911,\n",
              "         9: 8730,\n",
              "         10: 8037,\n",
              "         11: 7391,\n",
              "         12: 6781,\n",
              "         13: 5946,\n",
              "         14: 5389,\n",
              "         15: 4935,\n",
              "         16: 4642,\n",
              "         17: 4190,\n",
              "         18: 3768,\n",
              "         19: 3585,\n",
              "         20: 3189,\n",
              "         21: 2961,\n",
              "         22: 2723,\n",
              "         23: 2384,\n",
              "         24: 2283,\n",
              "         25: 2004,\n",
              "         26: 1690,\n",
              "         27: 1479,\n",
              "         28: 1073,\n",
              "         29: 721,\n",
              "         30: 523,\n",
              "         31: 301,\n",
              "         32: 206,\n",
              "         33: 120,\n",
              "         34: 63,\n",
              "         35: 39,\n",
              "         36: 25,\n",
              "         37: 11,\n",
              "         38: 6,\n",
              "         39: 4,\n",
              "         40: 2,\n",
              "         41: 2,\n",
              "         42: 1,\n",
              "         43: 1,\n",
              "         54: 1,\n",
              "         62: 2})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "duNxgu8QNfBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "03ef1a47-03a8-4a55-dc01-a5c05d3ef5a1"
      },
      "cell_type": "code",
      "source": [
        "for tweet in list_of_list_of_tweets:\n",
        "    if len(tweet) == 62:\n",
        "        print(\" \".join(tweet))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you was way over there . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . and im only ickle not easy pushing through crowds\n",
            "you was way over there . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . and im only ickle not easy pushing through crowds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cemmeu5sNfBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_features(tweets_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''\n",
        "    \n",
        "    # getting the correct rows x cols shape\n",
        "    features = np.zeros((len(tweets_ints), seq_length), dtype=int)\n",
        "\n",
        "    # for each review, I grab that review and \n",
        "    for i, row in enumerate(tweets_ints):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N4UOzWxPNfBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_length = max(tweets_lens)\n",
        "\n",
        "features = pad_features(tweets_ints, seq_length=seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sw3EF2ziNfBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2d321dc9-0d3d-4ece-e53a-2c2239445168"
      },
      "cell_type": "code",
      "source": [
        "print(features[:30,])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0 ...    31    21  1681]\n",
            " [    0     0     0 ...    32  3291 33389]\n",
            " [    0     0     0 ...   137 33390   162]\n",
            " ...\n",
            " [    0     0     0 ...     4   726   159]\n",
            " [    0     0     0 ... 33399   504     1]\n",
            " [    0     0     0 ...   531    53  1726]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qdERE_ICNfBm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train, Validation, Test Split "
      ]
    },
    {
      "metadata": {
        "id": "8B8FLYrGZDZx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "features, encoded_labels = shuffle(features, encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RjFdwVd7NfBn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "40a7112c-ca75-43e9-d780-fb7af5447863"
      },
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "split_idx = int(len(features)*split_frac)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(129514, 62) \n",
            "Validation set: \t(16189, 62) \n",
            "Test set: \t\t(16190, 62)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TPqAx3hoNfBp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### DataLoader and Batching"
      ]
    },
    {
      "metadata": {
        "id": "cBlSqs2ANfBp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure the SHUFFLE your training data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NZAs7wOgNfBq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2e9fe31-47d4-4336-dbaa-54d5ef8a1d61"
      },
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2590"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "g2o85AOxNfBs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the model"
      ]
    },
    {
      "metadata": {
        "id": "454Sp35MNfBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "810cb51d-2d5d-4033-d7d3-8d603663e874"
      },
      "cell_type": "code",
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "whLJ_d2yNfBu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        #self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        #sig_out = self.sig(out)\n",
        "        sig_out = out\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1, output_size)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3I_ob0PHNfBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "64ed1d3f-1eaa-4337-fc7f-600b6c7dfc48"
      },
      "cell_type": "code",
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 6\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 1\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(62374, 400)\n",
            "  (lstm): LSTM(400, 256, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3)\n",
            "  (fc): Linear(in_features=256, out_features=6, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ryw0E51UNfBz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "-Oo5NY6_NfBz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loss and optimization functions\n",
        "lr=0.0001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "54vpZW3DNfB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "adc62d62-0537-4f9b-887e-ad41f894bbb7"
      },
      "cell_type": "code",
      "source": [
        "# training params\n",
        "\n",
        "epochs = 2 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, labels.long())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output, labels.long())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/2... Step: 100... Loss: 1.752846... Val Loss: 1.769815\n",
            "Epoch: 1/2... Step: 200... Loss: 1.732317... Val Loss: 1.756974\n",
            "Epoch: 1/2... Step: 300... Loss: 1.735572... Val Loss: 1.748361\n",
            "Epoch: 1/2... Step: 400... Loss: 1.744648... Val Loss: 1.743155\n",
            "Epoch: 1/2... Step: 500... Loss: 1.714126... Val Loss: 1.738031\n",
            "Epoch: 1/2... Step: 600... Loss: 1.822322... Val Loss: 1.734642\n",
            "Epoch: 1/2... Step: 700... Loss: 1.702513... Val Loss: 1.731913\n",
            "Epoch: 1/2... Step: 800... Loss: 1.618994... Val Loss: 1.726707\n",
            "Epoch: 1/2... Step: 900... Loss: 1.763071... Val Loss: 1.721139\n",
            "Epoch: 1/2... Step: 1000... Loss: 1.731518... Val Loss: 1.717363\n",
            "Epoch: 1/2... Step: 1100... Loss: 1.827555... Val Loss: 1.711233\n",
            "Epoch: 1/2... Step: 1200... Loss: 1.634584... Val Loss: 1.710327\n",
            "Epoch: 1/2... Step: 1300... Loss: 1.726228... Val Loss: 1.704936\n",
            "Epoch: 1/2... Step: 1400... Loss: 1.698503... Val Loss: 1.702734\n",
            "Epoch: 1/2... Step: 1500... Loss: 1.707241... Val Loss: 1.699783\n",
            "Epoch: 1/2... Step: 1600... Loss: 1.722856... Val Loss: 1.696328\n",
            "Epoch: 1/2... Step: 1700... Loss: 1.611880... Val Loss: 1.694339\n",
            "Epoch: 1/2... Step: 1800... Loss: 1.692870... Val Loss: 1.693765\n",
            "Epoch: 1/2... Step: 1900... Loss: 1.774474... Val Loss: 1.689816\n",
            "Epoch: 1/2... Step: 2000... Loss: 1.651470... Val Loss: 1.689311\n",
            "Epoch: 1/2... Step: 2100... Loss: 1.645686... Val Loss: 1.687110\n",
            "Epoch: 1/2... Step: 2200... Loss: 1.652839... Val Loss: 1.683300\n",
            "Epoch: 1/2... Step: 2300... Loss: 1.656503... Val Loss: 1.682323\n",
            "Epoch: 1/2... Step: 2400... Loss: 1.715816... Val Loss: 1.680018\n",
            "Epoch: 1/2... Step: 2500... Loss: 1.575051... Val Loss: 1.678883\n",
            "Epoch: 2/2... Step: 2600... Loss: 1.673474... Val Loss: 1.676770\n",
            "Epoch: 2/2... Step: 2700... Loss: 1.608958... Val Loss: 1.676093\n",
            "Epoch: 2/2... Step: 2800... Loss: 1.694389... Val Loss: 1.675084\n",
            "Epoch: 2/2... Step: 2900... Loss: 1.550788... Val Loss: 1.672268\n",
            "Epoch: 2/2... Step: 3000... Loss: 1.695719... Val Loss: 1.672662\n",
            "Epoch: 2/2... Step: 3100... Loss: 1.598968... Val Loss: 1.669410\n",
            "Epoch: 2/2... Step: 3200... Loss: 1.611436... Val Loss: 1.669032\n",
            "Epoch: 2/2... Step: 3300... Loss: 1.733938... Val Loss: 1.670695\n",
            "Epoch: 2/2... Step: 3400... Loss: 1.706908... Val Loss: 1.666600\n",
            "Epoch: 2/2... Step: 3500... Loss: 1.717434... Val Loss: 1.668934\n",
            "Epoch: 2/2... Step: 3600... Loss: 1.575333... Val Loss: 1.664069\n",
            "Epoch: 2/2... Step: 3700... Loss: 1.628285... Val Loss: 1.662837\n",
            "Epoch: 2/2... Step: 3800... Loss: 1.801995... Val Loss: 1.663208\n",
            "Epoch: 2/2... Step: 3900... Loss: 1.729213... Val Loss: 1.660781\n",
            "Epoch: 2/2... Step: 4000... Loss: 1.641289... Val Loss: 1.659546\n",
            "Epoch: 2/2... Step: 4100... Loss: 1.472245... Val Loss: 1.658169\n",
            "Epoch: 2/2... Step: 4200... Loss: 1.716254... Val Loss: 1.662071\n",
            "Epoch: 2/2... Step: 4300... Loss: 1.628598... Val Loss: 1.658874\n",
            "Epoch: 2/2... Step: 4400... Loss: 1.703389... Val Loss: 1.660249\n",
            "Epoch: 2/2... Step: 4500... Loss: 1.598927... Val Loss: 1.661667\n",
            "Epoch: 2/2... Step: 4600... Loss: 1.692890... Val Loss: 1.659799\n",
            "Epoch: 2/2... Step: 4700... Loss: 1.670850... Val Loss: 1.657996\n",
            "Epoch: 2/2... Step: 4800... Loss: 1.799632... Val Loss: 1.654713\n",
            "Epoch: 2/2... Step: 4900... Loss: 1.605380... Val Loss: 1.657823\n",
            "Epoch: 2/2... Step: 5000... Loss: 1.619779... Val Loss: 1.657178\n",
            "Epoch: 2/2... Step: 5100... Loss: 1.635382... Val Loss: 1.654069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rv5mpr1VbeaZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ]
    },
    {
      "metadata": {
        "id": "osm_NGp0NfB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1594f0eb-f4d2-4551-a874-f8439331c93f"
      },
      "cell_type": "code",
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "correct, total = 0, 0\n",
        "\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output, labels.long())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = output.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "    total += labels.shape[0]\n",
        "    # compare predictions to true label\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = correct/total\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.649\n",
            "Test accuracy: 0.309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IV4f99CAg_qZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ]
    },
    {
      "metadata": {
        "id": "DVJJ82bxhGmR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# negative test review\n",
        "happy_tweet = 'Im so happy'\n",
        "sad_tweet = 'I feel so down today'\n",
        "angry = 'it boils my blood to see you'\n",
        "surprised_tweet = 'wow what a nice car'\n",
        "disgusted_tweet = 'im sick of this shit'\n",
        "afraid_tweet = 'theres a stranger at my home'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E0yUgFRChSHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8b4dac4-3cae-4dde-c3c2-e587efb59422"
      },
      "cell_type": "code",
      "source": [
        "def tokenize_review(test_tweet):\n",
        "    # splitting by spaces\n",
        "    test_words = test_tweet.lower().split()\n",
        "\n",
        "    # tokens\n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "\n",
        "    return test_ints\n",
        "\n",
        "# test code and generate tokenized review\n",
        "test_ints = tokenize_review(sad_tweet)\n",
        "print(test_ints)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 105, 15, 158, 77]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ASB7-USWiBBV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_emotion = {0:'Happy', 1:'Sad' , 2:'Angry', 3:'Surprised', 4:'Disgusted', 5:'Afraid'}\n",
        "\n",
        "def predict(net, test_tweet, sequence_length=max(tweets_lens)):\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "    # tokenize review\n",
        "    test_ints = tokenize_review(test_tweet)\n",
        "    \n",
        "    # pad tokenized sequence\n",
        "    seq_length=sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    # convert to tensor to pass into your model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "    \n",
        "    # get the output from the model\n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = output.max(1, keepdim=True)[1] \n",
        "    # printing output value, before rounding\n",
        "    print(test_tweet,' Prediction:', int_to_emotion[pred.item()])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7P0gyrtjjVrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5105962b-9ea2-4940-af7e-dfc51b863312"
      },
      "cell_type": "code",
      "source": [
        "predict(net,happy_tweet)\n",
        "predict(net,sad_tweet)\n",
        "predict(net,angry)\n",
        "predict(net,surprised_tweet)\n",
        "predict(net,disgusted_tweet)\n",
        "predict(net,afraid_tweet)\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Im so happy  Prediction: Happy\n",
            "I feel so down today  Prediction: Sad\n",
            "it boils my blood to see you  Prediction: Happy\n",
            "wow what a nice car  Prediction: Surprised\n",
            "im sick of this shit  Prediction: Disgusted\n",
            "theres a stranger at my home  Prediction: Happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_vfUZqqSjkSy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}