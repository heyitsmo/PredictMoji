{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweaked_Base_Model_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "S9NeXRWQgm5R"
      },
      "cell_type": "markdown",
      "source": [
        "### Todos:\n",
        "### 1. preprocess the data to make it suitable for glove embeding:\n",
        "### a. remove ' from words( don't -> dont)\n",
        "### b. convert all letters to lowercase ( Hoping -> hoping)\n",
        "### 2. find the frequency of each emoji to see if the dataset is balanced and balance the dataset(important)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kjt9iuwEgm5T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tGMIDRTHgm5b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "26_C6S4Ugm5e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    header = ['tweet','label']\n",
        "    data_set = pandas.read_csv('augmented_data.txt',delimiter='\\t',names = header)\n",
        "    return data_set\n",
        "\n",
        "def split_tweet(tweet):\n",
        "    return tweet.split()    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bmOzSIDpgm5h",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_set = get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ey347C1Zgm5l",
        "outputId": "9e501d11-0ad3-4fd9-b963-d5093c335180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "glove = torchtext.vocab.GloVe(name=\"twitter.27B\",dim=50)\n",
        "\n",
        "# inser padding character into glove embedding, we overwrite the first element and that's okay\n",
        "# because we don't use the first element in our vocab\n",
        "glove.vectors[0] = torch.tensor(np.zeros(50))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.twitter.27B.zip: 1.52GB [02:40, 9.47MB/s]                            \n",
            "100%|█████████▉| 1193019/1193514 [00:36<00:00, 33526.35it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sQ8JIzR4gm5q",
        "outputId": "736ca473-2d35-497b-ace0-d312add4084d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(data_set)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81679"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cWYeV80Sgm59",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_set_to_glove_index(glove_dict):\n",
        "    tweets_ints, encoded_labels = [],[]\n",
        "    for i in range(len(data_set)):\n",
        "        tweet = data_set['tweet'][i]\n",
        "        label = data_set['label'][i]\n",
        "        if(type(tweet) != str):\n",
        "            continue\n",
        "        idxs = [glove_dict.stoi[w]        # lookup the index of word\n",
        "            for w in tweet.split()\n",
        "            if w in glove_dict.stoi] # keep words that has an embedding\n",
        "        if not idxs: # ignore tweets without any word with an embedding\n",
        "            continue\n",
        "        tweets_ints.append(idxs)\n",
        "        encoded_labels.append(label)\n",
        "    return tweets_ints, encoded_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uci6Anmugm5-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets_ints, encoded_labels = data_set_to_glove_index(glove)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sB-kS3issuZ8",
        "outputId": "6d2eb9fc-c97c-4eb5-9cda-642671e7c696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# outlier review stats\n",
        "tweets_lens = Counter([len(x) for x in tweets_ints])\n",
        "print(\"Zero-length tweets: {}\".format(tweets_lens[0]))\n",
        "print(\"Maximum tweet length: {}\".format(max(tweets_lens)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length tweets: 0\n",
            "Maximum tweet length: 2160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CxaLejWos_Tw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_features(tweets_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''\n",
        "    \n",
        "    # getting the correct rows x cols shape\n",
        "    features = np.zeros((len(tweets_ints), seq_length), dtype=int)\n",
        "\n",
        "    # for each review, I grab that review and \n",
        "    for i, row in enumerate(tweets_ints):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wy8MFokXs_9K",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_length = max(tweets_lens)\n",
        "\n",
        "features = pad_features(tweets_ints, seq_length=seq_length)\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gm-jUzQ8teCo"
      },
      "cell_type": "markdown",
      "source": [
        "### Train, Validation, Test Split"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8yZGBcHztBGC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "features, encoded_labels = shuffle(features, encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0nYglhndtgYi",
        "outputId": "b10894a2-a42a-4398-e267-f5fa7b3e8319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "split_idx = int(len(features)*split_frac)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(64977, 2160) \n",
            "Validation set: \t(8122, 2160) \n",
            "Test set: \t\t(8123, 2160)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BV-wQQmb5d7j"
      },
      "cell_type": "markdown",
      "source": [
        "### DataLoader and Batching"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FHgkWFzb5eRF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure the SHUFFLE your training data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DxNTXSGU5mpX",
        "outputId": "9a57b865-81c7-46d4-eecf-e81b60e4ee73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AkI6wmX8gm6U"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TdnI3kw0gm6V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TweetLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TweetLSTM, self).__init__()\n",
        "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Look up the embedding\n",
        "        x = self.emb(x)\n",
        "        # Set an initial hidden state and cell state\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        # Forward propagate the LSTM\n",
        "        out, _ = self.rnn(x, (h0, c0))\n",
        "        # Pass the output of the last time step to the classifier\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "num_classes = 6\n",
        "model = TweetLSTM(input_size=50, hidden_size=50, num_classes=num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "90sHCtA9gm6c",
        "outputId": "d440611e-6172-4062-ab6d-121599960990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "    correct, total = 0, 0\n",
        "    for tweets, labels in data_loader:\n",
        "        output = model(tweets)\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += labels.shape[0]\n",
        "    return float(correct) / float(total)\n",
        "\n",
        "get_accuracy(model, test_loader)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 1193019/1193514 [00:50<00:00, 33526.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1874074074074074"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XUd3LjcAgm6g",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_rnn_network(model, train, valid, num_epochs=5, learning_rate=1e-5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    losses, train_acc, valid_acc = [], [], []\n",
        "    epochs = []\n",
        "    counter = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        for tweets, labels in train:\n",
        "            counter += 1\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(tweets)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if counter % 100 == 0:\n",
        "                print(\"Step %d of Epoch: %d; Loss %f \" % ( counter/100 ,epoch+1,float(loss)))\n",
        "                \n",
        "        losses.append(float(loss))\n",
        "\n",
        "        epochs.append(epoch)\n",
        "        train_acc.append(get_accuracy(model, train_loader))\n",
        "        valid_acc.append(get_accuracy(model, valid_loader))\n",
        "        print(\"Final Result for Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
        "              epoch+1, loss, train_acc[-1], valid_acc[-1]))\n",
        "        \n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(losses, label=\"Train\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(epochs, train_acc, label=\"Train\")\n",
        "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aPz9Hioigm6i",
        "outputId": "a4d51491-f65b-4948-f664-8cf13b9e06fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        }
      },
      "cell_type": "code",
      "source": [
        "model_lstm = TweetLSTM(input_size=50, hidden_size=50, num_classes=num_classes)\n",
        "train_rnn_network(model, train_loader, valid_loader, num_epochs=20, learning_rate=1e-3)\n",
        "get_accuracy(model, test_loader)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 of Epoch: 1; Loss 1.060895 \n",
            "Step 2 of Epoch: 1; Loss 0.995646 \n",
            "Step 3 of Epoch: 1; Loss 1.278215 \n",
            "Step 4 of Epoch: 1; Loss 0.964433 \n",
            "Step 5 of Epoch: 1; Loss 1.071372 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-988ca8f86859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_rnn_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-553a42891400>\u001b[0m in \u001b[0;36mtrain_rnn_network\u001b[0;34m(model, train, valid, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QA282W2hdZau"
      },
      "cell_type": "markdown",
      "source": [
        "### Saving the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iGIvu-ysukwk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "F0TSZ1O2dfo-"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "h9plkTXediGq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#state_dict = torch.load('checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eGTAC6Wudkv3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model.load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FqbUdJgD0iec"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TMF8kPVVvh91",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tweet Test\n",
        "happy_tweet = 'Im happy'\n",
        "sad_tweet = 'Im sad'\n",
        "angry = 'Im angry'\n",
        "surprised_tweet = 'Im surprised'\n",
        "disgusted_tweet = 'Im disgusted'\n",
        "afraid_tweet = 'Im afraid'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eO3IhcoX0vqy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tweet_to_glove_index(tweet,glove_dict):\n",
        "    tweets_ints = []\n",
        "    tweet = tweet.lower()\n",
        "    idxs = [glove_dict.stoi[w]        # lookup the index of word\n",
        "            for w in tweet.split()\n",
        "            if w in glove_dict.stoi] # keep words that has an embedding\n",
        "    tweets_ints.append(idxs)\n",
        "    return tweets_ints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eBnsPtrJ1NgH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_emotion = {0:'Happy', 1:'Sad' , 2:'Angry', 3:'Surprised', 4:'Disgusted', 5:'Afraid'}\n",
        "\n",
        "def predict(model, test_tweet, sequence_length=max(tweets_lens)):\n",
        "    \n",
        "    \n",
        "    # tokenize tweet\n",
        "    test_ints = tweet_to_glove_index(test_tweet,glove)\n",
        "    \n",
        "    # pad tokenized sequence\n",
        "    seq_length=sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    # convert to tensor to pass into your model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    \n",
        "    # get the output from the model\n",
        "    output = model(feature_tensor)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    output_prob = nn.functional.softmax(output,dim=1)\n",
        "    top_n_pred = output_prob.topk(3,dim=1) ## top 3 preds\n",
        "    top_n_pred_prob, top_n_pred_index = top_n_pred[0].detach().numpy()[0], top_n_pred[1].detach().numpy()[0]\n",
        "    print(test_tweet)\n",
        "    print('Prediction:')\n",
        "    for prob,index in zip(top_n_pred_prob,top_n_pred_index):\n",
        "        print(int_to_emotion[index] , 'with' , str(int(prob*100))+\"%\", 'confidence')\n",
        "    print('---------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TyMRFkX43fhn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict(model,happy_tweet)\n",
        "predict(model,sad_tweet)\n",
        "predict(model,angry)\n",
        "predict(model,surprised_tweet)\n",
        "predict(model,disgusted_tweet)\n",
        "predict(model,afraid_tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AUhEs_I4hJ8M",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets = ['he said he will be there and he showed up','he said he will be there but he did not show up', 'he said he will be there and he showed up with his friends',\n",
        "          'I lose', 'I have lost', 'I won', 'Live long', 'I am the best', 'I hate losing', 'I dont hate you', 'I do not hate you']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MollcketAEYZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for tweet in tweets:\n",
        "    predict(model, tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ttE3WT7qAGgM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}