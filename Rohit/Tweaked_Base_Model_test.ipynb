{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweaked_Base_Model_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "S9NeXRWQgm5R"
      },
      "cell_type": "markdown",
      "source": [
        "### Todos:\n",
        "### 1. preprocess the data to make it suitable for glove embeding:\n",
        "### a. remove ' from words( don't -> dont)\n",
        "### b. convert all letters to lowercase ( Hoping -> hoping)\n",
        "### 2. find the frequency of each emoji to see if the dataset is balanced and balance the dataset(important)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kjt9iuwEgm5T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tGMIDRTHgm5b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "26_C6S4Ugm5e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    header = ['tweet','label']\n",
        "    data_set = pandas.read_csv('augmented_data.txt',delimiter='\\t',names = header)\n",
        "    return data_set\n",
        "\n",
        "def split_tweet(tweet):\n",
        "    return tweet.split()    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bmOzSIDpgm5h",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_set = get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ey347C1Zgm5l",
        "outputId": "9e501d11-0ad3-4fd9-b963-d5093c335180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "glove = torchtext.vocab.GloVe(name=\"twitter.27B\",dim=50)\n",
        "\n",
        "# inser padding character into glove embedding, we overwrite the first element and that's okay\n",
        "# because we don't use the first element in our vocab\n",
        "glove.vectors[0] = torch.tensor(np.zeros(50))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.twitter.27B.zip: 1.52GB [02:40, 9.47MB/s]                            \n",
            "100%|█████████▉| 1193019/1193514 [00:36<00:00, 33526.35it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sQ8JIzR4gm5q",
        "outputId": "736ca473-2d35-497b-ace0-d312add4084d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(data_set)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81679"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cWYeV80Sgm59",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_set_to_glove_index(glove_dict):\n",
        "    tweets_ints, encoded_labels = [],[]\n",
        "    for i in range(len(data_set)):\n",
        "        tweet = data_set['tweet'][i]\n",
        "        label = data_set['label'][i]\n",
        "        if(type(tweet) != str):\n",
        "            continue\n",
        "        idxs = [glove_dict.stoi[w]        # lookup the index of word\n",
        "            for w in tweet.split()\n",
        "            if w in glove_dict.stoi] # keep words that has an embedding\n",
        "        if not idxs: # ignore tweets without any word with an embedding\n",
        "            continue\n",
        "        tweets_ints.append(idxs)\n",
        "        encoded_labels.append(label)\n",
        "    return tweets_ints, encoded_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uci6Anmugm5-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets_ints, encoded_labels = data_set_to_glove_index(glove)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sB-kS3issuZ8",
        "outputId": "6d2eb9fc-c97c-4eb5-9cda-642671e7c696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# outlier review stats\n",
        "tweets_lens = Counter([len(x) for x in tweets_ints])\n",
        "print(\"Zero-length tweets: {}\".format(tweets_lens[0]))\n",
        "print(\"Maximum tweet length: {}\".format(max(tweets_lens)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length tweets: 0\n",
            "Maximum tweet length: 2160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CxaLejWos_Tw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_features(tweets_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''\n",
        "    \n",
        "    # getting the correct rows x cols shape\n",
        "    features = np.zeros((len(tweets_ints), seq_length), dtype=int)\n",
        "\n",
        "    # for each review, I grab that review and \n",
        "    for i, row in enumerate(tweets_ints):\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wy8MFokXs_9K",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_length = max(tweets_lens)\n",
        "\n",
        "features = pad_features(tweets_ints, seq_length=seq_length)\n",
        "encoded_labels = np.array(encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gm-jUzQ8teCo"
      },
      "cell_type": "markdown",
      "source": [
        "### Train, Validation, Test Split"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8yZGBcHztBGC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "features, encoded_labels = shuffle(features, encoded_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0nYglhndtgYi",
        "outputId": "b10894a2-a42a-4398-e267-f5fa7b3e8319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "\n",
        "split_idx = int(len(features)*split_frac)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(64977, 2160) \n",
            "Validation set: \t(8122, 2160) \n",
            "Test set: \t\t(8123, 2160)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BV-wQQmb5d7j"
      },
      "cell_type": "markdown",
      "source": [
        "### DataLoader and Batching"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FHgkWFzb5eRF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure the SHUFFLE your training data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DxNTXSGU5mpX",
        "outputId": "9a57b865-81c7-46d4-eecf-e81b60e4ee73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AkI6wmX8gm6U"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TdnI3kw0gm6V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TweetLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TweetLSTM, self).__init__()\n",
        "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Look up the embedding\n",
        "        x = self.emb(x)\n",
        "        # Set an initial hidden state and cell state\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        # Forward propagate the LSTM\n",
        "        out, _ = self.rnn(x, (h0, c0))\n",
        "        # Pass the output of the last time step to the classifier\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "num_classes = 6\n",
        "model = TweetLSTM(input_size=50, hidden_size=50, num_classes=num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "90sHCtA9gm6c",
        "outputId": "d440611e-6172-4062-ab6d-121599960990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "    correct, total = 0, 0\n",
        "    for tweets, labels in data_loader:\n",
        "        output = model(tweets)\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += labels.shape[0]\n",
        "    return float(correct) / float(total)\n",
        "\n",
        "get_accuracy(model, test_loader)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 1193019/1193514 [00:50<00:00, 33526.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1874074074074074"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XUd3LjcAgm6g",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_rnn_network(model, train, valid, num_epochs=5, learning_rate=1e-5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    losses, train_acc, valid_acc = [], [], []\n",
        "    epochs = []\n",
        "    counter = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        for tweets, labels in train:\n",
        "            counter += 1\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(tweets)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if counter % 100 == 0:\n",
        "                print(\"Step %d of Epoch: %d; Loss %f \" % ( counter/100 ,epoch+1,float(loss)))\n",
        "                \n",
        "        losses.append(float(loss))\n",
        "\n",
        "        epochs.append(epoch)\n",
        "        train_acc.append(get_accuracy(model, train_loader))\n",
        "        valid_acc.append(get_accuracy(model, valid_loader))\n",
        "        print(\"Final Result for Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
        "              epoch+1, loss, train_acc[-1], valid_acc[-1]))\n",
        "        \n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(losses, label=\"Train\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(epochs, train_acc, label=\"Train\")\n",
        "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aPz9Hioigm6i",
        "outputId": "707ddd74-e833-496d-88a9-cccb5dfb2147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        }
      },
      "cell_type": "code",
      "source": [
        "model_lstm = TweetLSTM(input_size=50, hidden_size=50, num_classes=num_classes)\n",
        "train_rnn_network(model, train_loader, valid_loader, num_epochs=20, learning_rate=1e-3)\n",
        "get_accuracy(model, test_loader)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-988ca8f86859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_rnn_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-553a42891400>\u001b[0m in \u001b[0;36mtrain_rnn_network\u001b[0;34m(model, train, valid, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1bbe5ec484f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Forward propagate the LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Pass the output of the last time step to the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QA282W2hdZau"
      },
      "cell_type": "markdown",
      "source": [
        "### Saving the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iGIvu-ysukwk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "F0TSZ1O2dfo-"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "h9plkTXediGq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eGTAC6Wudkv3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FqbUdJgD0iec"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TMF8kPVVvh91",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Tweet Test\n",
        "happy_tweet = 'Im happy'\n",
        "sad_tweet = 'Im sad'\n",
        "angry = 'Im angry'\n",
        "surprised_tweet = 'Im surprised'\n",
        "disgusted_tweet = 'Im disgusted'\n",
        "afraid_tweet = 'Im afraid'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eO3IhcoX0vqy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tweet_to_glove_index(tweet,glove_dict):\n",
        "    tweets_ints = []\n",
        "    tweet = tweet.lower()\n",
        "    idxs = [glove_dict.stoi[w]        # lookup the index of word\n",
        "            for w in tweet.split()\n",
        "            if w in glove_dict.stoi] # keep words that has an embedding\n",
        "    tweets_ints.append(idxs)\n",
        "    return tweets_ints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eBnsPtrJ1NgH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_emotion = {0:'Happy', 1:'Sad' , 2:'Angry', 3:'Surprised', 4:'Disgusted', 5:'Afraid'}\n",
        "\n",
        "def predict(model, test_tweet, sequence_length=max(tweets_lens)):\n",
        "    \n",
        "    \n",
        "    # tokenize tweet\n",
        "    test_ints = tweet_to_glove_index(test_tweet,glove)\n",
        "    \n",
        "    # pad tokenized sequence\n",
        "    seq_length=sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    # convert to tensor to pass into your model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    \n",
        "    # get the output from the model\n",
        "    output = model(feature_tensor)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    output_prob = nn.functional.softmax(output,dim=1)\n",
        "    top_n_pred = output_prob.topk(3,dim=1) ## top 3 preds\n",
        "    top_n_pred_prob, top_n_pred_index = top_n_pred[0].detach().numpy()[0], top_n_pred[1].detach().numpy()[0]\n",
        "    print(test_tweet)\n",
        "    print('Prediction:')\n",
        "    for prob,index in zip(top_n_pred_prob,top_n_pred_index):\n",
        "        print(int_to_emotion[index] , 'with' , str(int(prob*100))+\"%\", 'confidence')\n",
        "    print('---------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TyMRFkX43fhn",
        "outputId": "6c333079-1f17-4e2e-8db1-1800f9276bf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        }
      },
      "cell_type": "code",
      "source": [
        "predict(model,happy_tweet)\n",
        "predict(model,sad_tweet)\n",
        "predict(model,angry)\n",
        "predict(model,surprised_tweet)\n",
        "predict(model,disgusted_tweet)\n",
        "predict(model,afraid_tweet)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Im happy\n",
            "Prediction:\n",
            "Happy with 46% confidence\n",
            "Surprised with 24% confidence\n",
            "Afraid with 12% confidence\n",
            "---------------\n",
            "Im sad\n",
            "Prediction:\n",
            "Sad with 82% confidence\n",
            "Disgusted with 5% confidence\n",
            "Happy with 5% confidence\n",
            "---------------\n",
            "Im angry\n",
            "Prediction:\n",
            "Disgusted with 34% confidence\n",
            "Angry with 32% confidence\n",
            "Happy with 10% confidence\n",
            "---------------\n",
            "Im surprised\n",
            "Prediction:\n",
            "Happy with 68% confidence\n",
            "Sad with 8% confidence\n",
            "Disgusted with 8% confidence\n",
            "---------------\n",
            "Im disgusted\n",
            "Prediction:\n",
            "Disgusted with 30% confidence\n",
            "Angry with 25% confidence\n",
            "Sad with 21% confidence\n",
            "---------------\n",
            "Im afraid\n",
            "Prediction:\n",
            "Surprised with 27% confidence\n",
            "Sad with 23% confidence\n",
            "Afraid with 22% confidence\n",
            "---------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AUhEs_I4hJ8M",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets = ['he said he will be there and he showed up','he said he will be there but he did not show up', 'he said he will be there and he showed up with his friends',\n",
        "          'I lose', 'I have lost', 'I won', 'Live long', 'I am the best', 'I hate losing', 'I dont hate you', 'I do not hate you']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MollcketAEYZ",
        "outputId": "aacc48bb-0b49-4d67-e105-d10364266ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1205
        }
      },
      "cell_type": "code",
      "source": [
        "for tweet in tweets:\n",
        "    predict(model, tweet)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he said he will be there and he showed up\n",
            "Prediction:\n",
            "Happy with 32% confidence\n",
            "Surprised with 20% confidence\n",
            "Sad with 17% confidence\n",
            "---------------\n",
            "he said he will be there but he did not show up\n",
            "Prediction:\n",
            "Sad with 29% confidence\n",
            "Surprised with 19% confidence\n",
            "Disgusted with 18% confidence\n",
            "---------------\n",
            "he said he will be there and he showed up with his friends\n",
            "Prediction:\n",
            "Surprised with 24% confidence\n",
            "Happy with 20% confidence\n",
            "Sad with 16% confidence\n",
            "---------------\n",
            "I lose\n",
            "Prediction:\n",
            "Happy with 38% confidence\n",
            "Sad with 26% confidence\n",
            "Angry with 14% confidence\n",
            "---------------\n",
            "I have lost\n",
            "Prediction:\n",
            "Happy with 30% confidence\n",
            "Surprised with 24% confidence\n",
            "Afraid with 16% confidence\n",
            "---------------\n",
            "I won\n",
            "Prediction:\n",
            "Happy with 26% confidence\n",
            "Sad with 23% confidence\n",
            "Angry with 14% confidence\n",
            "---------------\n",
            "Live long\n",
            "Prediction:\n",
            "Surprised with 31% confidence\n",
            "Sad with 25% confidence\n",
            "Happy with 23% confidence\n",
            "---------------\n",
            "I am the best\n",
            "Prediction:\n",
            "Happy with 46% confidence\n",
            "Sad with 23% confidence\n",
            "Afraid with 8% confidence\n",
            "---------------\n",
            "I hate losing\n",
            "Prediction:\n",
            "Sad with 46% confidence\n",
            "Angry with 18% confidence\n",
            "Disgusted with 18% confidence\n",
            "---------------\n",
            "I dont hate you\n",
            "Prediction:\n",
            "Angry with 33% confidence\n",
            "Disgusted with 29% confidence\n",
            "Happy with 18% confidence\n",
            "---------------\n",
            "I do not hate you\n",
            "Prediction:\n",
            "Happy with 30% confidence\n",
            "Sad with 29% confidence\n",
            "Disgusted with 16% confidence\n",
            "---------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ttE3WT7qAGgM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}