{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todos:\n",
    "### 1. preprocess the data to make it suitable for glove embeding:\n",
    "### a. remove ' from words( don't -> dont)\n",
    "### b. convert all letters to lowercase ( Hoping -> hoping)\n",
    "### 2. find the frequency of each emoji to see if the dataset is balanced and balance the dataset(important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import interesting_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    header = ['status_id','tweet','label']\n",
    "    data_set = pandas.read_csv('processed.txt',delimiter='\\t',names = header)\n",
    "    return data_set\n",
    "\n",
    "def get_data_set():\n",
    "    return global_data_set\n",
    "\n",
    "def get_glove_embedding(glove): \n",
    "    return nn.Embedding.from_pretrained(glove.vectors)\n",
    "\n",
    "def get_label_mapping():\n",
    "    original_list = interesting_labels.wanted_list\n",
    "    output_list = [i for i in range(len(original_list))]\n",
    "    return dict(zip(original_list,output_list))\n",
    "  \n",
    "def cleanup_tweet(tweet):\n",
    "    if(type(tweet) == str):\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tweet.replace('\\'','')\n",
    "    return tweet\n",
    "    \n",
    "# How many of each category there are\n",
    "# currently: {'Happy': 2097, 'Sad': 938, 'Angry': 235, 'Surprised': 234, 'Disgusted': 64, 'Afraid': 24}\n",
    "def get_total_each_category(data):\n",
    "    total = {\"Happy\":0, \"Sad\":0, \"Angry\":0, \"Surprised\":0, \"Disgusted\":0, \"Afraid\":0}\n",
    "    for i in range(len(data)):\n",
    "        if (data['label'][i] in interesting_labels.Happy):\n",
    "            total['Happy'] += 1\n",
    "        elif (data['label'][i] in interesting_labels.Sad):\n",
    "            total['Sad'] += 1\n",
    "        elif (data['label'][i] in interesting_labels.Angry):\n",
    "            total['Angry'] += 1\n",
    "        elif (data['label'][i] in interesting_labels.Surprised):\n",
    "            total['Surprised'] += 1\n",
    "        elif (data['label'][i] in interesting_labels.Disgusted):\n",
    "            total['Disgusted'] += 1\n",
    "        elif (data['label'][i] in interesting_labels.Afraid):\n",
    "            total['Afraid'] += 1\n",
    "    print (total)\n",
    "\n",
    "# Right now not generalized to all dataset shapes\n",
    "# At the moment will only change 1 word in sad, and the rest as many as possible\n",
    "# to get as many up to same shape\n",
    "def change_to_synonyms(data):\n",
    "    for i in range(len(data)):\n",
    "        if (data['label'][i] in interesting_labels.Sad):\n",
    "            string = data['tweet'][i]\n",
    "            # Need to change 1 synonym, randomly choose in each string\n",
    "            new_data = pandas.DataFrame({'status_id': pandas.Series(data['status_id'], dtype=np.int64),\n",
    "                                         'tweet': pandas.Series(data['tweet'], dtype=str),\n",
    "                                         'label': pandas.Series(data['label'], dtype=np.int64)})\n",
    "            data = data.append(new_data, ignore_index=True)\n",
    "        '''elif (data['label'][i] in interesting_labels.Angry):\n",
    "            for j, string in enumerate(data['tweet'][i]):\n",
    "                new_data = {data['status_id'][i]: (string, data['label'][i])}\n",
    "                data.append(new_data, ignore_index=True)\n",
    "        \n",
    "        elif (data['label'][i] in interesting_labels.Surprised):\n",
    "            for j, string in enumerate(data['tweet'][i]):\n",
    "                # string needs to equal the synoymns\n",
    "                new_data = {data['status_id'][i]: (string, data['label'][i])}\n",
    "                data.append(new_data, ignore_index=True)\n",
    "        \n",
    "        elif (data['label'][i] in interesting_labels.Disgusted):\n",
    "            for j, string in enumerate(data['tweet'][i]):\n",
    "                new_data = {data['status_id'][i]: (string, data['label'][i])}\n",
    "                data.append(new_data, ignore_index=True)\n",
    "        \n",
    "        elif (data['label'][i] in interesting_labels.Afraid):\n",
    "            for j, string in enumerate(data['tweet'][i]):\n",
    "                new_data = {data['status_id'][i]: (string, data['label'][i])}\n",
    "                data.append(new_data, ignore_index=True)'''\n",
    "    print(data.info())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = get_label_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data_set = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torchtext.vocab.GloVe(name=\"twitter.27B\",dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_emb = get_glove_embedding(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = get_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3592"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3592 entries, 0 to 3591\n",
      "Data columns (total 3 columns):\n",
      "status_id    3592 non-null int64\n",
      "tweet        3592 non-null object\n",
      "label        3592 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 84.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Happy': 2097, 'Sad': 938, 'Angry': 235, 'Surprised': 234, 'Disgusted': 64, 'Afraid': 24}\n",
      "938\n"
     ]
    }
   ],
   "source": [
    "get_total_each_category(data_set)\n",
    "count_sad(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-3ea2be83261d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchange_to_synonyms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-168-986684b7d162>\u001b[0m in \u001b[0;36mchange_to_synonyms\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     54\u001b[0m                                          \u001b[1;34m'tweet'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                                          'label': pandas.Series(data['label'], dtype=np.int64)})\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         '''elif (data['label'][i] in interesting_labels.Angry):\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps360\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   6209\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[0;32m   6210\u001b[0m                       \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6211\u001b[1;33m                       sort=sort)\n\u001b[0m\u001b[0;32m   6212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6213\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps360\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                        copy=copy, sort=sort)\n\u001b[1;32m--> 226\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps360\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    421\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m    422\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                 copy=self.copy)\n\u001b[0m\u001b[0;32m    424\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps360\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   5419\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5420\u001b[0m             b = make_block(\n\u001b[1;32m-> 5421\u001b[1;33m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5422\u001b[0m                 placement=placement)\n\u001b[0;32m   5423\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps360\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m   5577\u001b[0m                 \u001b[0mconcat_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5578\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5579\u001b[1;33m         \u001b[0mconcat_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concat_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5581\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aps360\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py\u001b[0m in \u001b[0;36m_concat_compat\u001b[1;34m(to_concat, axis)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0mto_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "data_set = change_to_synonyms(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_total_each_category(data_set)\n",
    "count_sad(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'str'>\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data_set['status_id'][0]))\n",
    "print(type(data_set['tweet'][0]))\n",
    "print(type(data_set['label'][0]))\n",
    "print(type(data_set['status_id'][4521]))\n",
    "print(type(data_set['tweet'][4521]))\n",
    "print(type(data_set['label'][4521]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>744211689319501825</td>\n",
       "      <td>Like hello, I'm watching it too. Mind lowering...</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>742516659219509252</td>\n",
       "      <td>can u Dm ur pics with the twins for my ig fan ...</td>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>746762559387111425</td>\n",
       "      <td>happy birthday jaz. Enjoy your day</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>743395873757892609</td>\n",
       "      <td>got joe and pewds book soon tabinof and da sid...</td>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>747399941434146817</td>\n",
       "      <td>Every time I up this late, I'm usually always ...</td>\n",
       "      <td>1381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>744374136793759744</td>\n",
       "      <td>I can't wait to meet my stepbrother next year.</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>747927031728377857</td>\n",
       "      <td>When you know you can't sing but you're singin...</td>\n",
       "      <td>1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>742575673332531200</td>\n",
       "      <td>I love that's daddy forever</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>747312310121697280</td>\n",
       "      <td>We are lonely</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>747090755890577409</td>\n",
       "      <td>Sleepy in church</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>744311326168780801</td>\n",
       "      <td>5 guys was sooooo worth it</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>746500533553291264</td>\n",
       "      <td>I was one away from hitting at Bingo!</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>747538907449921537</td>\n",
       "      <td>a good idea, I'm all for</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>742407595487547397</td>\n",
       "      <td>Driving in heels might be the most dangerous t...</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>742295120104546304</td>\n",
       "      <td>Excited to actually start work today and final...</td>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>741776463167430664</td>\n",
       "      <td>I Haven't Talked To Twizzy All Day I Swear I B...</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>743322032499089413</td>\n",
       "      <td>you and the homies gon have to pick up after tho</td>\n",
       "      <td>1381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>741401468528435200</td>\n",
       "      <td>.@greysonchance How can anyone ever possibly l...</td>\n",
       "      <td>1381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>744001439983247360</td>\n",
       "      <td>i love u yeri</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>741382849815662594</td>\n",
       "      <td>I don't get back till Monday night wait for me</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>742910363751088128</td>\n",
       "      <td>okay wow my heart</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>741955962420887552</td>\n",
       "      <td>thanks see you around!</td>\n",
       "      <td>1383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>747775902684160000</td>\n",
       "      <td>12 more hours then some holidays</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>747436079976943616</td>\n",
       "      <td>r u gna join us in the pc empire</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>743211787458215936</td>\n",
       "      <td>my maths isn't STRONG ANDREW</td>\n",
       "      <td>1381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>742445566924328961</td>\n",
       "      <td>HOW ARE THEY STILL LOOKING FOR A? It's infuria...</td>\n",
       "      <td>1466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>744726546699681792</td>\n",
       "      <td>The cav's when and after the game D wade comme...</td>\n",
       "      <td>1381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>743674700073598977</td>\n",
       "      <td>Ion Like This Person Im Becoming But Idk</td>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>744453726136578048</td>\n",
       "      <td>Fuck it, something to do</td>\n",
       "      <td>1381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>742668696691171329</td>\n",
       "      <td>Just bread ft. Nutella for today's menu</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>747818614930083844</td>\n",
       "      <td>i should be there ahhhh jealousnyaaaa</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4501</th>\n",
       "      <td>741891215365312512</td>\n",
       "      <td>Standing from 1pm up to now. Hays. So tired.</td>\n",
       "      <td>1421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4502</th>\n",
       "      <td>748271428810711040</td>\n",
       "      <td>I'm not looking forward to the pictures people...</td>\n",
       "      <td>1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4503</th>\n",
       "      <td>744047448662880257</td>\n",
       "      <td>shet maichard</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4504</th>\n",
       "      <td>745011494841024512</td>\n",
       "      <td>but still get out your feelings</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4505</th>\n",
       "      <td>741078978157498368</td>\n",
       "      <td>I have the worst migraine man</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4506</th>\n",
       "      <td>743295968607559680</td>\n",
       "      <td>It Was So Funny To Me</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>743295968607559680</td>\n",
       "      <td>It Was So Funny To Me</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>743232456615202816</td>\n",
       "      <td>Been downtown all day</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4509</th>\n",
       "      <td>743658508520939520</td>\n",
       "      <td>sooner or later ya gone get tired of hating on...</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4510</th>\n",
       "      <td>744977945979138048</td>\n",
       "      <td>I just wanna wear my prom dress again</td>\n",
       "      <td>1422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4511</th>\n",
       "      <td>748021182285357057</td>\n",
       "      <td>Endless summer tour looks so lit man</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4512</th>\n",
       "      <td>746927952215027712</td>\n",
       "      <td>frfr</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4513</th>\n",
       "      <td>748349730585927681</td>\n",
       "      <td>Totally not feeling work today. My head hurts....</td>\n",
       "      <td>1409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4514</th>\n",
       "      <td>743331470312251392</td>\n",
       "      <td>I miss Angie</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4515</th>\n",
       "      <td>744005701501018113</td>\n",
       "      <td>In my mind I left, but my heart still there</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>746726235602980866</td>\n",
       "      <td>I need to stop drinking so much or put my cash...</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4517</th>\n",
       "      <td>744444279230062592</td>\n",
       "      <td>wrong move pwetzzz wrong move ugh so many people</td>\n",
       "      <td>1421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>742036494341275652</td>\n",
       "      <td>Too much bad in the world, and not enough good...</td>\n",
       "      <td>1409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>747289721206767617</td>\n",
       "      <td>Messi saying he's leaving the national team ho...</td>\n",
       "      <td>1413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520</th>\n",
       "      <td>742961773867892737</td>\n",
       "      <td>Need to clean my room</td>\n",
       "      <td>1398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>743560794776932353</td>\n",
       "      <td>I almost got a chest tattoo</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4522</th>\n",
       "      <td>743560627075973120</td>\n",
       "      <td>I shop way too much</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4523</th>\n",
       "      <td>748482888044281856</td>\n",
       "      <td>So joke seeing people tweet about respect when...</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4524</th>\n",
       "      <td>742619605542141952</td>\n",
       "      <td>omg there's nothing wrong with her she's so cute</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>746494368689729537</td>\n",
       "      <td>my phone can't be dry like this all summer som...</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4526</th>\n",
       "      <td>742102200931876864</td>\n",
       "      <td>true</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4527</th>\n",
       "      <td>744616631159005184</td>\n",
       "      <td>you wack</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4528</th>\n",
       "      <td>748450521820839936</td>\n",
       "      <td>Boris. Gove. May. Crabb. As an LGBT person thi...</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>742449710653399042</td>\n",
       "      <td>i got alot going on right now</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4530 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               status_id                                              tweet  \\\n",
       "0     744211689319501825  Like hello, I'm watching it too. Mind lowering...   \n",
       "1     742516659219509252  can u Dm ur pics with the twins for my ig fan ...   \n",
       "2     746762559387111425                 happy birthday jaz. Enjoy your day   \n",
       "3     743395873757892609  got joe and pewds book soon tabinof and da sid...   \n",
       "4     747399941434146817  Every time I up this late, I'm usually always ...   \n",
       "5     744374136793759744     I can't wait to meet my stepbrother next year.   \n",
       "6     747927031728377857  When you know you can't sing but you're singin...   \n",
       "7     742575673332531200                        I love that's daddy forever   \n",
       "8     747312310121697280                                      We are lonely   \n",
       "9     747090755890577409                                   Sleepy in church   \n",
       "10    744311326168780801                         5 guys was sooooo worth it   \n",
       "11    746500533553291264              I was one away from hitting at Bingo!   \n",
       "12    747538907449921537                           a good idea, I'm all for   \n",
       "13    742407595487547397  Driving in heels might be the most dangerous t...   \n",
       "14    742295120104546304  Excited to actually start work today and final...   \n",
       "15    741776463167430664  I Haven't Talked To Twizzy All Day I Swear I B...   \n",
       "16    743322032499089413   you and the homies gon have to pick up after tho   \n",
       "17    741401468528435200  .@greysonchance How can anyone ever possibly l...   \n",
       "18    744001439983247360                                      i love u yeri   \n",
       "19    741382849815662594     I don't get back till Monday night wait for me   \n",
       "20    742910363751088128                                  okay wow my heart   \n",
       "21    741955962420887552                             thanks see you around!   \n",
       "22    747775902684160000                   12 more hours then some holidays   \n",
       "23    747436079976943616                   r u gna join us in the pc empire   \n",
       "24    743211787458215936                       my maths isn't STRONG ANDREW   \n",
       "25    742445566924328961  HOW ARE THEY STILL LOOKING FOR A? It's infuria...   \n",
       "26    744726546699681792  The cav's when and after the game D wade comme...   \n",
       "27    743674700073598977           Ion Like This Person Im Becoming But Idk   \n",
       "28    744453726136578048                           Fuck it, something to do   \n",
       "29    742668696691171329            Just bread ft. Nutella for today's menu   \n",
       "...                  ...                                                ...   \n",
       "4500  747818614930083844              i should be there ahhhh jealousnyaaaa   \n",
       "4501  741891215365312512       Standing from 1pm up to now. Hays. So tired.   \n",
       "4502  748271428810711040  I'm not looking forward to the pictures people...   \n",
       "4503  744047448662880257                                      shet maichard   \n",
       "4504  745011494841024512                    but still get out your feelings   \n",
       "4505  741078978157498368                      I have the worst migraine man   \n",
       "4506  743295968607559680                              It Was So Funny To Me   \n",
       "4507  743295968607559680                              It Was So Funny To Me   \n",
       "4508  743232456615202816                              Been downtown all day   \n",
       "4509  743658508520939520  sooner or later ya gone get tired of hating on...   \n",
       "4510  744977945979138048              I just wanna wear my prom dress again   \n",
       "4511  748021182285357057               Endless summer tour looks so lit man   \n",
       "4512  746927952215027712                                               frfr   \n",
       "4513  748349730585927681  Totally not feeling work today. My head hurts....   \n",
       "4514  743331470312251392                                       I miss Angie   \n",
       "4515  744005701501018113        In my mind I left, but my heart still there   \n",
       "4516  746726235602980866  I need to stop drinking so much or put my cash...   \n",
       "4517  744444279230062592   wrong move pwetzzz wrong move ugh so many people   \n",
       "4518  742036494341275652  Too much bad in the world, and not enough good...   \n",
       "4519  747289721206767617  Messi saying he's leaving the national team ho...   \n",
       "4520  742961773867892737                              Need to clean my room   \n",
       "4521  743560794776932353                        I almost got a chest tattoo   \n",
       "4522  743560627075973120                                I shop way too much   \n",
       "4523  748482888044281856  So joke seeing people tweet about respect when...   \n",
       "4524  742619605542141952   omg there's nothing wrong with her she's so cute   \n",
       "4525  746494368689729537  my phone can't be dry like this all summer som...   \n",
       "4526  742102200931876864                                               true   \n",
       "4527  744616631159005184                                           you wack   \n",
       "4528  748450521820839936  Boris. Gove. May. Crabb. As an LGBT person thi...   \n",
       "4529  742449710653399042                      i got alot going on right now   \n",
       "\n",
       "     label  \n",
       "0     1397  \n",
       "1     1380  \n",
       "2      623  \n",
       "3     1380  \n",
       "4     1381  \n",
       "5     1384  \n",
       "6     1389  \n",
       "7     1420  \n",
       "8      675  \n",
       "9     1399  \n",
       "10    1392  \n",
       "11    1420  \n",
       "12    1399  \n",
       "13    1384  \n",
       "14    1390  \n",
       "15    1420  \n",
       "16    1381  \n",
       "17    1381  \n",
       "18    1392  \n",
       "19    1424  \n",
       "20    1106  \n",
       "21    1383  \n",
       "22     616  \n",
       "23    1387  \n",
       "24    1381  \n",
       "25    1466  \n",
       "26    1381  \n",
       "27    1430  \n",
       "28    1381  \n",
       "29    1392  \n",
       "...    ...  \n",
       "4500  1424  \n",
       "4501  1421  \n",
       "4502  1416  \n",
       "4503  1420  \n",
       "4504  1424  \n",
       "4505  1424  \n",
       "4506  1420  \n",
       "4507  1424  \n",
       "4508  1420  \n",
       "4509  1424  \n",
       "4510  1422  \n",
       "4511  1424  \n",
       "4512  1420  \n",
       "4513  1409  \n",
       "4514  1400  \n",
       "4515  1400  \n",
       "4516  1106  \n",
       "4517  1421  \n",
       "4518  1409  \n",
       "4519  1413  \n",
       "4520  1398  \n",
       "4521  1399  \n",
       "4522  1399  \n",
       "4523  1424  \n",
       "4524  1420  \n",
       "4525  1420  \n",
       "4526  1400  \n",
       "4527  1424  \n",
       "4528  1400  \n",
       "4529  1420  \n",
       "\n",
       "[4530 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_set['tweet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['label']=data_set['label'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['tweet'] = data_set['tweet'].apply(cleanup_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tweet(tweet):\n",
    "    tweet = tweet.replace(\".\", \" . \") \\\n",
    "                 .replace(\",\", \" , \") \\\n",
    "                 .replace(\";\", \" ; \") \\\n",
    "                 .replace(\"?\", \" ? \")\n",
    "    return tweet.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_words(glove_vector):\n",
    "    train, valid, test = [],[],[]\n",
    "    data_set = get_data_set()\n",
    "    for i in range(len(data_set)):\n",
    "        tweet = data_set['tweet'][i]\n",
    "        if(type(tweet) != str):\n",
    "            continue\n",
    "        idxs = [glove_vector.stoi[w]        # lookup the index of word\n",
    "                for w in split_tweet(tweet)\n",
    "                if w in glove_vector.stoi] # keep words that has an embedding\n",
    "        if not idxs: # ignore tweets without any word with an embedding\n",
    "            continue\n",
    "        idxs = torch.tensor(idxs) # convert list to pytorch tensor\n",
    "        label = torch.tensor(data_set['label'][i]).long()\n",
    "        if i % 5 < 3:\n",
    "            train.append((idxs, label))\n",
    "        elif i % 5 == 4:\n",
    "            valid.append((idxs, label))\n",
    "        else:\n",
    "            test.append((idxs, label))\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = get_tweet_words(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_emb = glove_emb(train[0][0])\n",
    "tweet_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = nn.RNN(input_size=50,    # dimension of the input repr\n",
    "                   hidden_size=50,   # dimension of the hidden units\n",
    "                   batch_first=True) # input format is [batch_size, seq_len, repr_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input = tweet_emb.unsqueeze(0) # add the batch_size dimension\n",
    "h0 = torch.zeros(1, 1, 50)     # initial hidden layer\n",
    "out, last_hidden = rnn_layer(tweet_input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)\n",
    "print(last_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[:,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TweetRNN, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Look up the embedding\n",
    "        x = self.emb(x)\n",
    "        # Set an initial hidden state\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = TweetRNN(input_size=50, hidden_size=50, num_classes=len(interesting_labels.wanted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0:16097][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([element[1].item() for element in train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class TweetBatcher:\n",
    "    def __init__(self, tweets, batch_size=32, drop_last=False):\n",
    "        # store tweets by length\n",
    "        self.tweets_by_length = {}\n",
    "        for words, label in tweets:\n",
    "            # compute the length of the tweet\n",
    "            wlen = words.shape[0]\n",
    "            # put the tweet in the correct key inside self.tweet_by_length\n",
    "            if wlen not in self.tweets_by_length:\n",
    "                self.tweets_by_length[wlen] = []\n",
    "            self.tweets_by_length[wlen].append((words, label),)\n",
    "         \n",
    "        #  create a DataLoader for each set of tweets of the same length\n",
    "        self.loaders = {wlen : torch.utils.data.DataLoader(\n",
    "                                    tweets,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    drop_last=drop_last) # omit last batch if smaller than batch_size\n",
    "            for wlen, tweets in self.tweets_by_length.items()}\n",
    "        \n",
    "    def __iter__(self): # called by Python to create an iterator\n",
    "        # make an iterator for every tweet length\n",
    "        iters = [iter(loader) for loader in self.loaders.values()]\n",
    "        while iters:\n",
    "            # pick an iterator (a length)\n",
    "            im = random.choice(iters)\n",
    "            try:\n",
    "                yield next(im)\n",
    "            except StopIteration:\n",
    "                # no more elements in the iterator, remove it\n",
    "                iters.remove(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (tweets, labels) in enumerate(TweetBatcher(train, drop_last=True)):\n",
    "    print(tweets.shape, labels.shape)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    correct, total = 0, 0\n",
    "    for tweets, labels in data_loader:\n",
    "        output = model(tweets)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += labels.shape[0]\n",
    "    return float(correct) / float(total)\n",
    "\n",
    "test_loader = TweetBatcher(test, batch_size=32, drop_last=False)\n",
    "get_accuracy(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_network(model, train, valid, num_epochs=5, learning_rate=1e-5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses, train_acc, valid_acc = [], [], []\n",
    "    epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for tweets, labels in train:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(tweets)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "        train_acc.append(get_accuracy(model, train_loader))\n",
    "        valid_acc.append(get_accuracy(model, valid_loader))\n",
    "        print(\"Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
    "              epoch+1, loss, train_acc[-1], valid_acc[-1]))\n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(losses, label=\"Train\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TweetRNN(input_size=50, hidden_size=50, num_classes=len(interesting_labels.wanted_list))\n",
    "train_loader = TweetBatcher(train, batch_size=64, drop_last=True)\n",
    "valid_loader = TweetBatcher(valid, batch_size=64, drop_last=False)\n",
    "train_rnn_network(model, train_loader, valid_loader, num_epochs=30, learning_rate=2e-4)\n",
    "get_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TweetLSTM, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Look up the embedding\n",
    "        x = self.emb(x)\n",
    "        # Set an initial hidden state and cell state\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = TweetLSTM(input_size=50, hidden_size=50, num_classes=len(interesting_labels.wanted_list))\n",
    "train_rnn_network(model, train_loader, valid_loader, num_epochs=30, learning_rate=2e-4)\n",
    "get_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
